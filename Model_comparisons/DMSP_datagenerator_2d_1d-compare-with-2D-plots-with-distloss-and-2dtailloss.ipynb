{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/nasaomnireader-0.1.0-py3.6.egg/nasaomnireader/__init__.py\", line 5, in <module>\n",
      "    from nasaomnireader.omnireader_config import config\n",
      "ModuleNotFoundError: No module named 'nasaomnireader.omnireader_config'\n",
      "\n",
      "Solar wind data files will be saved to /home/jackalak/.local/share/nasaomnireader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# os.system('source /home/jackalak/heartbeat/cdf38_0-dist/bin/definitions.B')\n",
    "# os.environ[\"CDF_LIB\"] = '/home/jackalak/heartbeat/cdf38_0-dist/lib'\n",
    "# from spacepy import pycdf\n",
    "# from nasaomnireader import omnireader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 14}\n",
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12) \n",
    "matplotlib.rc('font', **font)\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 9\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "import ftplib\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.keras.backend.set_floatx(\n",
    "    'float32'\n",
    ")\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "import tensorflow\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import datetime\n",
    "from os.path import isfile, join\n",
    "from sys import getsizeof\n",
    "import glob\n",
    "\n",
    "from random import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "\n",
    "# os.system('mkdir figures2')\n",
    "# os.system('mkdir figures2/energyflux')\n",
    "# os.system('mkdir figures2/numberflux')\n",
    "# os.system('mkdir figures2/channelnumberflux')\n",
    "# os.system('mkdir figures2/numberflux_from_channels')\n",
    "# os.system('mkdir figures')\n",
    "\n",
    "os.system('source /home/jackalak/heartbeat/cdf38_0-dist/bin/definitions.B')\n",
    "os.environ[\"CDF_LIB\"] = '/home/jackalak/heartbeat/cdf38_0-dist/lib'\n",
    "from spacepy import pycdf\n",
    "import matplotlib.pyplot as plt\n",
    "from ovationpyme.ovation_prime import FluxEstimator,AverageEnergyEstimator,BinCorrector\n",
    "from ovationpyme.ovation_utilities import calc_avg_solarwind\n",
    "from ovationpyme.ovation_plotting import latlt2polar,polar2dial,pcolor_flux\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "from nasaomnireader import omnireader\n",
    "\n",
    "import ftplib\n",
    "import os\n",
    "\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pickle\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import datetime\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "import tensorflow\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "from sklearn import preprocessing\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 14}\n",
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12) \n",
    "matplotlib.rc('font', **font)\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 13, 10\n",
    "\n",
    "import datetime\n",
    "from os.path import isfile, join\n",
    "from sys import getsizeof\n",
    "from random import *\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/tensorflow/tensorflow/issues/956\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import InputSpec \n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "\n",
    "class PeriodicPadding2D(layers.Layer):\n",
    "\n",
    "  def __init__(self, padding=1, **kwargs):\n",
    "    super(PeriodicPadding2D, self).__init__(**kwargs)\n",
    "    self.padding = conv_utils.normalize_tuple(padding, 1, 'padding')\n",
    "    self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "  def wrap_pad(self, input, size):\n",
    "    M1 = tf.concat([input[:,:, -size:], input, input[:,:, 0:size]], 2)\n",
    "    M1 = tf.concat([M1[:,0:size, :], M1, M1[:,0:size, :]], 1) #not periodic\n",
    "    return M1\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  \n",
    "    if shape[1] is not None:\n",
    "      length = shape[1] + 2*self.padding[0]\n",
    "    else:\n",
    "      length = None\n",
    "    return tuple([shape[0], length, length])\n",
    "\n",
    "  def call(self, inputs): \n",
    "    return self.wrap_pad(inputs, self.padding[0])\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {'padding': self.padding}\n",
    "    base_config = super(PeriodicPadding2D, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "  \n",
    "    loss = K.sum( K.cast(K.greater(y_true, 0),'float64')*\n",
    "               K.square(y_true-y_pred) )  / ( \n",
    "                K.sum( K.cast(K.greater(y_true, 0),'float64') ))# finds the number of y_true  > 0\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    "    \n",
    "\n",
    "    \n",
    "    mse = K.sum( K.cast(K.greater(y_true, 0),'float64')*(\n",
    "        K.square(y_true-y_pred)) )/params['batch_size']  \n",
    "    return mse\n",
    "\n",
    "def for_CSV_val(scaler_X, model, features, test, XX_test):\n",
    "\n",
    "\n",
    "    num = test.shape[0]\n",
    "    \n",
    "\n",
    "    result_val = []\n",
    "\n",
    "    \n",
    "    for i in range(0,num):\n",
    "\n",
    "\n",
    "       #################################\n",
    "        #\n",
    "        #ML model\n",
    "        ##########################\n",
    "        results = model.predict( XX_test[i:i+1,:])\n",
    "        flux = results[0,:,:,0]             \n",
    "        pt = i\n",
    "\n",
    "        result_val.append( \n",
    "            flux[int((90-test['SC_AACGM_LAT'][pt])/45*128),int((test['SC_AACGM_LTIME'][pt])/24*128)] )     \n",
    "    \n",
    "        \n",
    "\n",
    "    return result_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(df_results, test):\n",
    "    \n",
    "    subdir = 'figures2/energyflux/'\n",
    "\n",
    "    y_val_log = np.log10(test['ELE_TOTAL_ENERGY_FLUX'])\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Electron Total Energy Flux, log10 scale')    \n",
    "    plt.plot(y_val_log[:],alpha=0.5)\n",
    "    plt.plot(df_results[:],alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('log10 eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'1',dpi=dpi)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Electron Total Energy Flux')   \n",
    "    plt.plot(10**y_val_log[:]*1.6e-6,alpha=0.5)\n",
    "    plt.plot(10**df_results[:]*1.6e-6,alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('erg/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'2',dpi=dpi)\n",
    "    start=int(y_val_log.shape[0]/2)\n",
    "    plt.figure()\n",
    "    plt.title('Electron Total Energy Flux, log10 scale')   \n",
    "    plt.plot(y_val_log[start:2000+start],alpha=0.5)\n",
    "    plt.plot(df_results[start:2000+start],alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('log10 eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'3',dpi=dpi)\n",
    "    plt.figure()\n",
    "    plt.title('Electron Total Energy Flux')   \n",
    "    plt.plot(10**y_val_log[start:2000+start]*1.6e-6,alpha=0.5)\n",
    "    plt.plot(10**df_results[start:2000+start]*1.6e-6,alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('erg/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'4',dpi=dpi)\n",
    "    \n",
    "\n",
    "    minr = np.min(y_val_log.values)\n",
    "    maxr = np.max(y_val_log.values)\n",
    "    plt.figure()\n",
    "    plt.title('Histogram of Electron Total Energy Flux, log10 scale')   \n",
    "    plt.hist(y_val_log.values,bins=200,alpha=0.5,range=(minr,maxr))\n",
    "    plt.hist(df_results.values,bins=200,alpha=0.5,range=(minr,maxr))\n",
    "    plt.legend(['val','result'], loc='upper left')\n",
    "    plt.ylabel('#/bin')\n",
    "    plt.xlabel('log10 eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'5',dpi=dpi)\n",
    "    plt.figure()\n",
    "    plt.title('Histogram of Electron Total Energy Flux')   \n",
    "    plt.hist(10**y_val_log.values*1.6e-6,bins=100, log=True,range=(10**minr*1.6e-6,10**maxr*1.6e-6),alpha=0.5)\n",
    "    plt.hist(10**df_results.values*1.6e-6,bins=100, log=True,range=(10**minr*1.6e-6,10**maxr*1.6e-6),alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('#/bin')\n",
    "    plt.xlabel('erg/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'6',dpi=dpi)\n",
    "    import matplotlib.colors as mcolors\n",
    "    gamma = 0.2#[0.8, 0.5, 0.3]\n",
    "\n",
    "    errors= y_val_log.values-df_results.values[:,0]\n",
    "    plt.figure();\n",
    "    plt.hist2d(test['SC_AACGM_LAT'].values, errors,\n",
    "                  bins=50, norm=mcolors.PowerNorm(gamma))\n",
    "    plt.title('Error density over SC_AACGM_LAT Bins')\n",
    "    plt.xlabel('SC_AACGM_LAT degrees')\n",
    "    plt.ylabel('log10(y_true)-log10(y_pred) eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'7',dpi=dpi)\n",
    "    plt.figure()\n",
    "    bin_total = np.zeros((200))\n",
    "    bin_error_total = np.zeros((200))\n",
    "    for j in range(0,y_val_log.values.shape[0]):\n",
    "        i = int((test['SC_AACGM_LAT'].values[j]-45)/((90-45)/200))\n",
    "        if i < 200:\n",
    "            bin_total[i] = bin_total[i]+1\n",
    "            bin_error_total[i] = bin_error_total[i] + np.abs(errors[j])\n",
    "\n",
    "    avg_error_over_hist = bin_error_total/bin_total\n",
    "    plt.scatter(np.linspace(45,90,num=200),avg_error_over_hist)\n",
    "    plt.title('Average Validation Error over SC_AACGM_LAT Bins')\n",
    "    plt.xlabel('SC_AACGM_LAT degrees')\n",
    "    plt.ylabel('log10(y_true)-log10(y_pred) eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'8',dpi=dpi)\n",
    "    bin_total = np.zeros((200))\n",
    "    bin_error_total = np.zeros((200))\n",
    "    for j in range(0,y_val_log.values.shape[0]):\n",
    "        i = int((y_val_log[j]-minr)/((maxr-minr)/200))\n",
    "        if i < 200:\n",
    "            bin_total[i] = bin_total[i]+1\n",
    "            bin_error_total[i] = bin_error_total[i] + np.abs(errors[j])\n",
    "\n",
    "    avg_error_over_hist = bin_error_total/(bin_total+.00001)\n",
    "    plt.figure()\n",
    "    plt.scatter(np.linspace(minr,maxr,num=200),avg_error_over_hist)\n",
    "    plt.title('Average Validation Error over target Bins')    \n",
    "    plt.xlabel('log10(y_true) eV/cm^2/ster/s')\n",
    "    plt.ylabel('log10(y_true)-log10(y_pred) eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'9',dpi=dpi)\n",
    "    import matplotlib.colors as mcolors\n",
    "    gamma = 0.2#[0.8, 0.5, 0.3]\n",
    "    errors= y_val_log.values-df_results.values[:,0]\n",
    "\n",
    "    plt.figure();\n",
    "    plt.hist2d(y_val_log.values, errors,\n",
    "                  bins=50, norm=mcolors.PowerNorm(gamma))\n",
    "    plt.colorbar()\n",
    "    plt.title('Error Density')\n",
    "    plt.xlabel('log10(y_true) eV/cm^2/ster/s')\n",
    "    plt.ylabel('log10(y_true)-log10(y_pred) eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'10',dpi=dpi)\n",
    "    plt.figure();\n",
    "    plt.hist2d(y_val_log.values, df_results.values[:,0],\n",
    "                  bins=50, norm=mcolors.PowerNorm(gamma))\n",
    "    plt.colorbar()\n",
    "    plt.title('Error Density')\n",
    "    plt.xlabel('log10(y_true) eV/cm^2/ster/s')\n",
    "    plt.ylabel('log10(y_pred) eV/cm^2/ster/s')\n",
    "    temp = np.array([7,8,9,10,11,12,13])\n",
    "    plt.plot(temp,temp,color='k')\n",
    "    plt.xlim([7,13])\n",
    "    plt.ylim([7,13])\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'11',dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_omni_data(t_start, t_end):\n",
    "\n",
    "    #--------------------------------------------------------#\n",
    "    #\tOMNI Data - includes solar wind, and geomag params   #\n",
    "    #--------------------------------------------------------#\n",
    "\n",
    "    #get OMNI data\n",
    "    omniInt = omnireader.omni_interval(t_start,t_end,'5min', cdf_or_txt = 'txt')\n",
    "\n",
    "    #print(omniInt.cdfs[0].vars) #prints all the variables available on omni\n",
    "\n",
    "    epochs = omniInt['Epoch'] #time array for omni 5min data\n",
    "    By,Bz,AE,SymH = omniInt['BY_GSM'],omniInt['BZ_GSM'],omniInt['AE_INDEX'], omniInt['SYM_H']\n",
    "    AL, AU = omniInt['AL_INDEX'],omniInt['AU_INDEX']\n",
    "    vsw,psw = omniInt['flow_speed'], omniInt['Pressure']\n",
    "    borovsky_reader = omnireader.borovsky(omniInt)\n",
    "    borovsky = borovsky_reader()\n",
    "    #newell_reader = omnireader.newell(omniInt)\n",
    "    #newell = newell_reader()\n",
    "\n",
    "    def NewellCF_calc(v,bz,by):\n",
    "        # v expected in km/s\n",
    "        # b's expected in nT    \n",
    "        NCF = np.zeros_like(v)\n",
    "        NCF.fill(np.nan)\n",
    "        bt = np.sqrt(by**2 + bz**2)\n",
    "        bztemp = bz\n",
    "        bztemp[bz == 0] = .001\n",
    "        #Caculate clock angle (theta_c = t_c)\n",
    "        tc = np.arctan2(by,bztemp)\n",
    "        neg_tc = bt*np.cos(tc)*bz < 0 \n",
    "        tc[neg_tc] = tc[neg_tc] + np.pi\n",
    "        sintc = np.abs(np.sin(tc/2.))\n",
    "        NCF = (v**1.33333)*(sintc**2.66667)*(bt**0.66667)\n",
    "        return NCF\n",
    "\n",
    "\n",
    "    newell = NewellCF_calc(vsw, Bz, By)\n",
    "\n",
    "\n",
    "    # \tproton_flux_10MeV, proton_flux_30MeV, proton_flux_60MeV = omniInt['PR-FLX_10'], omniInt['PR-FLX_30'], omniInt['PR-FLX_60']\n",
    "\n",
    "\n",
    "    #calculate clock angle\n",
    "    clock_angle = np.degrees(np.arctan2( By,Bz))\n",
    "    clock_angle[clock_angle < 0] = clock_angle[clock_angle<0] + 360.\n",
    "\n",
    "    #print('Got 5 minutes data')\n",
    "\n",
    "    omniInt_1hr = omnireader.omni_interval(t_start,t_end,'hourly', cdf_or_txt = 'txt')\n",
    "    F107,KP = omniInt_1hr['F10_INDEX'], omniInt_1hr['KP']\n",
    "    KP = pd.DataFrame(np.repeat(KP,12,axis=0))\n",
    "    F107 = pd.DataFrame(np.repeat(F107,12,axis=0))\n",
    "\n",
    "\n",
    "\n",
    "    #put all in a dataframe and save\n",
    "\n",
    "    dataframe = pd.DataFrame()\n",
    "    dataframe['Bz'] = Bz\n",
    "    dataframe['By'] = By\n",
    "    dataframe['Vsw'] = vsw\n",
    "    dataframe['Vx'] = omniInt['Vx']\n",
    "    dataframe['Psw'] = psw\n",
    "    dataframe['AE'] = AE\n",
    "    dataframe['AL'] = AL\n",
    "    dataframe['AU'] = AU\n",
    "    dataframe['SymH'] = SymH\n",
    "    dataframe['Clock Angle'] = clock_angle\n",
    "    dataframe['newell'] = newell\n",
    "    dataframe['borovsky'] = borovsky\n",
    "    dataframe['Kp'] = KP\n",
    "    dataframe['F107'] = F107\n",
    "    dataframe['PC'] = omniInt['PC_N_INDEX']\n",
    "    dataframe['Bx'] = omniInt['BX_GSE']\n",
    "    # \tdataframe = dataframe.replace(9999.99, np.nan) #replace 9999.99 with nans ??????????????\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "class t_hist():\n",
    "    '''\n",
    "    Class which takes solar wind data and creates some time history\n",
    "    for some specific time.\n",
    "    \n",
    "    Input:\n",
    "        Data ---------- A DataFrame of solar wind data at 5 minute\n",
    "                        cadence and datetime index.\n",
    "        Historic_time - The number of minutes into the past you\n",
    "                        would like the hisotry for. (E.g. for 1hr ago\n",
    "                        you would input 60 minutes).\n",
    "        window_mins --- If averaging for the time history, then this\n",
    "                        input specifies the window length, in minutes,\n",
    "                        centred on the historic_time specified.\n",
    "    '''\n",
    "    def __init__(self,data,historic_time,window_mins):\n",
    "        self.data = data\n",
    "        self.time = historic_time\n",
    "        self.window = window_mins\n",
    "        \n",
    "    def avg_hist(self):\n",
    "        '''\n",
    "        Function which returns a historic_time value, averaged over\n",
    "        the window_mins.\n",
    "        \n",
    "        Output:\n",
    "            - A dataframe of values of the time history.\n",
    "        '''\n",
    "        # Check that indices are datetime\n",
    "        self.is_datetime()\n",
    "        \n",
    "        if self.time % 60:\n",
    "            raise ValueError('Please choose a historic time value '+\n",
    "                             'which correspond to an integer '+\n",
    "                             'number of hours!')\n",
    "        \n",
    "        window_s = timedelta(minutes = self.time + self.window/2.0)\n",
    "        # '+5' ensures that the window is closed on the right\n",
    "        window_e = timedelta(minutes = self.time + 5 -\n",
    "                             self.window/2.0)\n",
    "        indices = self.data.index\n",
    "        \n",
    "        hist = [self.data[i-window_s : i-window_e].mean().values\n",
    "                for i in indices]\n",
    "        \n",
    "        col_label = '_'+str(self.time/60.0)[0]+'hr'\n",
    "        columns = [i+col_label for i in self.data.columns]\n",
    "        \n",
    "        th_df = pd.DataFrame(hist, index=indices,columns=columns)\n",
    "        \n",
    "        return th_df[th_df.index[0]+window_s:]\n",
    "    \n",
    "    def instant_hist(self):\n",
    "        '''\n",
    "        Function which returns an instantaneous historic_time value.\n",
    "        \n",
    "        Output:\n",
    "            - A dataframe of instantaneous values corresponding to\n",
    "              historic_time minutes in the past.\n",
    "        '''\n",
    "        # Check that indices are datetime\n",
    "        self.is_datetime()\n",
    "        \n",
    "        if self.time % 5:\n",
    "            raise ValueError('Please choose a historic time value '+\n",
    "                             'which correspond to a multiple of 5 '+\n",
    "                             'minutes!')\n",
    "            \n",
    "        t_offset = timedelta(minutes=self.time)\n",
    "        indices = self.data.index\n",
    "        \n",
    "\n",
    "        hist = [self.data.loc[i-t_offset].values\n",
    "                 for i in self.data.index\n",
    "                 if i >= indices[0]+t_offset]\n",
    "        \n",
    "        if self.time < 60:\n",
    "            if self.time >= 10:\n",
    "                col_label = '_'+str(self.time)[0:2]+'min'\n",
    "            else:\n",
    "                col_label = '_'+str(self.time)[0]+'min'\n",
    "        else:\n",
    "            col_label = '_'+str(self.time/60.0)[0]+'hr_I'\n",
    "        columns = [i+col_label for i in self.data.columns]\n",
    "        return pd.DataFrame(hist, index=indices[int(self.time/5):],\n",
    "                            columns=columns)\n",
    "    \n",
    "    def is_datetime(self):\n",
    "        dt_type = pd.core.indexes.datetimes.DatetimeIndex\n",
    "        if type(self.data.index) != dt_type:\n",
    "            raise ValueError('Dataframe index is not '+\n",
    "                             'in the correct datetime '+\n",
    "                             'format')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "def cleaning_data(data,safe_cols=[],sigma_val=4):\n",
    "    '''\n",
    "    Function which removes data which is 'sigma_val' stdevs from\n",
    "    the mean.\n",
    "\n",
    "    Note: 4 sigma encompasses ~99.994% of the data.\n",
    "          ~1 real piece of 5 min data is removed for\n",
    "          every 55 days of such data (assuming Gaussian).\n",
    "\n",
    "    Inputs:\n",
    "    sigma_val - (float) number of standard deviations from the\n",
    "                mean to consider as the limit of 'good' data.\n",
    "    safe_cols - Columns in the data which one might like to\n",
    "                keep without any changes (i.e., if there are\n",
    "                no null values in the initial dataset etc.).\n",
    "\n",
    "    Returns:\n",
    "     - Cleaned solar wind data dataframe.\n",
    "     - Dataframe of 'bad' solar wind data.\n",
    "    '''\n",
    "\n",
    "#   Initialising data and empty lists\n",
    "    sw_df = data\n",
    "    cleaned_cols = []\n",
    "    trash_data = []\n",
    "#   Looping through dataframe columns and removing 'bad' values\n",
    "    for i in sw_df.columns:\n",
    "        if i not in safe_cols:\n",
    "            std = sw_df[i].std()\n",
    "            mean = sw_df[i].mean()\n",
    "\n",
    "            cleaned = sw_df[i][sw_df[i]<mean+std*sigma_val]\n",
    "            trash = sw_df[i][sw_df[i]>=mean+std*sigma_val]\n",
    "\n",
    "            cleaned_cols.append(cleaned)\n",
    "            trash_data.append(trash)\n",
    "        else:\n",
    "            cleaned_cols.append(sw_df[i])\n",
    "            trash_data.append([np.nan])\n",
    "#   Initialising empty dataframes and appending data\n",
    "    sw_c_df = pd.DataFrame()\n",
    "    trash_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(sw_df.columns)):\n",
    "#         sw_c_df[sw_df.columns[i]] = cleaned_cols[i]\n",
    "#         sw_c_df = sw_c_df\n",
    "        sw_c_df_temp = pd.DataFrame(cleaned_cols[i],\n",
    "                                 columns=[sw_df.columns[i]])\n",
    "        sw_c_df = pd.concat([sw_c_df,sw_c_df_temp], axis=1)\n",
    "\n",
    "        trash_df_temp = pd.DataFrame(trash_data[i],\n",
    "                                     columns=[sw_df.columns[i]])\n",
    "        trash_df = pd.concat([trash_df,trash_df_temp], axis=1)\n",
    "\n",
    "#   Checking if the trash data contains non-'bad' data.\n",
    "    check_trash(trash_df)\n",
    "    return (sw_c_df, trash_df)\n",
    "\n",
    "#################################\n",
    "\n",
    "def sw_interp(data,method='linear'):\n",
    "    '''\n",
    "    Function which interpolates NaN values in the cleaned\n",
    "    data dataframe.\n",
    "\n",
    "    See Pandas documentation for other methods.\n",
    "\n",
    "    Input:\n",
    "    method - method of interpolation.\n",
    "\n",
    "    Return:\n",
    "     - Cleaned, interpolated data.\n",
    "    '''\n",
    "    return data.interpolate(method=method)\n",
    "\n",
    "#################################\n",
    "\n",
    "def check_trash(trash_data):\n",
    "    '''\n",
    "    Function which checks to see if all the removed data\n",
    "    is the 'bad' data fill value.\n",
    "\n",
    "    Returns:\n",
    "     - String detailing which parameters have had real\n",
    "       removed from them.\n",
    "    '''\n",
    "    for i in trash_data.columns:\n",
    "        if (trash_data[i].mean() <\n",
    "            trash_data[i].max()):\n",
    "            print('Some real data has been removed from: ',i)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "def time_history(data,auto=True):\n",
    "    '''\n",
    "    Function which calculates time history information\n",
    "    given an input dataframe.\n",
    "    \n",
    "    Averages are centred on the respective time-history\n",
    "    specified.\n",
    "    \n",
    "    Input:\n",
    "    data - a Pandas DataFrame containing 5 minute cadence\n",
    "           data.\n",
    "           MUST HAVE DATETIME INDEX.\n",
    "    auto - Whether or not to automatically clean the data.\n",
    "           If not True, then the cleaning_data() and\n",
    "           sw_interp() functions must be called individually\n",
    "           and the results of sw_interp() should be the data\n",
    "           fed to this function, time_history(). Set to false\n",
    "           to retrieve the non-interpolated data and the trash\n",
    "           data.\n",
    "    \n",
    "    Output:\n",
    "    A concatenated DataFrame containing\n",
    "        - the original data\n",
    "        - t-6hrs (1hr avg)\n",
    "        - t-5hrs (1hr avg)\n",
    "        - t-3hrs (30min avg)\n",
    "        - t-1hrs (30min avg)\n",
    "        - t-45min (instant)\n",
    "        - t-30min (instant)\n",
    "        - t-15min (instant)\n",
    "        - t-10min (instant)\n",
    "        - t-5min (instant)\n",
    "    '''\n",
    "    if auto is True:\n",
    "        c_data,t_data = cleaning_data(data,sigma_val=4,\n",
    "                                  safe_cols=[None])\n",
    "        c_i_data = sw_interp(c_data,method='linear')\n",
    "        \n",
    "        data = c_i_data\n",
    "    else:\n",
    "        pass\n",
    "    return pd.concat((data,\n",
    "                      t_hist(data,360,60).avg_hist(),\n",
    "                      t_hist(data,300,60).avg_hist(),\n",
    "                      t_hist(data,180,30).avg_hist(),\n",
    "                      t_hist(data,60,30).avg_hist(),\n",
    "                      t_hist(data,45,0).instant_hist(),\n",
    "                      t_hist(data,30,0).instant_hist(),\n",
    "                      t_hist(data,15,0).instant_hist(),\n",
    "                      t_hist(data,10,0).instant_hist(),\n",
    "                      t_hist(data,5,0).instant_hist()),axis=1)\n",
    "\n",
    "import ftplib\n",
    "import os\n",
    "\n",
    "def _is_ftp_dir(ftp_handle, name, guess_by_extension=True):\n",
    "    \"\"\" simply determines if an item listed on the ftp server is a valid directory or not \"\"\"\n",
    "\n",
    "    # if the name has a \".\" in the fourth to last position, its probably a file extension\n",
    "    # this is MUCH faster than trying to set every file to a working directory, and will work 99% of time.\n",
    "    if guess_by_extension is True:\n",
    "        if name[-4] == '.':\n",
    "            return False\n",
    "\n",
    "    original_cwd = ftp_handle.pwd()     # remember the current working directory\n",
    "    try:\n",
    "        ftp_handle.cwd(name)            # try to set directory to new name\n",
    "        ftp_handle.cwd(original_cwd)    # set it back to what it was\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _make_parent_dir(fpath):\n",
    "    \"\"\" ensures the parent directory of a filepath exists \"\"\"\n",
    "    dirname = os.path.dirname(fpath)\n",
    "    while not os.path.exists(dirname):\n",
    "        try:\n",
    "            os.mkdir(dirname)\n",
    "            print(\"created {0}\".format(dirname))\n",
    "        except:\n",
    "            _make_parent_dir(dirname)\n",
    "\n",
    "\n",
    "def _download_ftp_file(ftp_handle, name, dest, overwrite,datetime_start,datetime_end):\n",
    "    \"\"\" downloads a single file from an ftp server \"\"\"\n",
    "    _make_parent_dir(dest)\n",
    "    \n",
    "    if (name[-7:-1]!='SHA1SU'): #ignore SHA1SUM files\n",
    "        month = int(name[-15:-13])\n",
    "        day = int(name[-13:-11])\n",
    "        year = int(name[-19:-15])\n",
    "        base = datetime.datetime(year, month, day)\n",
    "        if (datetime_start <= base <= datetime_end):\n",
    "\n",
    "            if not os.path.exists(dest) or overwrite is True:\n",
    "                with open(dest, 'wb') as f:\n",
    "                    ftp_handle.retrbinary(\"RETR {0}\".format(name), f.write)\n",
    "                print(\"downloaded: {0}\".format(dest))\n",
    "            else:\n",
    "                print(\"already exists: {0}\".format(dest))\n",
    "\n",
    "\n",
    "def _mirror_ftp_dir(ftp_handle, name, overwrite, guess_by_extension,datetime_start,datetime_end):\n",
    "    \"\"\" replicates a directory on an ftp server recursively \"\"\"\n",
    "    for item in ftp_handle.nlst(name):\n",
    "        if _is_ftp_dir(ftp_handle, item):\n",
    "            _mirror_ftp_dir(ftp_handle, item, overwrite, guess_by_extension,datetime_start,datetime_end)\n",
    "        else:\n",
    "            _download_ftp_file(ftp_handle, item, item, overwrite,datetime_start,datetime_end)\n",
    "\n",
    "\n",
    "def download_ftp_tree(ftp_handle, path, destination, datetime_start,datetime_end, overwrite=False, guess_by_extension=True):\n",
    "    \"\"\"\n",
    "    Downloads an entire directory tree from an ftp server to the local destination\n",
    "\n",
    "    :param ftp_handle: an authenticated ftplib.FTP instance\n",
    "    :param path: the folder on the ftp server to download\n",
    "    :param destination: the local directory to store the copied folder\n",
    "    :param overwrite: set to True to force re-download of all files, even if they appear to exist already\n",
    "    :param guess_by_extension: It takes a while to explicitly check if every item is a directory or a file.\n",
    "        if this flag is set to True, it will assume any file ending with a three character extension \".???\" is\n",
    "        a file and not a directory. Set to False if some folders may have a \".\" in their names -4th position.\n",
    "    \"\"\"\n",
    "    os.chdir(destination)\n",
    "    _mirror_ftp_dir(ftp_handle, path, overwrite, guess_by_extension,datetime_start,datetime_end)\n",
    "    \n",
    "def  get_data(datetime_start,datetime_end,sc_id):\n",
    "\n",
    "    years = {6:[1987], 7:[1987],8:[1987],9:[1988]\n",
    "             ,12:[2000,2001,2002], 13:[2000,2001,2002,2003,2004,2005,2006,2007],14:[2000,2001,2002,2003,2004,2005],\n",
    "             15:[2000,2001,2002,2003,2004,2005,2006,2007,2008,2009],\n",
    "             16:[2010,2011,2012,2013,2014,2015], #year 2003-2009 is not accessible on site\n",
    "             17:[2009,2010,2011,2012,2013,2014,2015],\n",
    "             18:[2010,2011,2012,2013,2014,2015],#2009 not accessible\n",
    "            } \n",
    "    \n",
    "#     if !(years[sc_id].any() != datetime_start.year):\n",
    "#         print('bad sc_id year combination')\n",
    "#         exit()\n",
    "    \n",
    "    dmsp_feature_list = ['ELE_AVG_ENERGY','ELE_AVG_ENERGY_STD','ELE_TOTAL_ENERGY_FLUX','ELE_TOTAL_ENERGY_FLUX_STD',\n",
    "                         'SC_AACGM_LAT',  'SC_AACGM_LON','SC_AACGM_LTIME',] \n",
    "    #                     'ION_AVG_ENERGY','ION_AVG_ENERGY_STD','ION_TOTAL_ENERGY_FLUX','ION_TOTAL_ENERGY_FLUX_STD','SC_GEOCENTRIC_LAT','SC_GEOCENTRIC_LON','SC_GEOCENTRIC_R']\n",
    "    dmsp_feature_list_19 =['ELE_COUNTS_BKG','ELE_COUNTS_OBS','ELE_DIFF_ENERGY_FLUX','ELE_DIFF_ENERGY_FLUX_STD',\n",
    "                      'CHANNEL_ENERGIES','ELE_COUNTS_BKG','ELE_COUNTS_OBS',]\n",
    "    #                  'ELE_GEOMETRIC','ION_COUNTS_BKG','ION_COUNTS_OBS','ION_DIFF_ENERGY_FLUX','ION_DIFF_ENERGY_FLUX_STD','ION_GEOMETRIC',\n",
    "    #                  'SC_ECI','SC_ECI_LABEL']\n",
    "\n",
    "    import glob\n",
    "    all_sc_df = pd.DataFrame()\n",
    "    count = 0\n",
    "    for ii in [sc_id]:\n",
    "        sc_df = pd.DataFrame()\n",
    "        print(ii)\n",
    "        directory = 'pub/data/dmsp/dmspf'\n",
    "        if ii <10:\n",
    "            directory = directory + '0'\n",
    "        directory = directory + str(ii) + '/ssj/precipitating-electrons-ions'\n",
    "        for year in [datetime_start.year]:\n",
    "            print(year)\n",
    "            \n",
    "            from ftplib import FTP_TLS\n",
    "            ftp=FTP_TLS('cdaweb.gsfc.nasa.gov')\n",
    "            ftp.login()\n",
    "            ftp.dir()\n",
    "            download_ftp_tree(ftp, directory, '.',datetime_start,datetime_end)\n",
    "            \n",
    "            file_list=glob.glob(directory + '/'+str(year)+'/*')\n",
    "            \n",
    "            \n",
    "            for file in file_list:\n",
    "                print(file)\n",
    "                #try:\n",
    "\n",
    "                count = count+1\n",
    "                month = int(file[-15:-13])\n",
    "                day = int(file[-13:-11])\n",
    "                base = datetime.datetime(year, month, day)\n",
    "\n",
    "                if (datetime_start <= base <= datetime_end):\n",
    "                    indices = np.array([base + datetime.timedelta(seconds=iii) for iii in range(0,60*24*60)])\n",
    "                    df = pd.DataFrame(data=indices,columns=['index'])\n",
    "                    cdf = pycdf.CDF(file)\n",
    "                    count = count+1\n",
    "                    for feature in dmsp_feature_list:\n",
    "                        df[feature] = cdf[feature]\n",
    "\n",
    "                    ch_energies = cdf['CHANNEL_ENERGIES']\n",
    "                    #print(ch_energies[:])\n",
    "                    ch_energies = np.flip((ch_energies[:]))\n",
    "                    ele_geom = np.flip( cdf['ELE_GEOMETRIC'][:])\n",
    "                                       \n",
    "                    obs_m_bkg = np.flip(cdf['ELE_COUNTS_OBS'][:,:]- cdf['ELE_COUNTS_BKG'][:,:],axis=1)\n",
    "                    temp_jN = np.zeros((60*24*60,19))\n",
    "                    for kk in range(0,19):\n",
    "                        temp_jN[:,kk] =( obs_m_bkg[:,kk]\n",
    "                             )/          ele_geom[kk]\n",
    "                        \n",
    "                    df['ELE_TOTAL_COUNTS'] = temp_jN[:,0]*(ch_energies[1]-ch_energies[0])\n",
    "                    + temp_jN[:,18]*(ch_energies[18]-ch_energies[17])\n",
    "                    for i in range(1,18):\n",
    "                        df['ELE_TOTAL_COUNTS'] = df['ELE_TOTAL_COUNTS']  +     temp_jN[:,i]*(ch_energies[i+1]-ch_energies[i-1])/2.\n",
    "\n",
    "                    for i in range(0,19):\n",
    "                        name = 'ELE_COUNT_'+str(i+1)\n",
    "                        df[name]=temp_jN[:,i]\n",
    "                    for i in range(0,19):\n",
    "                        name = 'ELE_diff_'+str(i+1)\n",
    "                        df[name]=cdf['ELE_DIFF_ENERGY_FLUX'][:,i]\n",
    "\n",
    "                    df = df.set_index('index')\n",
    "                    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "                    print('df',df.shape)#, df)\n",
    "                    #Create a time window\n",
    "                    df = df[df['ELE_TOTAL_ENERGY_FLUX'] > 0].dropna(subset=['ELE_TOTAL_ENERGY_FLUX'])\n",
    "\n",
    "                     #Create a time window\n",
    "                    sTimeIMF = datetime.datetime(year,month,day)\n",
    "                    eTimeIMF = sTimeIMF + datetime.timedelta(hours = 24)\n",
    "\n",
    "                    df_omni_5min = download_omni_data(sTimeIMF- datetime.timedelta(hours = 6),\n",
    "                                                 eTimeIMF+ datetime.timedelta(hours = 6))\n",
    "                    print('df_omni_5min',df_omni_5min.shape)#, df_omni_5min)\n",
    "                    indices = np.array([sTimeIMF- datetime.timedelta(hours = 6)+ datetime.timedelta(minutes=5*iii) for iii in range(0,df_omni_5min.shape[0])])\n",
    "                    print('indices',indices.shape)#, indices)\n",
    "\n",
    "                    df_omni_5min = pd.DataFrame(data=df_omni_5min.values,columns=df_omni_5min.columns, index=indices)\n",
    "                    print('df_omni_5min',df_omni_5min.shape)#, df_omni_5min)\n",
    "\n",
    "\n",
    "                    # call time_history to clean up omnireader data        \n",
    "                    df_omni_5min_cleaned = time_history(df_omni_5min)\n",
    "                    print('df_omni_5min_cleaned',df_omni_5min_cleaned.shape)#,, df_omni_5min_cleaned)\n",
    "                    df_omni_1min_cleaned = pd.DataFrame(np.repeat(df_omni_5min_cleaned.values,5*60,axis=0))\n",
    "                    print('df_omni_5min_cleaned',df_omni_5min_cleaned.shape)#,, df_omni_5min_cleaned)\n",
    "\n",
    "        #             #create the indices\n",
    "                    indices = []\n",
    "                    for index in df_omni_5min_cleaned.index:\n",
    "                        for jj in range(0,60*5):\n",
    "                            indices.append(index+ datetime.timedelta(seconds=jj))\n",
    "                    print('indices',len(indices))#, indices)\n",
    "\n",
    "                    df_omni_1min_cleaned = pd.DataFrame(data=df_omni_1min_cleaned.values,columns=df_omni_5min_cleaned.columns,\n",
    "                                                        index=indices)    \n",
    "                    print('df_omni_5min_cleaned',df_omni_5min_cleaned.shape)#, df_omni_5min_cleaned)\n",
    "\n",
    "                    intersection_indices = df_omni_1min_cleaned.index.intersection(df.index)\n",
    "                    print('intersection_indices',intersection_indices.shape)#,, intersection_indices)\n",
    "\n",
    "                    df = df.loc[intersection_indices]\n",
    "                    print('df',df.shape)#, df)\n",
    "                    df_omni_1min_cleaned = df_omni_1min_cleaned.loc[intersection_indices]\n",
    "                    print('df_omni_5min_cleaned',df_omni_5min_cleaned.shape)#, df_omni_5min_cleaned)\n",
    "\n",
    "                    #combine Omni and DMSP data\n",
    "                    for feature in df_omni_1min_cleaned.columns:\n",
    "                        df[feature] = df_omni_1min_cleaned[feature]  \n",
    "\n",
    "                    sc_df = pd.concat([sc_df,df])\n",
    "                    print('sc_df',sc_df.shape)#, sc_df)\n",
    "#                 except Exception as e: \n",
    "#                     print('Error')\n",
    "#                     print(e)\n",
    "#                     print(file)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            sc_df['SC_ID']= ii\n",
    "\n",
    "            #filescname = \"./all_sc_df_1-sec_2010-\" + str(ii) + \".pkl\"\n",
    "            #pickling_on = open(filescname,\"wb\")\n",
    "            #pickle.dump(sc_df, pickling_on,protocol=4)\n",
    "\n",
    "            all_sc_df = pd.concat([all_sc_df,sc_df])   \n",
    "            #print(i, 'all_sc_df',all_sc_df.shape, all_sc_df)\n",
    "\n",
    "    test=all_sc_df\n",
    "    test['datetime']=test.index\n",
    "    test=test.sort_values(by=['datetime'])\n",
    "    test = test.set_index('datetime')\n",
    "\n",
    "    # get rid of ones near equatoer\n",
    "    test=test[np.abs(test['SC_AACGM_LAT'])>45]\n",
    "    # swap by for southern hemisphere\n",
    "    test.loc[test['SC_AACGM_LAT']<0 , 'By'] = -test.loc[test['SC_AACGM_LAT']<0 , 'By']\n",
    "    #combine southern with northern hemisphere data\n",
    "    test['SC_AACGM_LAT']=np.abs(test['SC_AACGM_LAT'])\n",
    "\n",
    "    test['cos_SC_AACGM_LTIME']=np.cos(test['SC_AACGM_LTIME']*2*3.14159/24)\n",
    "    test['sin_SC_AACGM_LTIME']=np.sin(test['SC_AACGM_LTIME']*2*3.14159/24)\n",
    "\n",
    "    doy_loop = test.index.day\n",
    "    ut_loop = test.index.hour*3600 + test.index.minute*60 + test.index.second\n",
    "    test['sin_doy']= np.sin(2*np.pi*doy_loop/365.)\n",
    "    test['cos_doy'] = np.cos(2*np.pi*doy_loop/365.)\n",
    "    test['sin_ut'] = np.sin(2*np.pi*ut_loop/86400.)\n",
    "    test['cos_ut'] = np.cos(2*np.pi*ut_loop/86400.)\n",
    "\n",
    "    del doy_loop,ut_loop\n",
    "    cdf\n",
    "    return test    \n",
    "\n",
    "\n",
    "dpi=200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_multiple(df_results,model_names, y_val_log):\n",
    "    \n",
    "    num = len(df_results)\n",
    "    names = ['val']\n",
    "    for i in range(0,num):\n",
    "        names.append(model_names[i])\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(y_val_log[:],alpha = 0.5)\n",
    "    for i in range(0,num):\n",
    "        plt.plot(df_results[i],alpha = 0.5)\n",
    "    plt.legend(names, loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(10**y_val_log[:]*1.6e-6,alpha = 0.5)\n",
    "    for i in range(0,num):\n",
    "        plt.plot(10**df_results[i]*1.6e-6,alpha = 0.5)\n",
    "    plt.legend(names, loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(10**y_val_log.values*1.6e-6,bins=200, range=(10**6*1.6e-6,10**14*1.6e-6), log=True,alpha = 0.5)\n",
    "    for i in range(0,num):\n",
    "        plt.hist(10**df_results[i].values*1.6e-6,bins=200, log=True,alpha = 0.5)\n",
    "    plt.legend(names, loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(y_val_log.values,bins=200,range=(6,14),alpha = 0.5)\n",
    "    for i in range(0,num):  \n",
    "        plt.hist(df_results[i].values,bins=200,range=(6,14),alpha = 0.5)\n",
    "    plt.legend(names, loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(y_val_log.values,bins=200,range=(6,14))\n",
    "    for k in range(0,num):\n",
    "        plt.hist(df_results[k].values,bins=200,range=(6,14),alpha = 0.5)\n",
    "    plt.legend(names, loc='upper left')\n",
    "    plt.ylim([0,2000])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure();\n",
    "    for k in range(0,num):     \n",
    "        errors= y_val_log.values-df_results[k].values[:,0]\n",
    "        plt.scatter(y_val_log.values,errors,s=1,alpha = 0.5)\n",
    "    plt.title('Errors over target Bins')\n",
    "    plt.legend(model_names, loc='upper left')                      \n",
    "    plt.show()\n",
    "\n",
    "    plt.figure();\n",
    "    for k in range(0,num):\n",
    "        errors= y_val_log.values-df_results[k].values[:,0]     \n",
    "        bin_total = np.zeros((200))\n",
    "        bin_error_total = np.zeros((200))\n",
    "        for j in range(0,y_val_log.values.shape[0]):\n",
    "            i = int((y_val_log[j]-6)/((14-6)/200))\n",
    "            if i < 200:\n",
    "                bin_total[i] = bin_total[i]+1\n",
    "                bin_error_total[i] = bin_error_total[i] + np.abs(errors[j])\n",
    "        avg_error_over_hist = bin_error_total/(bin_total+.00001)\n",
    "        plt.scatter(np.linspace(6,14,num=200),avg_error_over_hist,alpha = 0.5)\n",
    "    plt.title('Average Validation Error over target Bins')    \n",
    "    plt.legend(model_names, loc='upper left')\n",
    "    plt.ylim([0,2.6])\n",
    "    plt.show()\n",
    "\n",
    "    for k in range(0,num):  \n",
    "        plt.figure();\n",
    "\n",
    "        errors= y_val_log.values-df_results[k].values[:,0]\n",
    "        plt.hist2d(y_val_log.values,errors,bins=200)\n",
    "        plt.title('Errors over target Bins'+model_names[k])\n",
    "        plt.show()\n",
    "        \n",
    "    plt.figure();\n",
    "    for k in range(0,num):      \n",
    "        errors= y_val_log.values-df_results[k].values[:,0]     \n",
    "        plt.scatter(df_val['SC_AACGM_LAT'].values,errors,s=1,alpha = 0.5)\n",
    "    plt.legend(model_names, loc='upper left')                      \n",
    "    plt.title('Errors over SC_AACGM_LAT Bins')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure();\n",
    "    for k in range(0,num):\n",
    "        errors= y_val_log.values-df_results[k].values[:,0]     \n",
    "        bin_total = np.zeros((200))\n",
    "        bin_error_total = np.zeros((200))\n",
    "        for j in range(0,y_val_log.values.shape[0]):\n",
    "            i = int((df_val['SC_AACGM_LAT'].values[j]-45)/((90-45)/200))\n",
    "            if i < 200:\n",
    "                bin_total[i] = bin_total[i]+1\n",
    "                bin_error_total[i] = bin_error_total[i] + np.abs(errors[j])\n",
    "        avg_error_over_hist = bin_error_total/bin_total\n",
    "        plt.scatter(np.linspace(45,90,num=200),avg_error_over_hist,alpha = 0.5)\n",
    "    plt.title('Average Validation Error over SC_AACGM_LAT Bins')\n",
    "    plt.legend(model_names, loc='upper left')\n",
    "    plt.ylim([0,1.2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.system('source /home/jackalak/heartbeat/cdf38_0-dist/bin/definitions.B')\n",
    "os.environ[\"CDF_LIB\"] = '/home/jackalak/heartbeat/cdf38_0-dist/lib'\n",
    "from spacepy import pycdf\n",
    "\n",
    "from ovationpyme.ovation_prime import FluxEstimator,AverageEnergyEstimator,BinCorrector\n",
    "from ovationpyme.ovation_utilities import calc_avg_solarwind\n",
    "from ovationpyme.ovation_plotting import latlt2polar,polar2dial,pcolor_flux\n",
    "    \n",
    "def plot_model_multiple_polar(models,model_names, scalars, model_input_keys, dims, model_pipeline,df_val_inputs):\n",
    "    for pt in range(0,df_val_inputs.shape[0]):\n",
    "                                          \n",
    "        num = len(models)\n",
    "\n",
    "        subplotlist=[]\n",
    "        fig= plt.figure(figsize=(12,10))\n",
    "        subplotlist.append(plt.subplot2grid((3,2), (0,0), rowspan=1,colspan=1,polar=True) )\n",
    "        subplotlist.append( plt.subplot2grid((3,2), (0, 1), rowspan=1,colspan=1,polar=True) ) \n",
    "        subplotlist.append( plt.subplot2grid((3,2), (1,0), rowspan=1,colspan=1,polar=True))\n",
    "        subplotlist.append( plt.subplot2grid((3,2), (1, 1), rowspan=1,colspan=1,polar=True))\n",
    "        subplotlist.append( plt.subplot2grid((3,2), (2,0), rowspan=1,colspan=1,polar=True))\n",
    "        subplotlist.append( plt.subplot2grid((3,2), (2, 1), rowspan=1,colspan=1,polar=True))\n",
    "        subplotlist.append( plt.subplot2grid((3,2), (3,0), rowspan=1,colspan=1,polar=True))\n",
    "        subplotlist.append( plt.subplot2grid((3,2), (3, 1), rowspan=1,colspan=1,polar=True))    \n",
    "\n",
    "\n",
    "        mark= ['s', 'o', 'D', 'v']\n",
    "\n",
    "#         %matplotlib inline\n",
    "#         plt.ioff()\n",
    "\n",
    "        pcolor_kwargs = {'cmap':'gnuplot','vmin':7.5,'vmax':13}\n",
    "\n",
    "        for i in range(0,num):\n",
    "            print(model_names[i])\n",
    "            model = models[i]\n",
    "            X_test=df_val_inputs[model_input_keys[i]][pt:pt+1]\n",
    "\n",
    "            \n",
    "            if (dims[i] == 2):\n",
    "                \n",
    "                X_test= scalars[i].transform(X_test.values)\n",
    "                results = model.predict(X_test)\n",
    "                mlatgridN = np.linspace(90,45,num=128)\n",
    "                mltgridN =  np.linspace(0,24,num=128)   \n",
    "                flux = 10**(results[0,:,:,0])*1.60218e-12*3.14159\n",
    "\n",
    "                ax=subplotlist[i+1]\n",
    "                mappableN = pcolor_flux(ax,mlatgridN,mltgridN,np.log10(flux/1.60218e-12),'N',**pcolor_kwargs)\n",
    "\n",
    "                ax.set_title(model_names[i],fontweight=\"bold\", fontsize='medium',pad=10)\n",
    "\n",
    "\n",
    "                ax.set_theta_zero_location('S')\n",
    "                theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "                theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "                ax.set_thetagrids(theta_label_values,labels=theta_labels)\n",
    "\n",
    "                r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "                r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "                ax.set_rgrids(r_label_values,labels=r_labels)\n",
    "                ax.set_rlim([0.,45.])\n",
    "\n",
    "#                 ax.scatter(np.linspace(90,45,128),np.log10(flux[:,int(23/24*128)]/1.60218e-12), marker=mark[3])\n",
    "            else:\n",
    "                \n",
    "                \n",
    "                mlatgridN = np.linspace(90,45,num=128)\n",
    "                mltgridN =  np.linspace(0,24,num=128)          \n",
    "\n",
    "                #################################\n",
    "                #\n",
    "                #ML model\n",
    "                ##########################\n",
    "                model_input = np.zeros((128,128,len(model_input_keys[i])))\n",
    "                flux = np.zeros((mlatgridN.shape[0], mltgridN.shape[0]))\n",
    "\n",
    "\n",
    "                for j in range(0,mlatgridN.shape[0]):\n",
    "                    for k in range(0,mltgridN.shape[0]):        \n",
    "                        #calc cos and sin\n",
    "                        rads = mltgridN[k]*15*3.14159/180.\n",
    "                        model_input[j,k,:]=X_test.values\n",
    "                        if (model_pipeline[i] ==1) :\n",
    "                            model_input[j,k,7]=np.cos(rads)\n",
    "                            model_input[j,k,6]=np.sin(rads)\n",
    "                            model_input[j,k,0]=mlatgridN[j]\n",
    "                        else:\n",
    "                            model_input[j,k,142]=np.cos(rads)\n",
    "                            model_input[j,k,143]=np.sin(rads)\n",
    "                            model_input[j,k,0]=mlatgridN[j]          \n",
    "                shaped = np.reshape(model_input,(128*128,len(model_input_keys[i])))\n",
    "                X_val_scaled = scalars[i].transform(shaped)\n",
    "                #get auroral region and flux\n",
    "                flux = np.log10(10**(np.reshape(model.predict(X_val_scaled),(128,128)))*3.14159)\n",
    "        \n",
    "                ax=subplotlist[i+1]\n",
    "                mappableN = pcolor_flux(ax,mlatgridN,mltgridN,flux,'N',**pcolor_kwargs)\n",
    "\n",
    "                ax.set_title(model_names[i],fontweight=\"bold\", fontsize='medium',pad=10)\n",
    "\n",
    "\n",
    "                ax.set_theta_zero_location('S')\n",
    "                theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "                theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "                ax.set_thetagrids(theta_label_values,labels=theta_labels)\n",
    "\n",
    "                r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "                r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "                ax.set_rgrids(r_label_values,labels=r_labels)\n",
    "                ax.set_rlim([0.,45.])\n",
    "\n",
    "#                 ax.scatter(np.linspace(90,45,128),np.log10(flux[:,int(23/24*128)]/1.60218e-12), marker=mark[3])\n",
    "\n",
    "\n",
    "\n",
    "#         range1=np.min(np.min([-df_val['AL'].values,df_val['AE'].values,df_val['AU'].values]))\n",
    "#         range2=np.max(np.max([-df_val['AL'].values,df_val['AE'].values,df_val['AU'].values]))\n",
    "\n",
    "\n",
    "        ###########################3\n",
    "        # Ovation128\n",
    "        ########################\n",
    "        dt =    datetime.datetime( df_val_inputs.index[pt].date().timetuple().tm_year, df_val_inputs.index[pt].date().timetuple().tm_mon,\n",
    "                                 df_val_inputs.index[pt].date().timetuple().tm_mday, df_val_inputs.index[pt].date().timetuple().tm_hour, \n",
    "                                  df_val_inputs.index[pt].date().timetuple().tm_min) #year,month,day,hour,minute)\n",
    "        #df_val_inputs.index[pt].date()\n",
    "        print(dt)\n",
    "        auroral_types = ['diff','mono','wave','ions']\n",
    "        # axS = f.add_subplot(122,projection='polar')\n",
    "        for jj in range(0,3):\n",
    "            atype = auroral_types[jj]\n",
    "            jtype =\"energy\"\n",
    "            bincorrect = True\n",
    "            combine_hemispheres = True\n",
    "            dtstr = dt.strftime('%Y%m%d %H:%M')\n",
    "            if jtype=='average energy':\n",
    "                estimator = AverageEnergyEstimator(atype)\n",
    "                get_precip_for_time = estimator.get_eavg_for_time\n",
    "            else:\n",
    "                estimator = FluxEstimator(atype,jtype)\n",
    "                get_precip_for_time = estimator.get_flux_for_time\n",
    "\n",
    "\n",
    "            tflux_kwargs = {'combine_hemispheres':combine_hemispheres,\n",
    "                            'return_dF':True}\n",
    "            fluxtupleN = get_precip_for_time(dt,hemi='N',**tflux_kwargs)\n",
    "            mlatgridN,mltgridN,fluxgridN,newell_coupling = fluxtupleN\n",
    "            fluxtupleS = get_precip_for_time(dt,hemi='S',**tflux_kwargs)\n",
    "            mlatgridS,mltgridS,fluxgridS,newell_coupling = fluxtupleS\n",
    "\n",
    "            if bincorrect:\n",
    "                bcN = BinCorrector(mlatgridN,mltgridN)\n",
    "                fluxgridN = bcN.fix(fluxgridN)\n",
    "                bcS = BinCorrector(mlatgridS,mltgridS)\n",
    "                fluxgridS = bcS.fix(fluxgridS)\n",
    "                print(\"Correction Applied\")\n",
    "\n",
    "            if jj== 0:\n",
    "                fluxgridN_sum = fluxgridN\n",
    "                fluxgridS_sum = fluxgridS\n",
    "            else:\n",
    "                fluxgridN_sum = fluxgridN_sum+fluxgridN\n",
    "                fluxgridS_sum = fluxgridN_sum+fluxgridS\n",
    "\n",
    "        mappableN = pcolor_flux( subplotlist[0],mlatgridN,mltgridN,np.log10(fluxgridN_sum/1.60218e-12),'N',**pcolor_kwargs)\n",
    "\n",
    "        subplotlist[0].set_title('OVATION Pyme',pad =10,fontweight=\"bold\", fontsize='medium')\n",
    "\n",
    "\n",
    "        subplotlist[0].set_theta_zero_location('S')\n",
    "        theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "        theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "        subplotlist[0].set_thetagrids(theta_label_values,labels=theta_labels,fontsize='medium', )\n",
    "\n",
    "        r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "        r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "        subplotlist[0].set_rgrids(r_label_values,labels=r_labels)\n",
    "        subplotlist[0].set_rlim([0.,45.])\n",
    "\n",
    "#         subplotlist[0].scatter(np.ones((20))*23/24*2*3.14159 ,                   np.linspace(0,40,20)      )     \n",
    "\n",
    "        plt.colorbar(mappableN,ax=subplotlist[0],label='Total Energy Flux [erg/cm^s/s]')        \n",
    "\n",
    "        fig.tight_layout() \n",
    "#         fig.set_title('Log10 Predicted Electron Precipitation Energy Flux')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "model_names = []\n",
    "scalars = []\n",
    "model_input_keys = []\n",
    "dims = [2,2,1,1,2,1,1]\n",
    "model_pipeline = [1,1,1,1,1,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "2013\n",
      "lrwxrwxrwx   1 rcandey  cdaweb         18 Sep 26  2012 000_readme.txt -> /pub/000_readme.txt\n",
      "lrwxrwxrwx   1 rcandey  cdaweb         18 May  5  2017 000test -> /pub/000_readme.txt\n",
      "lrwxrwxrwx   1 rcandey  cdaweb         17 Oct  5  2018 datasets.json -> /pub/datasets.json\n",
      "lrwxrwxrwx   1 tkovalic cdaweb         24 Sep 25  2012 filelist.gz -> /pub/catalogs/filelist.gz\n",
      "lrwxrwxrwx   1 tkovalic cdaweb         30 Sep 25  2012 filelist.patch.gz -> /pub/catalogs/filelist.patch.gz\n",
      "lrwxrwxrwx   1 tkovalic cdaweb         21 Sep 25  2012 ls-lR.gz -> /pub/catalogs/ls-lR.gz\n",
      "lrwxrwxrwx   1 tkovalic cdaweb         27 Sep 25  2012 ls-lR.patch.gz -> /pub/catalogs/ls-lR.patch.gz\n",
      "drwxrwxr-x  11 tkovalic cdaweb         15 May 11 10:08 pub\n",
      "-rwxr-xr--   1 tkovalic cdaweb       1457 Sep 27  2019 robots.txt\n",
      "already exists: pub/data/dmsp/dmspf17/ssj/precipitating-electrons-ions/2013/dmsp-f17_ssj_precipitating-electrons-ions_20130317_v1.1.2.cdf\n",
      "pub/data/dmsp/dmspf17/ssj/precipitating-electrons-ions/2013/dmsp-f17_ssj_precipitating-electrons-ions_20130317_v1.1.2.cdf\n",
      "df (86400, 46)\n",
      "(105120,)\n",
      "Created interval between 2013-03-16 and 2013-03-18, cadence 5min, start index 21528, end index 21960\n",
      "(8760,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in less\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created interval between 2013-03-16 and 2013-03-18, cadence hourly, start index 1794, end index 1830\n",
      "Applying transform Hourly Kp*10 -> Kp to omni hourly variable KP\n",
      "df_omni_5min (432, 16)\n",
      "indices (432,)\n",
      "df_omni_5min (432, 16)\n",
      "df_omni_5min_cleaned (432, 160)\n",
      "df_omni_5min_cleaned (432, 160)\n",
      "indices 129600\n",
      "df_omni_5min_cleaned (432, 160)\n",
      "intersection_indices (81881,)\n",
      "df (81881, 46)\n",
      "df_omni_5min_cleaned (432, 160)\n",
      "sc_df (81881, 206)\n"
     ]
    }
   ],
   "source": [
    "df_val = get_data( datetime.datetime(2013, 3, 17),datetime.datetime(2013, 3, 17),17)\n",
    "df_val=df_val.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "  \n",
    "    loss = K.sum( K.cast(K.greater(y_true, 0),'float64')*\n",
    "               K.square(y_true-y_pred) )  / ( \n",
    "                K.sum( K.cast(K.greater(y_true, 0),'float64') ))# finds the number of y_true  > 0\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    "    \n",
    "\n",
    "    \n",
    "    mse = K.sum( K.cast(K.greater(y_true, 0),'float64')*(\n",
    "        K.square(y_true-y_pred)) )/params['batch_size']  \n",
    "    return mse\n",
    "\n",
    "def for_CSV_val(scaler_X, model, features, test, XX_test):\n",
    "\n",
    "\n",
    "    num = test.shape[0]\n",
    "    \n",
    "\n",
    "    result_val = []\n",
    "\n",
    "    \n",
    "    for i in range(0,num):\n",
    "\n",
    "\n",
    "       #################################\n",
    "        #\n",
    "        #ML model\n",
    "        ##########################\n",
    "        results = model.predict( XX_test[i:i+1,:])\n",
    "        flux = results[0,:,:,0]             \n",
    "        pt = i\n",
    "\n",
    "        result_val.append( \n",
    "            flux[int((90-test['SC_AACGM_LAT'][pt])/45*128),int((test['SC_AACGM_LTIME'][pt])/24*128)] )     \n",
    "    \n",
    "        \n",
    "\n",
    "    return result_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_145.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "filename='mse_pipeline1_33_2D_128x128_periodic_dropout_float64'\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "keys = ['SC_ID', 'sin_ut','cos_ut', 'sin_doy', 'cos_doy', \n",
    "                     'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', \n",
    "    'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr',\n",
    "    'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr',\n",
    "    'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr',\n",
    "    'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr',\n",
    "    'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr',\n",
    "    'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr',\n",
    "    'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', \n",
    "    'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', \n",
    "    'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', \n",
    "    'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', \n",
    "    'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', \n",
    "    'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', \n",
    "    'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', \n",
    "    'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', \n",
    "    'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', \n",
    "    'borovsky_5min', 'newell_5min']\n",
    "\n",
    "X_val = (df_val[keys])\n",
    "\n",
    "X_val_scaled = scaler_X.transform(X_val.values)\n",
    "\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val_log = df_val['ELE_TOTAL_ENERGY_FLUX']\n",
    "# # y_val_log['ELE_TOTAL_ENERGY_FLUX'] = np.array(y_val_log,dtype='float64')\n",
    "# # print(df_val)\n",
    "\n",
    "# y_val_log_90= pd.DataFrame(data=y_val_log.values, index = y_val_log.index, columns = ['energy'])\n",
    "# y_val_log_90 = y_val_log_90.nlargest(int(0.1*y_val_log_90.shape[0]),columns=['energy'])\n",
    "# X_val_90 = pd.DataFrame(data=X_val_scaled, index = y_val_log.index)\n",
    "# X_val_90 = X_val_90.loc[y_val_log_90.index,:]\n",
    "# df_val_90 = df_val.loc[y_val_log_90.index,:]\n",
    "# print('90% percentile', np.min(y_val_log_90.values),np.max(y_val_log_90.values))\n",
    "\n",
    "# y_val_log_95= pd.DataFrame(data=y_val_log.values, index = y_val_log.index, columns = ['energy'])\n",
    "# y_val_log_95 = y_val_log_95.nlargest(int(0.05*y_val_log_95.shape[0]),columns=['energy'])\n",
    "# X_val_95 = pd.DataFrame(data=X_val_scaled, index = y_val_log.index)\n",
    "# X_val_95 = X_val_95.loc[y_val_log_95.index,:]\n",
    "# df_val_95 = df_val.loc[y_val_log_95.index,:]\n",
    "\n",
    "# print('95% percentile', np.min(y_val_log_95.values),np.max(y_val_log_95.values))\n",
    "\n",
    "# y_val_log_99= pd.DataFrame(data=y_val_log.values, index = y_val_log.index, columns = ['energy'])\n",
    "# y_val_log_99 = y_val_log_99.nlargest(int(0.01*y_val_log_99.shape[0]),columns=['energy'])\n",
    "# X_val_99 = pd.DataFrame(data=X_val_scaled, index = y_val_log.index)\n",
    "# X_val_99 = X_val_99.loc[y_val_log_99.index,:]\n",
    "# df_val_99 = df_val.loc[y_val_log_99.index,:]\n",
    "# print('99% percentile', np.min(y_val_log_99.values),np.max(y_val_log_99.values))\n",
    "\n",
    "# # model.evaluate(X_val_90.values,y_val_log_90.values.astype(float))\n",
    "# results_2d_pipeline1_90 = pd.DataFrame(data=for_CSV_val(0, model, 0, df_val_90, X_val_90.values), index = y_val_log_90.index)\n",
    "# np.mean((results_2d_pipeline1_90-y_val_log_90)**2)\n",
    "# # model.evaluate(X_val_95.values,y_val_log_95.values.astype(float))\n",
    "# results_2d_pipeline1_95 = pd.DataFrame(data=for_CSV_val(0, model, 0, df_val_90, X_val_95.values), index = y_val_log_95.index)\n",
    "# np.mean((results_2d_pipeline1_95-y_val_log_95)**2)\n",
    "# # model.evaluate(X_val_99.values,y_val_log_99.values.astype(float))\n",
    "# results_2d_pipeline1_90 = pd.DataFrame(data=for_CSV_val(0, model, 0, df_val_90, X_val_90.values), index = y_val_log_90.index)\n",
    "# np.mean((results_2d_pipeline1_95-y_val_log_95)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = for_CSV_val(0, model, 0, df_val, X_test)\n",
    "\n",
    "results_2d_pipeline1 = pd.DataFrame(data=results, index = df_val.index)\n",
    "\n",
    "\n",
    "# filename ='results_2d_pipeline2_eval.dat'\n",
    "# results_2d_pipeline2_eval.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_145.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "filename='mse_new_pipeline_145_2D_128x128_periodic_dropout_tailloss_pipline1'\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "keys = ['SC_ID', 'sin_ut','cos_ut', 'sin_doy', 'cos_doy', \n",
    "                     'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', \n",
    "    'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr',\n",
    "    'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr',\n",
    "    'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr',\n",
    "    'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr',\n",
    "    'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr',\n",
    "    'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr',\n",
    "    'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', \n",
    "    'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', \n",
    "    'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', \n",
    "    'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', \n",
    "    'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', \n",
    "    'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', \n",
    "    'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', \n",
    "    'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', \n",
    "    'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', \n",
    "    'borovsky_5min', 'newell_5min']\n",
    "\n",
    "X_val = (df_val[keys])\n",
    "\n",
    "X_val_scaled = scaler_X.transform(X_val.values)\n",
    "\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)\n",
    "\n",
    "results = for_CSV_val(0, model, 0, df_val, X_test)\n",
    "\n",
    "results_2d_tailloss_pipeline1 = pd.DataFrame(data=results, index = df_val.index)\n",
    "\n",
    "\n",
    "# filename ='results_2d_pipeline2_eval.dat'\n",
    "# results_2d_pipeline2_eval.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "keys = ['SC_AACGM_LAT', 'SC_ID', 'sin_ut', 'cos_ut', 'sin_doy', 'cos_doy', \n",
    "        'sin_SC_AACGM_LTIME', 'cos_SC_AACGM_LTIME', 'F107', 'Bz', 'By', 'Bx', 'AE', 'AL',\n",
    "        'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', 'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr', 'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr', 'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', 'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', 'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', 'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', 'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'borovsky_5min', 'newell_5min']\n",
    "\n",
    "X_val_scaled = scaler_X.transform(df_val[keys].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "filename='old_pipeline1_148'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "model_3_pipeline1 = pd.DataFrame(data=results, index =df_val.index)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)\n",
    "\n",
    "# filename ='model_3_pipeline1_pipeline2_eval.dat'\n",
    "# model_3_pipeline1_pipeline2_eval.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "keys = ['SC_AACGM_LAT', 'SC_ID', 'sin_ut', 'cos_ut', 'sin_doy', 'cos_doy', \n",
    "        'sin_SC_AACGM_LTIME', 'cos_SC_AACGM_LTIME', 'F107', 'Bz', 'By', 'Bx', 'AE', 'AL',\n",
    "        'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', 'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr', 'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr', 'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', 'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', 'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', 'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', 'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'borovsky_5min', 'newell_5min']\n",
    "\n",
    "X_val_scaled = scaler_X.transform(df_val[keys].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "filename='model_3_dist_tail3_2'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "model_3_dist_tail_pipeline1 = pd.DataFrame(data=results, index =df_val.index)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)\n",
    "\n",
    "# filename ='model_3_pipeline1_pipeline2_eval.dat'\n",
    "# model_3_pipeline1_pipeline2_eval.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_145.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "keys=['SC_ID', 'sin_ut','cos_ut', 'sin_doy', 'cos_doy', \n",
    "                     'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', \n",
    "    'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr',\n",
    "    'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr',\n",
    "    'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr',\n",
    "    'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr',\n",
    "    'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr',\n",
    "    'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr',\n",
    "    'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', \n",
    "    'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', \n",
    "    'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', \n",
    "    'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', \n",
    "    'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', \n",
    "    'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', \n",
    "    'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', \n",
    "    'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', \n",
    "    'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', \n",
    "    'borovsky_5min', 'newell_5min'\n",
    "]\n",
    "X_val = (df_val[keys])\n",
    "\n",
    "X_val_scaled = scaler_X.transform(X_val.values)\n",
    "\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "\n",
    "filename='results_2d_no_window_pipeline1'\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)\n",
    "\n",
    "# filename ='results_2d_no_window_pipeline2_eval.dat'\n",
    "# results_2d_no_window_pipeline2_eval.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = for_CSV_val(scaler_X, model, X_val.columns, df_val, X_test)\n",
    "\n",
    "results_2d_no_window_pipeline1 = pd.DataFrame(data=results, index = X_val.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline2_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "keys = ['SC_AACGM_LAT', 'Bz', 'By', 'Vsw', 'Vx', 'Psw', 'AE', 'AL', 'AU', 'SymH', 'newell', 'borovsky', 'F107', 'PC', 'Bx', 'Bz_6hr', 'By_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'newell_6hr', 'borovsky_6hr', 'F107_6hr', 'PC_6hr', 'Bx_6hr', 'Bz_5hr', 'By_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'newell_5hr', 'borovsky_5hr', 'F107_5hr', 'PC_5hr', 'Bx_5hr', 'Bz_3hr', 'By_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'newell_3hr', 'borovsky_3hr', 'F107_3hr', 'PC_3hr', 'Bx_3hr', 'Bz_1hr', 'By_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'newell_1hr', 'borovsky_1hr', 'F107_1hr', 'PC_1hr', 'Bx_1hr', 'Bz_45min', 'By_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'newell_45min', 'borovsky_45min', 'F107_45min', 'PC_45min', 'Bx_45min', 'Bz_30min', 'By_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'newell_30min', 'borovsky_30min', 'F107_30min', 'PC_30min', 'Bx_30min', 'Bz_15min', 'By_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'newell_15min', 'borovsky_15min', 'F107_15min', 'PC_15min', 'Bx_15min', 'Bz_10min', 'By_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'newell_10min', 'borovsky_10min', 'F107_10min', 'PC_10min', 'Bx_10min', 'Bz_5min', 'By_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'newell_5min', 'borovsky_5min', 'F107_5min', 'PC_5min', 'Bx_5min', 'SC_ID', 'cos_SC_AACGM_LTIME', 'sin_SC_AACGM_LTIME', 'sin_doy', 'cos_doy', 'sin_ut', 'cos_ut']\n",
    "X_val_scaled = scaler_X.transform(df_val[keys].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "filename='new_pipeline2_148'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "df_results = pd.DataFrame(data=results, index = df_val.index)\n",
    "\n",
    "model_3_pipeline2 = df_results\n",
    "\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline2_15sec_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "keys=['SC_AACGM_LAT', 'Bz', 'By', 'Vsw', 'Vx', 'Psw', 'AE', 'AL', 'AU', 'SymH', 'newell', 'borovsky', 'F107', 'PC', 'Bx', 'Bz_6hr', 'By_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'newell_6hr', 'borovsky_6hr', 'F107_6hr', 'PC_6hr', 'Bx_6hr', 'Bz_5hr', 'By_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'newell_5hr', 'borovsky_5hr', 'F107_5hr', 'PC_5hr', 'Bx_5hr', 'Bz_3hr', 'By_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'newell_3hr', 'borovsky_3hr', 'F107_3hr', 'PC_3hr', 'Bx_3hr', 'Bz_1hr', 'By_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'newell_1hr', 'borovsky_1hr', 'F107_1hr', 'PC_1hr', 'Bx_1hr', 'Bz_45min', 'By_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'newell_45min', 'borovsky_45min', 'F107_45min', 'PC_45min', 'Bx_45min', 'Bz_30min', 'By_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'newell_30min', \n",
    "      'borovsky_30min', 'F107_30min', 'PC_30min', 'Bx_30min', 'Bz_15min', 'By_15min', 'Vsw_15min', 'Vx_15min', \n",
    "      'Psw_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'newell_15min', 'borovsky_15min', 'F107_15min',\n",
    "      'PC_15min', 'Bx_15min', 'Bz_10min', 'By_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'AE_10min', 'AL_10min',\n",
    "      'AU_10min', 'SymH_10min', 'newell_10min', 'borovsky_10min', 'F107_10min', 'PC_10min', 'Bx_10min', 'Bz_5min', \n",
    "      'By_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'newell_5min', \n",
    "      'borovsky_5min', 'F107_5min', 'PC_5min', 'Bx_5min', 'SC_ID', 'cos_SC_AACGM_LTIME', 'sin_SC_AACGM_LTIME',\n",
    "      'sin_doy', 'cos_doy', 'sin_ut', 'cos_ut']\n",
    "X_val_scaled = scaler_X.transform(df_val[keys].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "filename='new_pipeline2_148_15sec'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "df_results = pd.DataFrame(data=results, index = df_val.index)\n",
    "\n",
    "\n",
    "model_3_pipeline2_15sec = df_results\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_dpi_ratio', { dpi_ratio: fig.ratio });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "    if (this.ratio !== 1) {\n",
       "        fig.send_message('set_dpi_ratio', { dpi_ratio: this.ratio });\n",
       "    }\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n",
       "    if (this.ResizeObserver === undefined) {\n",
       "        if (window.ResizeObserver !== undefined) {\n",
       "            this.ResizeObserver = window.ResizeObserver;\n",
       "        } else {\n",
       "            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n",
       "            this.ResizeObserver = obs.ResizeObserver;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    this.resizeObserverInstance.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch (cursor) {\n",
       "        case 0:\n",
       "            cursor = 'pointer';\n",
       "            break;\n",
       "        case 1:\n",
       "            cursor = 'default';\n",
       "            break;\n",
       "        case 2:\n",
       "            cursor = 'crosshair';\n",
       "            break;\n",
       "        case 3:\n",
       "            cursor = 'move';\n",
       "            break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = 'image/png';\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.which === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.which;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which !== 17) {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    if (event.altKey && event.which !== 18) {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    if (event.shiftKey && event.which !== 16) {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "\n",
       "///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n",
       "// prettier-ignore\n",
       "var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data']);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.on(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    if (event.target !== this) {\n",
       "        // Ignore bubbled events from children.\n",
       "        return;\n",
       "    }\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager) {\n",
       "        manager = IPython.keyboard_manager;\n",
       "    }\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='fb707e2d-22b9-4543-91b5-341007b5a6c4'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "GridSpec slice would result in no space allocated for subplot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-99283dfde7df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_model_multiple_polar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-b702e43ab064>\u001b[0m in \u001b[0;36mplot_model_multiple_polar\u001b[0;34m(models, model_names, scalars, model_input_keys, dims, model_pipeline, df_val_inputs)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msubplotlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot2grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowspan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolspan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msubplotlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot2grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowspan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolspan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msubplotlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot2grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowspan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolspan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0msubplotlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot2grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowspan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolspan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplot2grid\u001b[0;34m(shape, loc, rowspan, colspan, fig, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     subplotspec = GridSpec(s1, s2).new_subplotspec(loc,\n\u001b[1;32m   1421\u001b[0m                                                    \u001b[0mrowspan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrowspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m                                                    colspan=colspan)\n\u001b[0m\u001b[1;32m   1423\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubplotspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36mnew_subplotspec\u001b[0;34m(self, loc, rowspan, colspan)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m     96\u001b[0m         \u001b[0mloc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0msubplotspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mloc1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrowspan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mloc2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcolspan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msubplotspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unrecognized subplot spec\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             num1, num2 = np.ravel_multi_index(\n\u001b[0;32m--> 237\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0m_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m                 (nrows, ncols))\n\u001b[1;32m    239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m_normalize\u001b[0;34m(key, size, axis)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 raise IndexError(\"GridSpec slice would result in no space \"\n\u001b[0m\u001b[1;32m    218\u001b[0m                                  \"allocated for subplot\")\n\u001b[1;32m    219\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: GridSpec slice would result in no space allocated for subplot"
     ]
    }
   ],
   "source": [
    "plot_model_multiple_polar(models,model_names, scalars, model_input_keys, dims, model_pipeline,df_val[0:(df_val.shape[0]):3600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = df_val['ELE_TOTAL_ENERGY_FLUX'].copy(deep=True)\n",
    "\n",
    "\n",
    "y_val[y_val == 0] = 0.0001\n",
    "y_val_log = np.log10(y_val.copy(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot((model_3_pipeline2_15sec ))\n",
    "plt.plot((model_3_pipeline1))\n",
    "plt.plot((model_3_pipeline2))\n",
    "plt.plot((results_2d_pipeline1))\n",
    "plt.plot((results_2d_no_window_pipeline1))\n",
    "plt.plot(y_val_log, alpha=.5)\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1','val','y_val_log'])\n",
    "plt.show()\n",
    "\n",
    "filename='march_17_17_1sec_compare.pkl'\n",
    "pickle.dump( (model_3_pipeline2_15sec,\n",
    "              model_3_pipeline1,\n",
    "              model_3_pipeline2,\n",
    "              results_2d_pipeline1,\n",
    "              results_2d_no_window_pipeline1,\n",
    "              y_val_log)\n",
    "            ,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot((model_3_pipeline2_15sec ))\n",
    "plt.plot((model_3_pipeline1))\n",
    "plt.plot((model_3_pipeline2))\n",
    "plt.plot((results_2d_pipeline1))\n",
    "plt.plot((results_2d_no_window_pipeline1))\n",
    "#plt.plot(y_val_log)\n",
    "plt.plot(y_val_log[0:y_val_log.shape[0]:15], alpha=.5)\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1','val','y_val_log'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot((model_3_pipeline2_15sec[0:y_val_log.shape[0]:15] ))\n",
    "plt.plot((model_3_pipeline1[0:y_val_log.shape[0]:15]))\n",
    "plt.plot((model_3_pipeline2[0:y_val_log.shape[0]:15]))\n",
    "plt.plot((results_2d_pipeline1[0:y_val_log.shape[0]:15]))\n",
    "plt.plot((results_2d_no_window_pipeline1[0:y_val_log.shape[0]:15]))\n",
    "#plt.plot(y_val_log)\n",
    "plt.plot(y_val_log[0:y_val_log.shape[0]:15], alpha=.5)\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1','val','y_val_log'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename='march_17_17_1sec_compare.pkl'\n",
    "(model_3_pipeline2_15sec,\n",
    "              model_3_pipeline1,\n",
    "              model_3_pipeline2,\n",
    "              results_2d_pipeline1,\n",
    "              results_2d_no_window_pipeline1,\n",
    "              y_val_log) =   pickle.load(open(filename,'rb'))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot((model_3_pipeline2_15sec ))\n",
    "plt.plot((model_3_pipeline1))\n",
    "plt.plot((model_3_pipeline2))\n",
    "plt.plot((results_2d_pipeline1))\n",
    "plt.plot((results_2d_no_window_pipeline1))\n",
    "plt.plot(y_val_log[0:y_val_log.shape[0]:15], alpha=.5)\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1','val','y_val_log'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_no_window_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1','val','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend([ 'model_3_pipeline1','results_2d_pipeline1','y_val'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "# plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5)\n",
    "# plt.hist((model_3_pipeline1),bins=100,alpha = 0.5)\n",
    "# plt.hist((model_3_pipeline2),bins=100, alpha = 0.5)\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_no_window_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['results_2d_pipeline1','results_2d_no_window_pipeline1','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec','model_3_pipeline2','y_val_log'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_model_multiple([model_3_pipeline2_15sec,model_3_pipeline1,model_3_pipeline2,results_2d_pipeline1,results_2d_no_window_pipeline1],\n",
    "                    ['model_3_pipeline2_15sec','model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1'],y_val_log)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "model_names = []\n",
    "scalars = []\n",
    "model_input_keys = []\n",
    "dims = [2,1,2,1,1]\n",
    "model_pipeline = [1,1,1,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now for 2010 sc 16 veriying\n",
    "\n",
    "# LOAD THE COMBINED SOURTHERN HEMISPHERE AND NORTHERN HEMISPHERE AS CREATED ABOVE\n",
    "df_train = pd.DataFrame()\n",
    "ii=16\n",
    "filescname = \"/home/jackalak/heartbeat/Downloads/sc_df_cleaned_GFi_\" + str(ii) + \".pkl\"\n",
    "pickle_file = open(filescname, \"rb\") \n",
    "df_train=( (pickle.load(pickle_file)) )     \n",
    "df_train = df_train.sort_index()\n",
    "\n",
    "\n",
    "# Separate training and testing data\n",
    "mask_val = [(df_train.index.year == 2010) & (df_train['SC_ID'].values==16)]\n",
    "df_val = df_train[mask_val[0]].copy(deep=True)\n",
    "df_train = df_train.drop( df_train.index[mask_val[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_145.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "filename='mse_pipeline1_33_2D_128x128_periodic_dropout_float64'\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "\n",
    "X_val = (df_val[['SC_ID', 'sin_ut','cos_ut', 'sin_doy', 'cos_doy', \n",
    "                     'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', \n",
    "    'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr',\n",
    "    'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr',\n",
    "    'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr',\n",
    "    'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr',\n",
    "    'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr',\n",
    "    'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr',\n",
    "    'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', \n",
    "    'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', \n",
    "    'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', \n",
    "    'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', \n",
    "    'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', \n",
    "    'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', \n",
    "    'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', \n",
    "    'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', \n",
    "    'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', \n",
    "    'borovsky_5min', 'newell_5min'\n",
    "]])\n",
    "\n",
    "X_val_scaled = scaler_X.transform(X_val.values)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "results = for_CSV_val(0, model, 0, df_val, X_test)\n",
    "\n",
    "results_2d_pipeline1 = pd.DataFrame(data=results, index = df_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "\n",
    "X_val_scaled = scaler_X.transform(df_val[['SC_AACGM_LAT', 'SC_ID', 'sin_ut', 'cos_ut', 'sin_doy', 'cos_doy', 'sin_SC_AACGM_LTIME', 'cos_SC_AACGM_LTIME', 'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', 'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr', 'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr', 'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', 'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', 'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', 'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', 'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'borovsky_5min', 'newell_5min']].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "filename='old_pipeline1_148'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "model_3_pipeline1 = pd.DataFrame(data=results, index =df_val.index)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_145.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "X_val = (df_val[['SC_ID', 'sin_ut','cos_ut', 'sin_doy', 'cos_doy', \n",
    "                     'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', \n",
    "    'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr',\n",
    "    'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr',\n",
    "    'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr',\n",
    "    'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr',\n",
    "    'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr',\n",
    "    'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr',\n",
    "    'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', \n",
    "    'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', \n",
    "    'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', \n",
    "    'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', \n",
    "    'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', \n",
    "    'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', \n",
    "    'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', \n",
    "    'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', \n",
    "    'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', \n",
    "    'borovsky_5min', 'newell_5min'\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_scaled = scaler_X.transform(X_val.values)\n",
    "\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "\n",
    "filename='results_2d_no_window_pipeline1'\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "\n",
    "results = for_CSV_val(scaler_X, model, X_val.columns, df_val, X_test)\n",
    "\n",
    "results_2d_no_window_pipeline1 = pd.DataFrame(data=results, index = X_val.index)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline2_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "\n",
    "X_val_scaled = scaler_X.transform(df_val[['SC_AACGM_LAT', 'Bz', 'By', 'Vsw', 'Vx', 'Psw', 'AE', 'AL', 'AU', 'SymH', 'newell', 'borovsky', 'F107', 'PC', 'Bx', 'Bz_6hr', 'By_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'newell_6hr', 'borovsky_6hr', 'F107_6hr', 'PC_6hr', 'Bx_6hr', 'Bz_5hr', 'By_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'newell_5hr', 'borovsky_5hr', 'F107_5hr', 'PC_5hr', 'Bx_5hr', 'Bz_3hr', 'By_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'newell_3hr', 'borovsky_3hr', 'F107_3hr', 'PC_3hr', 'Bx_3hr', 'Bz_1hr', 'By_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'newell_1hr', 'borovsky_1hr', 'F107_1hr', 'PC_1hr', 'Bx_1hr', 'Bz_45min', 'By_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'newell_45min', 'borovsky_45min', 'F107_45min', 'PC_45min', 'Bx_45min', 'Bz_30min', 'By_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'newell_30min', 'borovsky_30min', 'F107_30min', 'PC_30min', 'Bx_30min', 'Bz_15min', 'By_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'newell_15min', 'borovsky_15min', 'F107_15min', 'PC_15min', 'Bx_15min', 'Bz_10min', 'By_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'newell_10min', 'borovsky_10min', 'F107_10min', 'PC_10min', 'Bx_10min', 'Bz_5min', 'By_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'newell_5min', 'borovsky_5min', 'F107_5min', 'PC_5min', 'Bx_5min', 'SC_ID', 'cos_SC_AACGM_LTIME', 'sin_SC_AACGM_LTIME', 'sin_doy', 'cos_doy', 'sin_ut', 'cos_ut']].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "filename='new_pipeline2_148'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "df_results = pd.DataFrame(data=results, index = df_val.index)\n",
    "\n",
    "model_3_pipeline2 = df_results\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)\n",
    "\n",
    "filename = 'Scalar_X_pipeline2_15sec_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "\n",
    "X_val_scaled = scaler_X.transform(df_val[['SC_AACGM_LAT', 'Bz', 'By', 'Vsw', 'Vx', 'Psw', 'AE', 'AL', 'AU', 'SymH', 'newell', 'borovsky', 'F107', 'PC', 'Bx', 'Bz_6hr', 'By_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'newell_6hr', 'borovsky_6hr', 'F107_6hr', 'PC_6hr', 'Bx_6hr', 'Bz_5hr', 'By_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'newell_5hr', 'borovsky_5hr', 'F107_5hr', 'PC_5hr', 'Bx_5hr', 'Bz_3hr', 'By_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'newell_3hr', 'borovsky_3hr', 'F107_3hr', 'PC_3hr', 'Bx_3hr', 'Bz_1hr', 'By_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'newell_1hr', 'borovsky_1hr', 'F107_1hr', 'PC_1hr', 'Bx_1hr', 'Bz_45min', 'By_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'newell_45min', 'borovsky_45min', 'F107_45min', 'PC_45min', 'Bx_45min', 'Bz_30min', 'By_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'newell_30min', 'borovsky_30min', 'F107_30min', 'PC_30min', 'Bx_30min', 'Bz_15min', 'By_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'newell_15min', 'borovsky_15min', 'F107_15min', 'PC_15min', 'Bx_15min', 'Bz_10min', 'By_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'newell_10min', 'borovsky_10min', 'F107_10min', 'PC_10min', 'Bx_10min', 'Bz_5min', 'By_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'newell_5min', 'borovsky_5min', 'F107_5min', 'PC_5min', 'Bx_5min', 'SC_ID', 'cos_SC_AACGM_LTIME', 'sin_SC_AACGM_LTIME', 'sin_doy', 'cos_doy', 'sin_ut', 'cos_ut']].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "filename='new_pipeline2_148_15sec'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "df_results = pd.DataFrame(data=results, index = df_val.index)\n",
    "\n",
    "\n",
    "model_3_pipeline2_15sec = df_results\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_multiple_polar(models,model_names, scalars, model_input_keys, dims, model_pipeline,df_val[0:(df_val.shape[0]):3600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_val = df_val['ELE_TOTAL_ENERGY_FLUX'].copy(deep=True)\n",
    "\n",
    "\n",
    "y_val[y_val == 0] = 0.0001\n",
    "y_val_log = np.log10(y_val.copy(deep=True))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot((model_3_pipeline2_15sec ))\n",
    "plt.plot((model_3_pipeline1))\n",
    "plt.plot((model_3_pipeline2))\n",
    "plt.plot((results_2d_pipeline1))\n",
    "# plt.plot((results_2d_no_window_pipeline1))\n",
    "plt.plot(y_val_log, alpha=.5)\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','val','y_val_log'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# filename='16_2010_15sec_compare.pkl'\n",
    "# pickle.dump( (model_3_pipeline2_15sec,\n",
    "#               model_3_pipeline1,\n",
    "#               model_3_pipeline2,\n",
    "#               results_2d_pipeline1,\n",
    "#               results_2d_no_window_pipeline1,\n",
    "#               y_val_log)\n",
    "#             ,open(filename,'wb'))\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "#plt.hist((results_2d_no_window_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','val','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend([ 'model_3_pipeline1','results_2d_pipeline1','y_val'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "# # plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5)\n",
    "# # plt.hist((model_3_pipeline1),bins=100,alpha = 0.5)\n",
    "# # plt.hist((model_3_pipeline2),bins=100, alpha = 0.5)\n",
    "# plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "# plt.hist((results_2d_no_window_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "# plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "# plt.legend(['results_2d_pipeline1','results_2d_no_window_pipeline1','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec','model_3_pipeline2','y_val_log'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "# plt.hist((results_2d_no_window_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1','val','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend([ 'model_3_pipeline1','results_2d_pipeline1','y_val'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "# plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5)\n",
    "# plt.hist((model_3_pipeline1),bins=100,alpha = 0.5)\n",
    "# plt.hist((model_3_pipeline2),bins=100, alpha = 0.5)\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_no_window_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['results_2d_pipeline1','results_2d_no_window_pipeline1','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec','model_3_pipeline2','y_val_log'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_model_multiple([model_3_pipeline2_15sec,model_3_pipeline1,model_3_pipeline2,results_2d_pipeline1,results_2d_no_window_pipeline1],\n",
    "                    ['model_3_pipeline2_15sec','model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1'],y_val_log)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_multiple([model_3_pipeline1,model_3_dist_tail_pipeline1,results_2d_pipeline1,results_2d_tailloss_pipeline1],\n",
    "                    ['model_3_pipeline1','model_3_dist_tail_pipeline1','results_2d_pipeline1','results_2d_tailloss_pipeline1'],y_val_log)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "todo:\n",
    "compare to old pipeline no moving window 2d\n",
    "all histogram and avg error over bins\n",
    "all on 2016 pipeline1\n",
    "1 sec compare\n",
    "all on 2016 first day 1 sec\n",
    "all on march test\n",
    "all on jan test\n",
    "\n",
    "original data with distribution weighted loss function\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
