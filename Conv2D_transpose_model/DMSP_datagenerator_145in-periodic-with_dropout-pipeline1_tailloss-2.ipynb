{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# os.system('source /home/jackalak/heartbeat/cdf38_0-dist/bin/definitions.B')\n",
    "# os.environ[\"CDF_LIB\"] = '/home/jackalak/heartbeat/cdf38_0-dist/lib'\n",
    "# from spacepy import pycdf\n",
    "# from nasaomnireader import omnireader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "import ftplib\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.keras.backend.set_floatx(\n",
    "    'float32'\n",
    ")\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "import tensorflow\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import datetime\n",
    "from os.path import isfile, join\n",
    "from sys import getsizeof\n",
    "import glob\n",
    "\n",
    "from random import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/tensorflow/tensorflow/issues/956\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import InputSpec \n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "\n",
    "class PeriodicPadding2D(layers.Layer):\n",
    "\n",
    "  def __init__(self, padding=1, **kwargs):\n",
    "    super(PeriodicPadding2D, self).__init__(**kwargs)\n",
    "    self.padding = conv_utils.normalize_tuple(padding, 1, 'padding')\n",
    "    self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "  def wrap_pad(self, input, size):\n",
    "    M1 = tf.concat([input[:,:, -size:], input, input[:,:, 0:size]], 2)\n",
    "    M1 = tf.concat([M1[:,0:size, :], M1, M1[:,0:size, :]], 1) #not periodic\n",
    "    return M1\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  \n",
    "    if shape[1] is not None:\n",
    "      length = shape[1] + 2*self.padding[0]\n",
    "    else:\n",
    "      length = None\n",
    "    return tuple([shape[0], length, length])\n",
    "\n",
    "  def call(self, inputs): \n",
    "    return self.wrap_pad(inputs, self.padding[0])\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {'padding': self.padding}\n",
    "    base_config = super(PeriodicPadding2D, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(df_results, test):\n",
    "    \n",
    "    subdir = 'figures2/energyflux/'\n",
    "\n",
    "    y_val_log = np.log10(test['ELE_TOTAL_ENERGY_FLUX'])\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Electron Total Energy Flux, log10 scale')    \n",
    "    plt.plot(y_val_log[:],alpha=0.5)\n",
    "    plt.plot(df_results[:],alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('log10 eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'1',dpi=dpi)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Electron Total Energy Flux')   \n",
    "    plt.plot(10**y_val_log[:]*1.6e-6,alpha=0.5)\n",
    "    plt.plot(10**df_results[:]*1.6e-6,alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('erg/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'2',dpi=dpi)\n",
    "    start=int(y_val_log.shape[0]/2)\n",
    "    plt.figure()\n",
    "    plt.title('Electron Total Energy Flux, log10 scale')   \n",
    "    plt.plot(y_val_log[start:2000+start],alpha=0.5)\n",
    "    plt.plot(df_results[start:2000+start],alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('log10 eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'3',dpi=dpi)\n",
    "    plt.figure()\n",
    "    plt.title('Electron Total Energy Flux')   \n",
    "    plt.plot(10**y_val_log[start:2000+start]*1.6e-6,alpha=0.5)\n",
    "    plt.plot(10**df_results[start:2000+start]*1.6e-6,alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('erg/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'4',dpi=dpi)\n",
    "    \n",
    "\n",
    "    minr = np.min(y_val_log.values)\n",
    "    maxr = np.max(y_val_log.values)\n",
    "    plt.figure()\n",
    "    plt.title('Histogram of Electron Total Energy Flux, log10 scale')   \n",
    "    plt.hist(y_val_log.values,bins=200,alpha=0.5,range=(minr,maxr))\n",
    "    plt.hist(df_results.values,bins=200,alpha=0.5,range=(minr,maxr))\n",
    "    plt.legend(['val','result'], loc='upper left')\n",
    "    plt.ylabel('#/bin')\n",
    "    plt.xlabel('log10 eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'5',dpi=dpi)\n",
    "    plt.figure()\n",
    "    plt.title('Histogram of Electron Total Energy Flux')   \n",
    "    plt.hist(10**y_val_log.values*1.6e-6,bins=100, log=True,range=(10**minr*1.6e-6,10**maxr*1.6e-6),alpha=0.5)\n",
    "    plt.hist(10**df_results.values*1.6e-6,bins=100, log=True,range=(10**minr*1.6e-6,10**maxr*1.6e-6),alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('#/bin')\n",
    "    plt.xlabel('erg/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'6',dpi=dpi)\n",
    "    import matplotlib.colors as mcolors\n",
    "    gamma = 0.2#[0.8, 0.5, 0.3]\n",
    "\n",
    "    errors= y_val_log.values-df_results.values[:,0]\n",
    "    plt.figure();\n",
    "    plt.hist2d(test['SC_AACGM_LAT'].values, errors,\n",
    "                  bins=50, norm=mcolors.PowerNorm(gamma))\n",
    "    plt.title('Error density over SC_AACGM_LAT Bins')\n",
    "    plt.xlabel('SC_AACGM_LAT degrees')\n",
    "    plt.ylabel('log10(y_true)-log10(y_pred) eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'7',dpi=dpi)\n",
    "    plt.figure()\n",
    "    bin_total = np.zeros((200))\n",
    "    bin_error_total = np.zeros((200))\n",
    "    for j in range(0,y_val_log.values.shape[0]):\n",
    "        i = int((test['SC_AACGM_LAT'].values[j]-45)/((90-45)/200))\n",
    "        if i < 200:\n",
    "            bin_total[i] = bin_total[i]+1\n",
    "            bin_error_total[i] = bin_error_total[i] + np.abs(errors[j])\n",
    "\n",
    "    avg_error_over_hist = bin_error_total/bin_total\n",
    "    plt.scatter(np.linspace(45,90,num=200),avg_error_over_hist)\n",
    "    plt.title('Average Validation Error over SC_AACGM_LAT Bins')\n",
    "    plt.xlabel('SC_AACGM_LAT degrees')\n",
    "    plt.ylabel('log10(y_true)-log10(y_pred) eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'8',dpi=dpi)\n",
    "    bin_total = np.zeros((200))\n",
    "    bin_error_total = np.zeros((200))\n",
    "    for j in range(0,y_val_log.values.shape[0]):\n",
    "        i = int((y_val_log[j]-minr)/((maxr-minr)/200))\n",
    "        if i < 200:\n",
    "            bin_total[i] = bin_total[i]+1\n",
    "            bin_error_total[i] = bin_error_total[i] + np.abs(errors[j])\n",
    "\n",
    "    avg_error_over_hist = bin_error_total/(bin_total+.00001)\n",
    "    plt.figure()\n",
    "    plt.scatter(np.linspace(minr,maxr,num=200),avg_error_over_hist)\n",
    "    plt.title('Average Validation Error over target Bins')    \n",
    "    plt.xlabel('log10(y_true) eV/cm^2/ster/s')\n",
    "    plt.ylabel('log10(y_true)-log10(y_pred) eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'9',dpi=dpi)\n",
    "    import matplotlib.colors as mcolors\n",
    "    gamma = 0.2#[0.8, 0.5, 0.3]\n",
    "    errors= y_val_log.values-df_results.values[:,0]\n",
    "\n",
    "    plt.figure();\n",
    "    plt.hist2d(y_val_log.values, errors,\n",
    "                  bins=50, norm=mcolors.PowerNorm(gamma))\n",
    "    plt.colorbar()\n",
    "    plt.title('Error Density')\n",
    "    plt.xlabel('log10(y_true) eV/cm^2/ster/s')\n",
    "    plt.ylabel('log10(y_true)-log10(y_pred) eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'10',dpi=dpi)\n",
    "    plt.figure();\n",
    "    plt.hist2d(y_val_log.values, df_results.values[:,0],\n",
    "                  bins=50, norm=mcolors.PowerNorm(gamma))\n",
    "    plt.colorbar()\n",
    "    plt.title('Error Density')\n",
    "    plt.xlabel('log10(y_true) eV/cm^2/ster/s')\n",
    "    plt.ylabel('log10(y_pred) eV/cm^2/ster/s')\n",
    "    temp = np.array([7,8,9,10,11,12,13])\n",
    "    plt.plot(temp,temp,color='k')\n",
    "    plt.xlim([7,13])\n",
    "    plt.ylim([7,13])\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'11',dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # LOAD THE COMBINED SOURTHERN HEMISPHERE AND NORTHERN HEMISPHERE AS CREATED ABOVE\n",
    "# test = pd.DataFrame()\n",
    "\n",
    "# for ii in [17,18,6,7,8,9,12,13,14,15,16,]:\n",
    "\n",
    "#     filescname = \"../Downloads/sc_df_cleaned_GFi_\" + str(ii) + \".pkl\"\n",
    "#     pickle_file = open(filescname, \"rb\") \n",
    "\n",
    "#     test =pd.concat([test,pickle.load(pickle_file)] )\n",
    "    \n",
    "# # filescname = \"./all_sc_df_1min_SH_NH_combined.pkl\"\n",
    "# # pickle_file = open(filescname, \"rb\") \n",
    "# # test =pickle.load(pickle_file)\n",
    "# print(test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SC_AACGM_LAT', 'SC_AACGM_LTIME', 'ELE_TOTAL_ENERGY_FLUX', 'ELE_TOTAL_ENERGY_FLUX_STD', 'ELE_AVG_ENERGY', 'ELE_AVG_ENERGY_STD', 'ID_SC', 'sin_ut', 'cos_ut', 'sin_doy', 'cos_doy', 'sin_SC_AACGM_LTIME', 'cos_SC_AACGM_LTIME', 'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'vsw', 'vx', 'psw', 'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'PC_6hr', 'vsw_6hr', 'vx_6hr', 'psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr', 'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'vsw_5hr', 'vx_5hr', 'psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'PC_3hr', 'vsw_3hr', 'vx_3hr', 'psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr', 'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'vsw_1hr', 'vx_1hr', 'psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'vsw_45min', 'vx_45min', 'psw_45min', 'borovsky_45min', 'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'PC_30min', 'vsw_30min', 'vx_30min', 'psw_30min', 'borovsky_30min', 'newell_30min', 'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'PC_15min', 'vsw_15min', 'vx_15min', 'psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', 'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', 'vsw_10min', 'vx_10min', 'psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', 'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'vsw_5min', 'vx_5min', 'psw_5min', 'borovsky_5min', 'newell_5min']\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LOAD THE COMBINED SOURTHERN HEMISPHERE AND NORTHERN HEMISPHERE AS CREATED ABOVE\n",
    "df_cumulative = pd.DataFrame()\n",
    "# for ii in [17,18,6,7,8,9,12,13,14,15,16,]:\n",
    "#     filescname = \"./sc_df_cleaned_GFi_\" + str(ii) + \".pkl\"\n",
    "#     pickle_file = open(filescname, \"rb\") \n",
    "#     df_cumulative =pd.concat([df_cumulative,pickle.load(pickle_file)] )\n",
    "\n",
    "filescname = \"/home/jackalak/heartbeat_work/HEARTBEAT/ParticlePrecipitation/ML_DB_subsamp_ext_full_dfCumulative_complexHemisphereCombine.csv\"\n",
    "file= open(filescname, \"rb\") \n",
    "df_cumulative =pd.read_csv(file)\n",
    "df_cumulative = df_cumulative.set_index('Datetimes')\n",
    "df_cumulative.index = pd.to_datetime(df_cumulative.index)\n",
    "print(df_cumulative.columns.tolist())\n",
    "\n",
    "df_cumulative=df_cumulative[['SC_AACGM_LAT','SC_AACGM_LTIME', 'ELE_TOTAL_ENERGY_FLUX', 'ID_SC', 'sin_ut',\n",
    "                             'cos_ut', 'sin_doy', 'cos_doy', 'sin_SC_AACGM_LTIME', 'cos_SC_AACGM_LTIME',\n",
    "                               'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'vsw', 'vx', 'psw', 'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'PC_6hr', 'vsw_6hr', 'vx_6hr', 'psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr', 'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'vsw_5hr', 'vx_5hr', 'psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'PC_3hr', 'vsw_3hr', 'vx_3hr', 'psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr', 'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'vsw_1hr', 'vx_1hr', 'psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'vsw_45min', 'vx_45min', 'psw_45min', 'borovsky_45min', 'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'PC_30min', 'vsw_30min', 'vx_30min', 'psw_30min', 'borovsky_30min', 'newell_30min', 'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'PC_15min', 'vsw_15min', 'vx_15min', 'psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', 'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', 'vsw_10min', 'vx_10min', 'psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', 'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'vsw_5min', 'vx_5min', 'psw_5min', 'borovsky_5min', 'newell_5min'        ]]\n",
    "\n",
    "df_cumulative.sort_index()\n",
    "df_cumulative\n",
    "\n",
    "# Separate training and testing data\n",
    "mask_val = [(df_cumulative.index.year == 2010) & (df_cumulative['ID_SC'].values==16)]\n",
    "df_val = df_cumulative[mask_val[0]].copy(deep=True)\n",
    "df_train = df_cumulative.copy(deep=True).drop( df_cumulative.index[mask_val[0]])\n",
    "\n",
    "del df_cumulative\n",
    "\n",
    "feature_cols = [ 'ID_SC', 'sin_ut','cos_ut', 'sin_doy', 'cos_doy', \n",
    "                     'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'vsw', 'vx', 'psw', 'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'PC_6hr', 'vsw_6hr', 'vx_6hr', 'psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr', 'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'vsw_5hr', 'vx_5hr', 'psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'PC_3hr', 'vsw_3hr', 'vx_3hr', 'psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr', 'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'vsw_1hr', 'vx_1hr', 'psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'vsw_45min', 'vx_45min', 'psw_45min', 'borovsky_45min', 'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'PC_30min', 'vsw_30min', 'vx_30min', 'psw_30min', 'borovsky_30min', 'newell_30min', 'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'PC_15min', 'vsw_15min', 'vx_15min', 'psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', 'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', 'vsw_10min', 'vx_10min', 'psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', 'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'vsw_5min', 'vx_5min', 'psw_5min', 'borovsky_5min', 'newell_5min'               ]\n",
    "print(len(feature_cols))\n",
    "\n",
    "# Construct X and y\n",
    "# feature_cols = [c for c in df_train.columns if not 'ELE' in c]\n",
    "# print( (feature_cols))\n",
    "#print(df_cumulative.columns)\n",
    "X_val = df_val[feature_cols].copy(deep=True)\n",
    "y_val = df_val['ELE_TOTAL_ENERGY_FLUX'].copy(deep=True)\n",
    "X_train = df_train[feature_cols].copy(deep=True)\n",
    "y_train = df_train['ELE_TOTAL_ENERGY_FLUX'].copy(deep=True)\n",
    "scaler_X = preprocessing.RobustScaler()\n",
    "scaler_X = scaler_X.fit(X_train.values)\n",
    "X_val_scaled = scaler_X.transform(X_val.values)\n",
    "X_train_scaled = scaler_X.transform(X_train.values)\n",
    "\n",
    "numFeatures = len(X_train.columns.to_list())\n",
    "feature_labels = X_train.columns.to_list()\n",
    "y_train_erg = y_train.copy(deep=True) * (1.60218e-12)\n",
    "y_val_erg = y_val.copy(deep=True) * (1.60218e-12)\n",
    "\n",
    "y_train[y_train == 0] = 0.0001\n",
    "y_val[y_val == 0] = 0.0001\n",
    "y_train_log = np.log10(y_train.copy(deep=True))\n",
    "y_val_log = np.log10(y_val.copy(deep=True))\n",
    "XX = np.array(X_train_scaled, dtype=np.float32)\n",
    "XX_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "del X_val, X_train\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DataGenerator_train(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(33), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        print(list_IDs)\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim))\n",
    "        y = np.zeros((self.batch_size,128,128))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] =XX[ID,:] #np.load('data/' + ID + '.npy')\n",
    "            for j in range(0,15): # will use moving window for 5 timesteps up to 3 SC_IDs\n",
    "                if ((ID-7+j > 0) and (ID-7+j)<len(self.list_IDs)):\n",
    "                    if(abs( (df_train.index[ID]-\n",
    "                       df_train.index[ID-7+j] ).seconds) <=2*60 ):\n",
    "                        \n",
    "                        mlat = df_train['SC_AACGM_LAT'][ID-7+j]\n",
    "                        mltime = df_train['SC_AACGM_LTIME'][ID-7+j]/24*2*np.pi \n",
    "                        mlat_j = int((90-mlat)/45*128)\n",
    "                        mltime_k = int((mltime)/(2*np.pi)*128)\n",
    "\n",
    "                        y[i,mlat_j,mltime_k] = y_train_log[ID-7+j]\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "\n",
    "class DataGenerator_test(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(33), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        print(list_IDs)\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim))\n",
    "        y = np.zeros((self.batch_size,128,128))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] =XX_test[ID,:] #np.load('data/' + ID + '.npy')\n",
    "            mlat = df_val['SC_AACGM_LAT'][ID]\n",
    "            mltime = df_val['SC_AACGM_LTIME'][ID]/24*2*np.pi \n",
    "            mlat_j = int((90-mlat)/45*128)\n",
    "            mltime_k = int((mltime)/(2*np.pi)*128)\n",
    "            #print( mlat,   mltime ,mlat_j,  mltime_k )\n",
    "\n",
    "\n",
    "            y[i,mlat_j,mltime_k] = y_val_log[ID]\n",
    "\n",
    "        return X, y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0       1       2 ... 1838280 1838281 1838282]\n",
      "[    0     1     2 ... 55207 55208 55209]\n"
     ]
    }
   ],
   "source": [
    "num_in = 145\n",
    "\n",
    "# Parameters\n",
    "params = {'dim': 145,\n",
    "          'batch_size': 2048,\n",
    "          'n_classes': 6, # not used\n",
    "          'n_channels': 1, # not used\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "partition ={ 'train': np.linspace(0, df_train.shape[0]-1, df_train.shape[0]).astype(int), 'validation': np.linspace(0, df_val.shape[0]-1, df_val.shape[0]).astype(int)}\n",
    "labels = { 'train': np.linspace(0, df_train.shape[0]-1, df_train.shape[0]).astype(int), 'validation': np.linspace(0, df_val.shape[0]-1, df_val.shape[0]).astype(int)}\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator_train(partition['train'], labels, **params)\n",
    "validation_generator = DataGenerator_test(partition['validation'], labels, **params)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    loss = K.sum(\n",
    "        K.cast(K.greater(y_true, 0),'float32')*K.square(y_true-y_pred)*\n",
    "        ( 1. +\n",
    "#         2.5*K.cast(K.greater(y_true,11.5),'float32')*K.cast(K.less_equal(y_pred,11.5),'float32') +\n",
    "#           5*K.cast(K.greater(y_true,12.),'float32')*K.cast(K.less_equal(y_pred,12.0),'float32') +\n",
    "         10*K.cast(K.greater(y_true,12.5),'float32')*K.cast(K.less_equal(y_pred,12.5),'float32') +\n",
    "         10*K.cast(K.greater(y_true,12.75),'float32')*K.cast(K.less_equal(y_pred,12.75),'float32') +\n",
    "         10*K.cast(K.greater(y_true,13.),'float32')*K.cast(K.less_equal(y_pred,13.),'float32')  \n",
    "        ) \n",
    "    )  / ( K.sum( K.cast(K.greater(y_true, 0),'float32') ))# finds the number of y_true  > 0\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    "    \n",
    "\n",
    "    \n",
    "    mse = K.sum( K.cast(K.greater(y_true, 0),'float32')*(\n",
    "        K.square(y_true-y_pred)) )/params['batch_size']  \n",
    "    return mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 145)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               37376     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 16, 16, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 32, 32, 4)         328       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 128, 128, 4)       404       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 128, 4)       0         \n",
      "_________________________________________________________________\n",
      "periodic_padding2d (Periodic (None, 134, 134, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 128, 128, 1)       197       \n",
      "=================================================================\n",
      "Total params: 65,281\n",
      "Trainable params: 65,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1 = Input(shape=(num_in))\n",
    "\n",
    "model1 = Dense(256, activation='relu')(input1)\n",
    "model1 = model1 = Dropout(0.5)(model1)\n",
    "model1 = Dense(int(64),   activation='relu')(model1)\n",
    "model1 = Dense(int(32),   activation='relu')(model1)\n",
    "model1 = Dense(int(256),   activation='relu')(model1)\n",
    "model1 = tf.keras.layers.Reshape((16, 16, 1))(model1)\n",
    "# 16x16 to 16 32x32 feature map\n",
    "model1 = tf.keras.layers.Conv2DTranspose(4, (9,9),\n",
    "                strides=(2,2), padding='same')(model1)\n",
    "model1 = tf.keras.layers.Conv2DTranspose(4, (5,5),\n",
    "                strides=(4,4), padding='same')(model1)\n",
    "# 4 128x128 feature map to 1 128x128\n",
    "model1 = model1 = Dropout(0.5)(model1)\n",
    "model1 = PeriodicPadding2D(3)(model1)\n",
    "model1 = tf.keras.layers.Conv2D(1, kernel_size=(7,7),\n",
    "                              padding='valid')(model1)\n",
    "output = model1\n",
    "\n",
    "model = tensorflow.keras.models.Model(inputs=input1, outputs=output)\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(loss=custom_loss, optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001),metrics=custom_mse)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0624 09:43:38.186772 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/897 [..............................] - ETA: 6:20 - loss: 87.7669 - custom_mse: 228.0018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 09:44:20.237824 139774923241280 callbacks.py:328] Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2855s vs `on_train_batch_end` time: 0.5621s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896/897 [============================>.] - ETA: 5s - loss: 7.2892 - custom_mse: 18.6587 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 11:10:46.214136 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - ETA: 0s - loss: 7.2826 - custom_mse: 18.6415"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 11:10:47.401834 139763117971200 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "W0624 11:11:09.964229 139763117971200 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - 5212s 6s/step - loss: 7.2826 - custom_mse: 18.6415 - val_loss: 0.9910 - val_custom_mse: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 11:11:11.774669 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "896/897 [============================>.] - ETA: 5s - loss: 1.2457 - custom_mse: 2.9667 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 12:36:19.104088 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - ETA: 0s - loss: 1.2455 - custom_mse: 2.9666"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 12:36:19.753744 139763537409792 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "W0624 12:36:42.245943 139763537409792 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - 5072s 6s/step - loss: 1.2455 - custom_mse: 2.9666 - val_loss: 0.9356 - val_custom_mse: 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 12:36:43.702721 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300\n",
      "896/897 [============================>.] - ETA: 5s - loss: 1.1202 - custom_mse: 2.6510 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 14:04:48.242464 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - ETA: 0s - loss: 1.1202 - custom_mse: 2.6510"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 14:04:48.815586 139763520624384 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "W0624 14:05:11.054223 139763520624384 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - 5252s 6s/step - loss: 1.1202 - custom_mse: 2.6510 - val_loss: 0.9240 - val_custom_mse: 0.8641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 14:05:12.742923 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/300\n",
      "896/897 [============================>.] - ETA: 5s - loss: 1.0686 - custom_mse: 2.5177 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 15:33:17.331961 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - ETA: 0s - loss: 1.0686 - custom_mse: 2.5177"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 15:33:18.270491 139763537409792 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "W0624 15:33:40.699085 139763537409792 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - 5248s 6s/step - loss: 1.0686 - custom_mse: 2.5177 - val_loss: 0.9055 - val_custom_mse: 0.8517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 15:33:42.222842 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/300\n",
      "896/897 [============================>.] - ETA: 5s - loss: 1.0307 - custom_mse: 2.4307 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 17:01:20.226798 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - ETA: 0s - loss: 1.0306 - custom_mse: 2.4306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 17:01:20.713200 139763529017088 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "W0624 17:01:43.067063 139763529017088 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - 5224s 6s/step - loss: 1.0306 - custom_mse: 2.4306 - val_loss: 0.9059 - val_custom_mse: 0.8490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 17:01:44.506713 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/300\n",
      "896/897 [============================>.] - ETA: 5s - loss: 1.0013 - custom_mse: 2.3568 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 18:28:43.730953 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - ETA: 0s - loss: 1.0012 - custom_mse: 2.3567"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 18:28:44.411044 139763537409792 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "W0624 18:29:07.401585 139763537409792 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - 5185s 6s/step - loss: 1.0012 - custom_mse: 2.3567 - val_loss: 0.8767 - val_custom_mse: 0.8263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 18:29:09.062967 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "896/897 [============================>.] - ETA: 5s - loss: 0.9688 - custom_mse: 2.2757 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 19:53:52.761345 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - ETA: 0s - loss: 0.9690 - custom_mse: 2.2757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 19:53:53.407507 139763520624384 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "W0624 19:54:15.166691 139763520624384 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "897/897 [==============================] - 5049s 6s/step - loss: 0.9690 - custom_mse: 2.2757 - val_loss: 0.9019 - val_custom_mse: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 19:54:16.682869 139764560795392 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/300\n",
      "417/897 [============>.................] - ETA: 46:30 - loss: 0.9342 - custom_mse: 2.2028"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[2048,4,134,134] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/functional_1/conv2d/Conv2D/Conv2DBackpropInput (defined at <ipython-input-10-a1bdff816566>:6) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_1499]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a1bdff816566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',  restore_best_weights=True,\n\u001b[1;32m      5\u001b[0m              patience=60)], use_multiprocessing=True,\n\u001b[0;32m----> 6\u001b[0;31m                      workers=6)#\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[2048,4,134,134] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/functional_1/conv2d/Conv2D/Conv2DBackpropInput (defined at <ipython-input-10-a1bdff816566>:6) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_1499]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_generator, \n",
    "                    validation_data=validation_generator,\n",
    "                    batch_size=params['batch_size'],epochs=300,#verbose=2,\n",
    "                   callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',  restore_best_weights=True,\n",
    "             patience=60)], use_multiprocessing=True,\n",
    "                     workers=6)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0628 08:59:40.616883 139763092793088 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/897 [..............................] - ETA: 6:24 - loss: 0.8842 - custom_mse: 2.2291"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0628 09:00:21.372299 139774923241280 callbacks.py:328] Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2675s vs `on_train_batch_end` time: 0.5905s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372/897 [===========>..................] - ETA: 51:58 - loss: 0.9179 - custom_mse: 2.1606"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "GPU sync failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e53878aed4d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',  restore_best_weights=True,\n\u001b[1;32m      9\u001b[0m              patience=60)], use_multiprocessing=True,\n\u001b[0;32m---> 10\u001b[0;31m                      workers=6)#\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \"\"\"\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: GPU sync failed"
     ]
    }
   ],
   "source": [
    "filename='mse_new_pipeline_145_2D_128x128_periodic_dropout_tailloss_pipline1_2'\n",
    "model.save(filename)\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "\n",
    "history = model.fit(training_generator, \n",
    "                    validation_data=validation_generator,\n",
    "                    batch_size=params['batch_size'],epochs=300,#verbose=2,\n",
    "                   callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',  restore_best_weights=True,\n",
    "             patience=60)], use_multiprocessing=True,\n",
    "                     workers=6)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='mse_new_pipeline_145_2D_128x128_periodic_dropout_tailloss_pipline1_2'\n",
    "model.save(filename)\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "\n",
    "plt.figure()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'][:])\n",
    "plt.plot(history.history['val_loss'][:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.ylim([0,.9])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'][:])\n",
    "plt.plot(history.history['val_loss'][:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='mse_new_pipeline_145_2D_128x128_periodic_dropout_tailloss_pipline1'\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "\n",
    "\n",
    "history = model.fit(training_generator, \n",
    "                    validation_data=validation_generator,\n",
    "                    batch_size=params['batch_size'],epochs=300,#verbose=2,\n",
    "                   callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',  restore_best_weights=True,\n",
    "             patience=60)], use_multiprocessing=True,\n",
    "                     workers=6)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='mse_new_pipeline_145_2D_128x128_periodic_dropout_tailloss_pipline1'\n",
    "model.save(filename)\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "\n",
    "plt.figure()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'][:])\n",
    "plt.plot(history.history['val_loss'][:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.ylim([0,.9])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'][:])\n",
    "plt.plot(history.history['val_loss'][:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mark= ['s', 'o', 'D', 'v']\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ioff()\n",
    "range1=np.min(np.min([-df_val['AL'].values,df_val['AE'].values,df_val['AU'].values]))\n",
    "range2=np.max(np.max([-df_val['AL'].values,df_val['AE'].values,df_val['AU'].values]))\n",
    "os.system('source /home/jackalak/heartbeat/cdf38_0-dist/bin/definitions.B')\n",
    "os.environ[\"CDF_LIB\"] = '/home/jackalak/heartbeat/cdf38_0-dist/lib'\n",
    "from spacepy import pycdf\n",
    "\n",
    "from ovationpyme.ovation_prime import FluxEstimator,AverageEnergyEstimator,BinCorrector\n",
    "from ovationpyme.ovation_utilities import calc_avg_solarwind\n",
    "from ovationpyme.ovation_plotting import latlt2polar,polar2dial,pcolor_flux\n",
    "max_val = 13\n",
    "min_val = 7.5\n",
    "    \n",
    "for ii in range(0,100):\n",
    "    results = model.predict( XX_test[0+ii*100:2+ii*100,:])\n",
    "\n",
    "    fig= plt.figure(figsize=(12,10))\n",
    "    ax1 = plt.subplot2grid((4,2), (0,0), colspan=2,rowspan=1)\n",
    "    ax3 = plt.subplot2grid((4,2), (1,0), colspan=2,rowspan=1)\n",
    "    ax4 = plt.subplot2grid((4,2), (2,0), rowspan=2,colspan=1,polar=True)\n",
    "    ax5 = plt.subplot2grid((4,2), (2, 1), rowspan=2,colspan=1,polar=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ax1.plot(-df_val['AL'], marker=mark[1],markersize=3)\n",
    "    ax1.plot(df_val['AE'], marker=mark[2],markersize=3)\n",
    "    ax1.plot(df_val['AU'], marker=mark[3],markersize=3)\n",
    "    ax1.plot([df_val.index[ii*100],df_val.index[ii*100]],[range1,range2],'--')\n",
    "\n",
    "    ax1.legend(['-AL','AE','AU'])\n",
    "    ax1.set_xlabel('time (month, day, year labels)')\n",
    "    ax1.set_title('Example Magnetic Indices',fontweight=\"bold\")\n",
    "\n",
    "    mlatgridN = np.linspace(90,45,num=128)\n",
    "    mltgridN =  np.linspace(0,24,num=128)   \n",
    "    flux = 10**(results[0,:,:,0])*1.60218e-12*3.14159\n",
    "\n",
    "\n",
    "    pcolor_kwargs = {'cmap':'gnuplot','vmin':(10**7.5)*1.60218e-12*3.14159,'vmax':(10**12.5)*1.60218e-12*3.14159}\n",
    "    mappableN = pcolor_flux(ax5,mlatgridN,mltgridN,flux,'N',**pcolor_kwargs)\n",
    "\n",
    "    ax5.set_title('Predicted Electron Precipitation Energy Flux (Neural Net)',fontweight=\"bold\", fontsize='medium',pad=10)\n",
    "\n",
    "\n",
    "    ax5.set_theta_zero_location('S')\n",
    "    theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "    theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "    ax5.set_thetagrids(theta_label_values,labels=theta_labels)\n",
    "\n",
    "    r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "    r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "    ax5.set_rgrids(r_label_values,labels=r_labels)\n",
    "    ax5.set_rlim([0.,45.])\n",
    "\n",
    "\n",
    "    ###########################3\n",
    "    # Ovation\n",
    "    ########################\n",
    "    dt = y_val_log.index[ii*100]\n",
    "    print(dt)\n",
    "    auroral_types = ['diff','mono','wave','ions']\n",
    "    # axS = f.add_subplot(122,projection='polar')\n",
    "    for jj in range(0,3):\n",
    "        atype = auroral_types[jj]\n",
    "        jtype =\"energy\"\n",
    "        bincorrect = True\n",
    "        combine_hemispheres = True\n",
    "        dtstr = dt.strftime('%Y%m%d %H:%M')\n",
    "        if jtype=='average energy':\n",
    "            estimator = AverageEnergyEstimator(atype)\n",
    "            get_precip_for_time = estimator.get_eavg_for_time\n",
    "        else:\n",
    "            estimator = FluxEstimator(atype,jtype)\n",
    "            get_precip_for_time = estimator.get_flux_for_time\n",
    "\n",
    "\n",
    "        tflux_kwargs = {'combine_hemispheres':combine_hemispheres,\n",
    "                        'return_dF':True}\n",
    "        fluxtupleN = get_precip_for_time(dt,hemi='N',**tflux_kwargs)\n",
    "        mlatgridN,mltgridN,fluxgridN,newell_coupling = fluxtupleN\n",
    "        fluxtupleS = get_precip_for_time(dt,hemi='S',**tflux_kwargs)\n",
    "        mlatgridS,mltgridS,fluxgridS,newell_coupling = fluxtupleS\n",
    "\n",
    "        if bincorrect:\n",
    "            bcN = BinCorrector(mlatgridN,mltgridN)\n",
    "            fluxgridN = bcN.fix(fluxgridN)\n",
    "            bcS = BinCorrector(mlatgridS,mltgridS)\n",
    "            fluxgridS = bcS.fix(fluxgridS)\n",
    "            print(\"Correction Applied\")\n",
    "\n",
    "        if jj== 0:\n",
    "            fluxgridN_sum = fluxgridN\n",
    "            fluxgridS_sum = fluxgridS\n",
    "        else:\n",
    "            fluxgridN_sum = fluxgridN_sum+fluxgridN\n",
    "            fluxgridS_sum = fluxgridN_sum+fluxgridS\n",
    "\n",
    "    mappableN = pcolor_flux(ax4,mlatgridN,mltgridN,fluxgridN_sum,'N',**pcolor_kwargs)\n",
    "\n",
    "    ax4.set_title('Predicted Electron Precipitation Energy Flux (OVATION Pyme)',pad =10,fontweight=\"bold\", fontsize='medium')\n",
    "\n",
    "\n",
    "    ax4.set_theta_zero_location('S')\n",
    "    theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "    theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "    ax4.set_thetagrids(theta_label_values,labels=theta_labels,fontsize='medium', )\n",
    "\n",
    "    r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "    r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "    ax4.set_rgrids(r_label_values,labels=r_labels)\n",
    "    ax4.set_rlim([0.,45.])\n",
    "    \n",
    "    ax4.scatter(np.ones((20))*23/24*2*3.14159 ,                   np.linspace(0,40,20)      )     \n",
    "    ax5.scatter(np.ones((20))*23/24*2*3.14159 ,                   np.linspace(0,45,20 )     )     \n",
    "\n",
    "    plt.colorbar(mappableN,ax=ax5,label='Total Energy Flux [erg/cm^s/s]')\n",
    "    \n",
    "    ax3.set_title('Log Scale Electron Precipitation Energy Flux [Log10 eV/cm^2/s]'+' '+str(dt),fontweight=\"bold\")\n",
    "    ax3.scatter(np.linspace(90,50,80),np.log10(fluxgridN_sum[:,int(23/24*96)]/1.60218e-12), marker=mark[2])\n",
    "    ax3.scatter(np.linspace(90,45,128),np.log10(flux[:,int(23/24*128)]/1.60218e-12), marker=mark[3])\n",
    "    ax3.legend(['OVATION Pyme model','neural net: 0.5 deg / Mlat'])\n",
    "    ax3.set_ylim(top=max_val,bottom=min_val)\n",
    "    fig.tight_layout() \n",
    "\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mark= ['s', 'o', 'D', 'v']\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ioff()\n",
    "range1=np.min(np.min([-df_val['AL'].values,df_val['AE'].values,df_val['AU'].values]))\n",
    "range2=np.max(np.max([-df_val['AL'].values,df_val['AE'].values,df_val['AU'].values]))\n",
    "os.system('source /home/jackalak/heartbeat/cdf38_0-dist/bin/definitions.B')\n",
    "os.environ[\"CDF_LIB\"] = '/home/jackalak/heartbeat/cdf38_0-dist/lib'\n",
    "from spacepy import pycdf\n",
    "\n",
    "from ovationpyme.ovation_prime import FluxEstimator,AverageEnergyEstimator,BinCorrector\n",
    "from ovationpyme.ovation_utilities import calc_avg_solarwind\n",
    "from ovationpyme.ovation_plotting import latlt2polar,polar2dial,pcolor_flux\n",
    "max_val = 13\n",
    "min_val = 7.5\n",
    "    \n",
    "for ii in range(0,100):\n",
    "    results = model.predict( XX_test[0+ii*100:2+ii*100,:])\n",
    "\n",
    "    fig= plt.figure(figsize=(12,10))\n",
    "    ax1 = plt.subplot2grid((4,2), (0,0), colspan=2,rowspan=1)\n",
    "    ax3 = plt.subplot2grid((4,2), (1,0), colspan=2,rowspan=1)\n",
    "    ax4 = plt.subplot2grid((4,2), (2,0), rowspan=2,colspan=1,polar=True)\n",
    "    ax5 = plt.subplot2grid((4,2), (2, 1), rowspan=2,colspan=1,polar=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ax1.plot(-df_val['AL'], marker=mark[1],markersize=3)\n",
    "    ax1.plot(df_val['AE'], marker=mark[2],markersize=3)\n",
    "    ax1.plot(df_val['AU'], marker=mark[3],markersize=3)\n",
    "    ax1.plot([df_val.index[ii*100],df_val.index[ii*100]],[range1,range2],'--')\n",
    "\n",
    "    ax1.legend(['-AL','AE','AU'])\n",
    "    ax1.set_xlabel('time (month, day, year labels)')\n",
    "    ax1.set_title('Example Magnetic Indices',fontweight=\"bold\")\n",
    "\n",
    "    mlatgridN = np.linspace(90,45,num=128)\n",
    "    mltgridN =  np.linspace(0,24,num=128)   \n",
    "    flux = 10**(results[0,:,:,0])*1.60218e-12*3.14159\n",
    "\n",
    "\n",
    "    pcolor_kwargs = {'cmap':'gnuplot','vmin':7.5,'vmax':12.5}\n",
    "    mappableN = pcolor_flux(ax5,mlatgridN,mltgridN,np.log10(flux/1.60218e-12),'N',**pcolor_kwargs)\n",
    "\n",
    "    ax5.set_title('Log10 Predicted Electron Precipitation Energy Flux (Neural Net)',fontweight=\"bold\", fontsize='medium',pad=10)\n",
    "\n",
    "\n",
    "    ax5.set_theta_zero_location('S')\n",
    "    theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "    theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "    ax5.set_thetagrids(theta_label_values,labels=theta_labels)\n",
    "\n",
    "    r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "    r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "    ax5.set_rgrids(r_label_values,labels=r_labels)\n",
    "    ax5.set_rlim([0.,45.])\n",
    "\n",
    "\n",
    "    ###########################3\n",
    "    # Ovation128\n",
    "    ########################\n",
    "    dt = y_val_log.index[ii*100]\n",
    "    print(dt)\n",
    "    auroral_types = ['diff','mono','wave','ions']\n",
    "    # axS = f.add_subplot(122,projection='polar')\n",
    "    for jj in range(0,3):\n",
    "        atype = auroral_types[jj]\n",
    "        jtype =\"energy\"\n",
    "        bincorrect = True\n",
    "        combine_hemispheres = True\n",
    "        dtstr = dt.strftime('%Y%m%d %H:%M')\n",
    "        if jtype=='average energy':\n",
    "            estimator = AverageEnergyEstimator(atype)\n",
    "            get_precip_for_time = estimator.get_eavg_for_time\n",
    "        else:\n",
    "            estimator = FluxEstimator(atype,jtype)\n",
    "            get_precip_for_time = estimator.get_flux_for_time\n",
    "\n",
    "\n",
    "        tflux_kwargs = {'combine_hemispheres':combine_hemispheres,\n",
    "                        'return_dF':True}\n",
    "        fluxtupleN = get_precip_for_time(dt,hemi='N',**tflux_kwargs)\n",
    "        mlatgridN,mltgridN,fluxgridN,newell_coupling = fluxtupleN\n",
    "        fluxtupleS = get_precip_for_time(dt,hemi='S',**tflux_kwargs)\n",
    "        mlatgridS,mltgridS,fluxgridS,newell_coupling = fluxtupleS\n",
    "\n",
    "        if bincorrect:\n",
    "            bcN = BinCorrector(mlatgridN,mltgridN)\n",
    "            fluxgridN = bcN.fix(fluxgridN)\n",
    "            bcS = BinCorrector(mlatgridS,mltgridS)\n",
    "            fluxgridS = bcS.fix(fluxgridS)\n",
    "            print(\"Correction Applied\")\n",
    "\n",
    "        if jj== 0:\n",
    "            fluxgridN_sum = fluxgridN\n",
    "            fluxgridS_sum = fluxgridS\n",
    "        else:\n",
    "            fluxgridN_sum = fluxgridN_sum+fluxgridN\n",
    "            fluxgridS_sum = fluxgridN_sum+fluxgridS\n",
    "\n",
    "    mappableN = pcolor_flux(ax4,mlatgridN,mltgridN,np.log10(fluxgridN_sum/1.60218e-12),'N',**pcolor_kwargs)\n",
    "\n",
    "    ax4.set_title('Log10 Predicted Electron Precipitation Energy Flux (OVATION Pyme)',pad =10,fontweight=\"bold\", fontsize='medium')\n",
    "\n",
    "\n",
    "    ax4.set_theta_zero_location('S')\n",
    "    theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "    theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "    ax4.set_thetagrids(theta_label_values,labels=theta_labels,fontsize='medium', )\n",
    "\n",
    "    r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "    r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "    ax4.set_rgrids(r_label_values,labels=r_labels)\n",
    "    ax4.set_rlim([0.,45.])\n",
    "    \n",
    "    ax4.scatter(np.ones((20))*23/24*2*3.14159 ,                   np.linspace(0,40,20)      )     \n",
    "    ax5.scatter(np.ones((20))*23/24*2*3.14159 ,                   np.linspace(0,45,20 )     )     \n",
    "\n",
    "    plt.colorbar(mappableN,ax=ax5,label='Total Energy Flux [erg/cm^s/s]')\n",
    "    \n",
    "    ax3.set_title('Log Scale Electron Precipitation Energy Flux [Log10 eV/cm^2/s]'+' '+str(dt),fontweight=\"bold\")\n",
    "    ax3.scatter(np.linspace(90,50,80),np.log10(fluxgridN_sum[:,int(23/24*96)]/1.60218e-12), marker=mark[2])\n",
    "    ax3.scatter(np.linspace(90,45,128),np.log10(flux[:,int(23/24*128)]/1.60218e-12), marker=mark[3])\n",
    "    ax3.legend(['OVATION Pyme model','neural net: 0.5 deg / Mlat'])\n",
    "    ax3.set_ylim(top=max_val,bottom=min_val)\n",
    "    fig.tight_layout() \n",
    "\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hemisphere_CSV_val(scaler_X, model, features, test, XX_test):\n",
    "\n",
    "    mark= ['s', 'o', 'D', 'v']\n",
    "\n",
    "    y_val_log = np.log10(test['ELE_TOTAL_ENERGY_FLUX']+.0001)\n",
    "\n",
    "    num = test.shape[0]\n",
    "    \n",
    "    %matplotlib inline\n",
    "    plt.ioff()\n",
    "    ovation_val = []\n",
    "    result_val = []\n",
    "    \n",
    "    range1=np.max(np.max([-test['AL'].values,test['AE'].values,test['AU'].values]))\n",
    "    \n",
    "    ml_hemi = []\n",
    "    ovation_hemi = []\n",
    "    \n",
    "    for i in range(0,num):\n",
    "\n",
    "\n",
    "       #################################\n",
    "        #\n",
    "        #ML model\n",
    "        ##########################\n",
    "        results = model.predict( XX_test[i:i+1,:])\n",
    "        mlatgridN = np.linspace(90,45,num=128)\n",
    "        mltgridN =  np.linspace(0,24,num=128)   \n",
    "        flux = 10**(results[0,:,:,0])*1.60218e-12*3.14159\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ###########################3\n",
    "        # Ovation\n",
    "        ########################\n",
    "        dt = test.index[i]\n",
    "        auroral_types = ['diff','mono','wave','ions']\n",
    "        for jj in range(0,3):\n",
    "            atype = auroral_types[jj]\n",
    "            jtype =\"energy\"\n",
    "            bincorrect = True\n",
    "            combine_hemispheres = True\n",
    "            dtstr = dt.strftime('%Y%m%d %H:%M')\n",
    "            if jtype=='average energy':\n",
    "                estimator = AverageEnergyEstimator(atype)\n",
    "                get_precip_for_time = estimator.get_eavg_for_time\n",
    "            else:\n",
    "                estimator = FluxEstimator(atype,jtype)\n",
    "                get_precip_for_time = estimator.get_flux_for_time\n",
    "\n",
    "\n",
    "            tflux_kwargs = {'combine_hemispheres':combine_hemispheres,\n",
    "                            'return_dF':True}\n",
    "            fluxtupleN = get_precip_for_time(dt,hemi='N',**tflux_kwargs)\n",
    "            mlatgridN,mltgridN,fluxgridN,newell_coupling = fluxtupleN\n",
    "            fluxtupleS = get_precip_for_time(dt,hemi='S',**tflux_kwargs)\n",
    "            mlatgridS,mltgridS,fluxgridS,newell_coupling = fluxtupleS\n",
    "\n",
    "            if bincorrect:\n",
    "                bcN = BinCorrector(mlatgridN,mltgridN)\n",
    "                fluxgridN = bcN.fix(fluxgridN)\n",
    "                bcS = BinCorrector(mlatgridS,mltgridS)\n",
    "                fluxgridS = bcS.fix(fluxgridS)\n",
    "                print(\"Correction Applied\")\n",
    "\n",
    "            if jj== 0:\n",
    "                fluxgridN_sum = fluxgridN\n",
    "                fluxgridS_sum = fluxgridS\n",
    "            else:\n",
    "                fluxgridN_sum = fluxgridN_sum+fluxgridN\n",
    "                fluxgridS_sum = fluxgridN_sum+fluxgridS\n",
    "\n",
    "           \n",
    "       \n",
    "        pt = i\n",
    "\n",
    "        result_val.append( \n",
    "            flux[int((90-test['SC_AACGM_LAT'][pt])/45*128),int((test['SC_AACGM_LTIME'][pt])/24*128)] )\n",
    "        if  test['SC_AACGM_LAT'][pt] <= 50:\n",
    "            ovation_val.append( fluxgridN_sum[0,int((test['SC_AACGM_LTIME'][pt])/24*96)] )\n",
    "        else:\n",
    "            ovation_val.append( \n",
    "                fluxgridN_sum[int((90-test['SC_AACGM_LAT'][pt])/40*80),int((test['SC_AACGM_LTIME'][pt])/24*96)] )\n",
    "\n",
    "       \n",
    "        ml_hemi.append(flux)\n",
    "        ovation_hemi.append(fluxgridN_sum)\n",
    "    \n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(0,num):\n",
    "        \n",
    "        flux = ml_hemi[i]\n",
    "        fluxgridN_sum =ovation_hemi[i]\n",
    "\n",
    "        fig= plt.figure(figsize=(20,12))\n",
    "        ax3 = plt.subplot2grid((4,4), (0,0), colspan=4,rowspan=1)\n",
    "        ax1 = plt.subplot2grid((4,4), (1,0), colspan=4,rowspan=1)\n",
    "        ax4 = plt.subplot2grid((4,4), (2,0), rowspan=2,colspan=2,polar=True)\n",
    "        ax5 = plt.subplot2grid((4,4), (2, 2), rowspan=2,colspan=2,polar=True)\n",
    "        \n",
    "        ax1.plot(-test['AL'], marker=mark[1],markersize=3)\n",
    "        ax1.plot(test['AE'], marker=mark[2],markersize=3)\n",
    "        ax1.plot(test['AU'], marker=mark[3],markersize=3)\n",
    "        ax1.plot([test.index[i],test.index[i]],[0,10000],'--')\n",
    "\n",
    "        ax1.legend(['-AL','AE','AU'])\n",
    "        ax1.set_ylim(top=range1)\n",
    "        ax1.set_xlim(right=test.index[-1])\n",
    "        ax1.set_xlabel('time (month, day, year labels)')\n",
    "        ax1.set_title('Example Magnetic Indices',fontweight=\"bold\")\n",
    "\n",
    "       #################################\n",
    "        #\n",
    "        #ML model\n",
    "        ##########################\n",
    "        mlatgridN = np.linspace(90,45,num=128)\n",
    "        mltgridN =  np.linspace(0,24,num=128)  \n",
    "\n",
    "\n",
    "\n",
    "        pcolor_kwargs = {'cmap':'gnuplot','vmin':(10**0)*1.60218e-12,'vmax':(10**12.5)*1.60218e-12*3.14159}\n",
    "        mappableN = pcolor_flux(ax5,mlatgridN,mltgridN,flux,'N',**pcolor_kwargs)\n",
    "\n",
    "        ax5.set_title('Predicted Electron Precipitation Energy Flux (Neural Net)',fontweight=\"bold\", fontsize='medium',pad=10)\n",
    "        ax5.set_theta_zero_location('S')\n",
    "        theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "        theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "        ax5.set_thetagrids(theta_label_values,labels=theta_labels)\n",
    "\n",
    "        r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "        r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "        ax5.set_rgrids(r_label_values,labels=r_labels)\n",
    "        ax5.set_rlim([0.,45.])\n",
    "\n",
    "\n",
    "        ###########################3\n",
    "        # Ovation\n",
    "        ########################\n",
    "        dt = test.index[i]\n",
    "        dtstr = dt.strftime('%Y%m%d %H:%M')\n",
    "        mlatgridN = np.linspace(90,50,num=80)\n",
    "        mltgridN = np.linspace(90,45,num=96)\n",
    "\n",
    "        mappableN = pcolor_flux(ax4,mlatgridN,mltgridN,fluxgridN_sum,'N',**pcolor_kwargs)\n",
    "\n",
    "        ax4.set_title('Predicted Electron Precipitation Energy Flux (OVATION Pyme)',pad =10,fontweight=\"bold\", fontsize='medium')\n",
    "        ax4.set_theta_zero_location('S')\n",
    "        theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "        theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "        ax4.set_thetagrids(theta_label_values,labels=theta_labels,fontsize='medium', )\n",
    "\n",
    "        r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "        r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "        ax4.set_rgrids(r_label_values,labels=r_labels)\n",
    "        ax4.set_rlim([0.,45.])\n",
    "        plt.colorbar(mappableN,ax=ax5,label='Total Flux [erg/cm^2/s]',fraction=0.05,pad=0.09)\n",
    "        plt.colorbar(mappableN,ax=ax4,label='Total Flux [erg/cm^2/s]',fraction=0.05,pad=0.09)\n",
    "\n",
    "           \n",
    "        ax4.scatter(test['SC_AACGM_LTIME'][i]/24*2*3.14159,90-test['SC_AACGM_LAT'][i])     \n",
    "        ax5.scatter(test['SC_AACGM_LTIME'][i]/24*2*3.14159,90-test['SC_AACGM_LAT'][i])     \n",
    "        ax4.tick_params(axis='y', colors='white')\n",
    "        ax5.tick_params(axis='y', colors='white')\n",
    "\n",
    "\n",
    "        ax3.set_title('Log Scale Electron Precipitation Energy Flux [Log10 eV/cm^2/s]'+' '+str(dt),fontweight=\"bold\")\n",
    "        ax3.set_title('Log Scale Electron Precipitation Energy Flux Log10 [eV/cm^2/s]'+' '+str(dt),fontweight=\"bold\")\n",
    "        ax3.plot(test.index,np.log10(np.array(ovation_val)/1.60218e-12), marker=mark[1])\n",
    "        ax3.plot(test.index,np.log10(np.array(result_val)/1.60218e-12), marker=mark[2])\n",
    "        ax3.plot(test.index,np.log10(10**(y_val_log)*3.14159), marker=mark[3])\n",
    "        ax3.plot([test.index[i],test.index[i]],[7.5,13.5],'--')\n",
    "        ax3.legend(['OVATION Pyme model','neural net: 0.5 deg / Mlat','measured value','current time'])\n",
    "        ax3.set_ylim(top=13.5,bottom=7.5)\n",
    "        ax3.set_xlim(right=test.index[-1])\n",
    "\n",
    "        fig.tight_layout() \n",
    "        name = 'figures/movie_val_'+str(i) + '.png'\n",
    "        fig.savefig(name,dpi=400)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    #os.system('ffmpeg -r:v 5 -i \"figures/movie_val_%01d.png\" -codec:v libx264 -preset veryslow  \"movie_val_2013_SC17.mp4\";ffmpeg -i movie_val_2013_SC17.mp4 -c:v libx264 -c:a libmp3lame -b:a 384K movie_val_2013_SC17.avi;')\n",
    "    return result_val, ovation_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = ['SC_AACGM_LAT', 'ELE_TOTAL_ENERGY_FLUX', 'ID_SC', 'sin_ut',\n",
    "                             'cos_ut', 'sin_doy', 'cos_doy', 'sin_SC_AACGM_LTIME', 'cos_SC_AACGM_LTIME',\n",
    "                             'F107', 'AE', 'AL', 'AU', 'SymH', \n",
    "                             'F107_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'F107_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', \n",
    "                             'F107_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', \n",
    "                             'F107_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', \n",
    "                             ]\n",
    "\n",
    "result_val, ovation_val = plot_hemisphere_CSV_val(scaler_X, model, features, df_val[0:df_val.shape[0]:600], XX_test[0:df_val.shape[0]:600,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average( (y_val_log[0:df_val.shape[0]:600]-np.log10(np.array(result_val)/1.60218e-12/3.14159))**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_CSV_val(scaler_X, model, features, test, XX_test):\n",
    "\n",
    "    mark= ['s', 'o', 'D', 'v']\n",
    "\n",
    "    y_val_log = np.log10(test['ELE_TOTAL_ENERGY_FLUX']+.0001)\n",
    "\n",
    "    num = test.shape[0]\n",
    "    \n",
    "    %matplotlib inline\n",
    "    plt.ioff()\n",
    "    result_val = []\n",
    "\n",
    "    \n",
    "    for i in range(0,num):\n",
    "\n",
    "\n",
    "       #################################\n",
    "        #\n",
    "        #ML model\n",
    "        ##########################\n",
    "        results = model.predict( XX_test[i:i+1,:])\n",
    "        mlatgridN = np.linspace(90,45,num=128)\n",
    "        mltgridN =  np.linspace(0,24,num=128)   \n",
    "        flux = results[0,:,:,0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "           \n",
    "       \n",
    "        pt = i\n",
    "\n",
    "        result_val.append( \n",
    "            flux[int((90-test['SC_AACGM_LAT'][pt])/45*128),int((test['SC_AACGM_LTIME'][pt])/24*128)] )\n",
    "\n",
    "       \n",
    "    \n",
    "        \n",
    "\n",
    "    return result_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_val = for_CSV_val(scaler_X, model, features, df_val, XX_test)\n",
    "np.average((y_val_log-(np.array(result_val)))**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(y_val_log,  alpha=0.5, bins=200)\n",
    "plt.hist( result_val, alpha=0.5, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dpi=200\n",
    "df_results = pd.DataFrame(data=result_val, index = df_val.index)\n",
    "\n",
    "plot_model(df_results, df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
