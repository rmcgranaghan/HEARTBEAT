{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "os.system('source /home/jackalak/heartbeat/cdf38_0-dist/bin/definitions.B')\n",
    "os.environ[\"CDF_LIB\"] = '/home/jackalak/heartbeat/cdf38_0-dist/lib'\n",
    "from spacepy import pycdf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "from nasaomnireader import omnireader\n",
    "\n",
    "import ftplib\n",
    "import os\n",
    "\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pickle\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import datetime\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "import tensorflow\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "from sklearn import preprocessing\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "import datetime\n",
    "from os.path import isfile, join\n",
    "from sys import getsizeof\n",
    "from random import *\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_omni_data(t_start, t_end):\n",
    "\n",
    "    #--------------------------------------------------------#\n",
    "    #\tOMNI Data - includes solar wind, and geomag params   #\n",
    "    #--------------------------------------------------------#\n",
    "\n",
    "    #get OMNI data\n",
    "    omniInt = omnireader.omni_interval(t_start,t_end,'5min', cdf_or_txt = 'txt')\n",
    "\n",
    "    #print(omniInt.cdfs[0].vars) #prints all the variables available on omni\n",
    "\n",
    "    epochs = omniInt['Epoch'] #time array for omni 5min data\n",
    "    By,Bz,AE,SymH = omniInt['BY_GSM'],omniInt['BZ_GSM'],omniInt['AE_INDEX'], omniInt['SYM_H']\n",
    "    AL, AU = omniInt['AL_INDEX'],omniInt['AU_INDEX']\n",
    "    vsw,psw = omniInt['flow_speed'], omniInt['Pressure']\n",
    "    borovsky_reader = omnireader.borovsky(omniInt)\n",
    "    borovsky = borovsky_reader()\n",
    "    #newell_reader = omnireader.newell(omniInt)\n",
    "    #newell = newell_reader()\n",
    "\n",
    "    def NewellCF_calc(v,bz,by):\n",
    "        # v expected in km/s\n",
    "        # b's expected in nT    \n",
    "        NCF = np.zeros_like(v)\n",
    "        NCF.fill(np.nan)\n",
    "        bt = np.sqrt(by**2 + bz**2)\n",
    "        bztemp = bz\n",
    "        bztemp[bz == 0] = .001\n",
    "        #Caculate clock angle (theta_c = t_c)\n",
    "        tc = np.arctan2(by,bztemp)\n",
    "        neg_tc = bt*np.cos(tc)*bz < 0 \n",
    "        tc[neg_tc] = tc[neg_tc] + np.pi\n",
    "        sintc = np.abs(np.sin(tc/2.))\n",
    "        NCF = (v**1.33333)*(sintc**2.66667)*(bt**0.66667)\n",
    "        return NCF\n",
    "\n",
    "\n",
    "    newell = NewellCF_calc(vsw, Bz, By)\n",
    "\n",
    "\n",
    "    # \tproton_flux_10MeV, proton_flux_30MeV, proton_flux_60MeV = omniInt['PR-FLX_10'], omniInt['PR-FLX_30'], omniInt['PR-FLX_60']\n",
    "\n",
    "\n",
    "    #calculate clock angle\n",
    "    clock_angle = np.degrees(np.arctan2( By,Bz))\n",
    "    clock_angle[clock_angle < 0] = clock_angle[clock_angle<0] + 360.\n",
    "\n",
    "    #print('Got 5 minutes data')\n",
    "\n",
    "    omniInt_1hr = omnireader.omni_interval(t_start,t_end,'hourly', cdf_or_txt = 'txt')\n",
    "    F107,KP = omniInt_1hr['F10_INDEX'], omniInt_1hr['KP']\n",
    "    KP = pd.DataFrame(np.repeat(KP,12,axis=0))\n",
    "    F107 = pd.DataFrame(np.repeat(F107,12,axis=0))\n",
    "\n",
    "\n",
    "\n",
    "    #put all in a dataframe and save\n",
    "\n",
    "    dataframe = pd.DataFrame()\n",
    "    dataframe['Bz'] = Bz\n",
    "    dataframe['By'] = By\n",
    "    dataframe['Vsw'] = vsw\n",
    "    dataframe['Vx'] = omniInt['Vx']\n",
    "    dataframe['Psw'] = psw\n",
    "    dataframe['AE'] = AE\n",
    "    dataframe['AL'] = AL\n",
    "    dataframe['AU'] = AU\n",
    "    dataframe['SymH'] = SymH\n",
    "    dataframe['Clock Angle'] = clock_angle\n",
    "    dataframe['newell'] = newell\n",
    "    dataframe['borovsky'] = borovsky\n",
    "    dataframe['Kp'] = KP\n",
    "    dataframe['F107'] = F107\n",
    "    dataframe['PC'] = omniInt['PC_N_INDEX']\n",
    "    dataframe['Bx'] = omniInt['BX_GSE']\n",
    "    # \tdataframe = dataframe.replace(9999.99, np.nan) #replace 9999.99 with nans ??????????????\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "class t_hist():\n",
    "    '''\n",
    "    Class which takes solar wind data and creates some time history\n",
    "    for some specific time.\n",
    "    \n",
    "    Input:\n",
    "        Data ---------- A DataFrame of solar wind data at 5 minute\n",
    "                        cadence and datetime index.\n",
    "        Historic_time - The number of minutes into the past you\n",
    "                        would like the hisotry for. (E.g. for 1hr ago\n",
    "                        you would input 60 minutes).\n",
    "        window_mins --- If averaging for the time history, then this\n",
    "                        input specifies the window length, in minutes,\n",
    "                        centred on the historic_time specified.\n",
    "    '''\n",
    "    def __init__(self,data,historic_time,window_mins):\n",
    "        self.data = data\n",
    "        self.time = historic_time\n",
    "        self.window = window_mins\n",
    "        \n",
    "    def avg_hist(self):\n",
    "        '''\n",
    "        Function which returns a historic_time value, averaged over\n",
    "        the window_mins.\n",
    "        \n",
    "        Output:\n",
    "            - A dataframe of values of the time history.\n",
    "        '''\n",
    "        # Check that indices are datetime\n",
    "        self.is_datetime()\n",
    "        \n",
    "        if self.time % 60:\n",
    "            raise ValueError('Please choose a historic time value '+\n",
    "                             'which correspond to an integer '+\n",
    "                             'number of hours!')\n",
    "        \n",
    "        window_s = timedelta(minutes = self.time + self.window/2.0)\n",
    "        # '+5' ensures that the window is closed on the right\n",
    "        window_e = timedelta(minutes = self.time + 5 -\n",
    "                             self.window/2.0)\n",
    "        indices = self.data.index\n",
    "        \n",
    "        hist = [self.data[i-window_s : i-window_e].mean().values\n",
    "                for i in indices]\n",
    "        \n",
    "        col_label = '_'+str(self.time/60.0)[0]+'hr'\n",
    "        columns = [i+col_label for i in self.data.columns]\n",
    "        \n",
    "        th_df = pd.DataFrame(hist, index=indices,columns=columns)\n",
    "        \n",
    "        return th_df[th_df.index[0]+window_s:]\n",
    "    \n",
    "    def instant_hist(self):\n",
    "        '''\n",
    "        Function which returns an instantaneous historic_time value.\n",
    "        \n",
    "        Output:\n",
    "            - A dataframe of instantaneous values corresponding to\n",
    "              historic_time minutes in the past.\n",
    "        '''\n",
    "        # Check that indices are datetime\n",
    "        self.is_datetime()\n",
    "        \n",
    "        if self.time % 5:\n",
    "            raise ValueError('Please choose a historic time value '+\n",
    "                             'which correspond to a multiple of 5 '+\n",
    "                             'minutes!')\n",
    "            \n",
    "        t_offset = timedelta(minutes=self.time)\n",
    "        indices = self.data.index\n",
    "        \n",
    "\n",
    "        hist = [self.data.loc[i-t_offset].values\n",
    "                 for i in self.data.index\n",
    "                 if i >= indices[0]+t_offset]\n",
    "        \n",
    "        if self.time < 60:\n",
    "            if self.time >= 10:\n",
    "                col_label = '_'+str(self.time)[0:2]+'min'\n",
    "            else:\n",
    "                col_label = '_'+str(self.time)[0]+'min'\n",
    "        else:\n",
    "            col_label = '_'+str(self.time/60.0)[0]+'hr_I'\n",
    "        columns = [i+col_label for i in self.data.columns]\n",
    "        return pd.DataFrame(hist, index=indices[int(self.time/5):],\n",
    "                            columns=columns)\n",
    "    \n",
    "    def is_datetime(self):\n",
    "        dt_type = pd.core.indexes.datetimes.DatetimeIndex\n",
    "        if type(self.data.index) != dt_type:\n",
    "            raise ValueError('Dataframe index is not '+\n",
    "                             'in the correct datetime '+\n",
    "                             'format')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "def cleaning_data(data,safe_cols=[],sigma_val=4):\n",
    "    '''\n",
    "    Function which removes data which is 'sigma_val' stdevs from\n",
    "    the mean.\n",
    "\n",
    "    Note: 4 sigma encompasses ~99.994% of the data.\n",
    "          ~1 real piece of 5 min data is removed for\n",
    "          every 55 days of such data (assuming Gaussian).\n",
    "\n",
    "    Inputs:\n",
    "    sigma_val - (float) number of standard deviations from the\n",
    "                mean to consider as the limit of 'good' data.\n",
    "    safe_cols - Columns in the data which one might like to\n",
    "                keep without any changes (i.e., if there are\n",
    "                no null values in the initial dataset etc.).\n",
    "\n",
    "    Returns:\n",
    "     - Cleaned solar wind data dataframe.\n",
    "     - Dataframe of 'bad' solar wind data.\n",
    "    '''\n",
    "\n",
    "#   Initialising data and empty lists\n",
    "    sw_df = data\n",
    "    cleaned_cols = []\n",
    "    trash_data = []\n",
    "#   Looping through dataframe columns and removing 'bad' values\n",
    "    for i in sw_df.columns:\n",
    "        if i not in safe_cols:\n",
    "            std = sw_df[i].std()\n",
    "            mean = sw_df[i].mean()\n",
    "\n",
    "            cleaned = sw_df[i][sw_df[i]<mean+std*sigma_val]\n",
    "            trash = sw_df[i][sw_df[i]>=mean+std*sigma_val]\n",
    "\n",
    "            cleaned_cols.append(cleaned)\n",
    "            trash_data.append(trash)\n",
    "        else:\n",
    "            cleaned_cols.append(sw_df[i])\n",
    "            trash_data.append([np.nan])\n",
    "#   Initialising empty dataframes and appending data\n",
    "    sw_c_df = pd.DataFrame()\n",
    "    trash_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(sw_df.columns)):\n",
    "#         sw_c_df[sw_df.columns[i]] = cleaned_cols[i]\n",
    "#         sw_c_df = sw_c_df\n",
    "        sw_c_df_temp = pd.DataFrame(cleaned_cols[i],\n",
    "                                 columns=[sw_df.columns[i]])\n",
    "        sw_c_df = pd.concat([sw_c_df,sw_c_df_temp], axis=1)\n",
    "\n",
    "        trash_df_temp = pd.DataFrame(trash_data[i],\n",
    "                                     columns=[sw_df.columns[i]])\n",
    "        trash_df = pd.concat([trash_df,trash_df_temp], axis=1)\n",
    "\n",
    "#   Checking if the trash data contains non-'bad' data.\n",
    "    check_trash(trash_df)\n",
    "    return (sw_c_df, trash_df)\n",
    "\n",
    "#################################\n",
    "\n",
    "def sw_interp(data,method='linear'):\n",
    "    '''\n",
    "    Function which interpolates NaN values in the cleaned\n",
    "    data dataframe.\n",
    "\n",
    "    See Pandas documentation for other methods.\n",
    "\n",
    "    Input:\n",
    "    method - method of interpolation.\n",
    "\n",
    "    Return:\n",
    "     - Cleaned, interpolated data.\n",
    "    '''\n",
    "    return data.interpolate(method=method)\n",
    "\n",
    "#################################\n",
    "\n",
    "def check_trash(trash_data):\n",
    "    '''\n",
    "    Function which checks to see if all the removed data\n",
    "    is the 'bad' data fill value.\n",
    "\n",
    "    Returns:\n",
    "     - String detailing which parameters have had real\n",
    "       removed from them.\n",
    "    '''\n",
    "    for i in trash_data.columns:\n",
    "        if (trash_data[i].mean() <\n",
    "            trash_data[i].max()):\n",
    "            print('Some real data has been removed from: ',i)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "def time_history(data,auto=True):\n",
    "    '''\n",
    "    Function which calculates time history information\n",
    "    given an input dataframe.\n",
    "    \n",
    "    Averages are centred on the respective time-history\n",
    "    specified.\n",
    "    \n",
    "    Input:\n",
    "    data - a Pandas DataFrame containing 5 minute cadence\n",
    "           data.\n",
    "           MUST HAVE DATETIME INDEX.\n",
    "    auto - Whether or not to automatically clean the data.\n",
    "           If not True, then the cleaning_data() and\n",
    "           sw_interp() functions must be called individually\n",
    "           and the results of sw_interp() should be the data\n",
    "           fed to this function, time_history(). Set to false\n",
    "           to retrieve the non-interpolated data and the trash\n",
    "           data.\n",
    "    \n",
    "    Output:\n",
    "    A concatenated DataFrame containing\n",
    "        - the original data\n",
    "        - t-6hrs (1hr avg)\n",
    "        - t-5hrs (1hr avg)\n",
    "        - t-3hrs (30min avg)\n",
    "        - t-1hrs (30min avg)\n",
    "        - t-45min (instant)\n",
    "        - t-30min (instant)\n",
    "        - t-15min (instant)\n",
    "        - t-10min (instant)\n",
    "        - t-5min (instant)\n",
    "    '''\n",
    "    if auto is True:\n",
    "        c_data,t_data = cleaning_data(data,sigma_val=4,\n",
    "                                  safe_cols=[None])\n",
    "        c_i_data = sw_interp(c_data,method='linear')\n",
    "        \n",
    "        data = c_i_data\n",
    "    else:\n",
    "        pass\n",
    "    return pd.concat((data,\n",
    "                      t_hist(data,360,60).avg_hist(),\n",
    "                      t_hist(data,300,60).avg_hist(),\n",
    "                      t_hist(data,180,30).avg_hist(),\n",
    "                      t_hist(data,60,30).avg_hist(),\n",
    "                      t_hist(data,45,0).instant_hist(),\n",
    "                      t_hist(data,30,0).instant_hist(),\n",
    "                      t_hist(data,15,0).instant_hist(),\n",
    "                      t_hist(data,10,0).instant_hist(),\n",
    "                      t_hist(data,5,0).instant_hist()),axis=1)\n",
    "\n",
    "import ftplib\n",
    "import os\n",
    "\n",
    "def _is_ftp_dir(ftp_handle, name, guess_by_extension=True):\n",
    "    \"\"\" simply determines if an item listed on the ftp server is a valid directory or not \"\"\"\n",
    "\n",
    "    # if the name has a \".\" in the fourth to last position, its probably a file extension\n",
    "    # this is MUCH faster than trying to set every file to a working directory, and will work 99% of time.\n",
    "    if guess_by_extension is True:\n",
    "        if name[-4] == '.':\n",
    "            return False\n",
    "\n",
    "    original_cwd = ftp_handle.pwd()     # remember the current working directory\n",
    "    try:\n",
    "        ftp_handle.cwd(name)            # try to set directory to new name\n",
    "        ftp_handle.cwd(original_cwd)    # set it back to what it was\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _make_parent_dir(fpath):\n",
    "    \"\"\" ensures the parent directory of a filepath exists \"\"\"\n",
    "    dirname = os.path.dirname(fpath)\n",
    "    while not os.path.exists(dirname):\n",
    "        try:\n",
    "            os.mkdir(dirname)\n",
    "            print(\"created {0}\".format(dirname))\n",
    "        except:\n",
    "            _make_parent_dir(dirname)\n",
    "\n",
    "\n",
    "def _download_ftp_file(ftp_handle, name, dest, overwrite,datetime_start,datetime_end):\n",
    "    \"\"\" downloads a single file from an ftp server \"\"\"\n",
    "    _make_parent_dir(dest)\n",
    "    \n",
    "    if (name[-7:-1]!='SHA1SU'): #ignore SHA1SUM files\n",
    "        month = int(name[-15:-13])\n",
    "        day = int(name[-13:-11])\n",
    "        year = int(name[-19:-15])\n",
    "        base = datetime.datetime(year, month, day)\n",
    "        if (datetime_start <= base <= datetime_end):\n",
    "\n",
    "            if not os.path.exists(dest) or overwrite is True:\n",
    "                with open(dest, 'wb') as f:\n",
    "                    ftp_handle.retrbinary(\"RETR {0}\".format(name), f.write)\n",
    "                print(\"downloaded: {0}\".format(dest))\n",
    "            else:\n",
    "                print(\"already exists: {0}\".format(dest))\n",
    "\n",
    "\n",
    "def _mirror_ftp_dir(ftp_handle, name, overwrite, guess_by_extension,datetime_start,datetime_end):\n",
    "    \"\"\" replicates a directory on an ftp server recursively \"\"\"\n",
    "    for item in ftp_handle.nlst(name):\n",
    "        if _is_ftp_dir(ftp_handle, item):\n",
    "            _mirror_ftp_dir(ftp_handle, item, overwrite, guess_by_extension,datetime_start,datetime_end)\n",
    "        else:\n",
    "            _download_ftp_file(ftp_handle, item, item, overwrite,datetime_start,datetime_end)\n",
    "\n",
    "\n",
    "def download_ftp_tree(ftp_handle, path, destination, datetime_start,datetime_end, overwrite=False, guess_by_extension=True):\n",
    "    \"\"\"\n",
    "    Downloads an entire directory tree from an ftp server to the local destination\n",
    "\n",
    "    :param ftp_handle: an authenticated ftplib.FTP instance\n",
    "    :param path: the folder on the ftp server to download\n",
    "    :param destination: the local directory to store the copied folder\n",
    "    :param overwrite: set to True to force re-download of all files, even if they appear to exist already\n",
    "    :param guess_by_extension: It takes a while to explicitly check if every item is a directory or a file.\n",
    "        if this flag is set to True, it will assume any file ending with a three character extension \".???\" is\n",
    "        a file and not a directory. Set to False if some folders may have a \".\" in their names -4th position.\n",
    "    \"\"\"\n",
    "    os.chdir(destination)\n",
    "    _mirror_ftp_dir(ftp_handle, path, overwrite, guess_by_extension,datetime_start,datetime_end)\n",
    "    \n",
    "# def  get_data(datetime_start,datetime_end,sc_id):\n",
    "\n",
    "#     years = {6:[1987], 7:[1987],8:[1987],9:[1988]\n",
    "#              ,12:[2000,2001,2002], 13:[2000,2001,2002,2003,2004,2005,2006,2007],14:[2000,2001,2002,2003,2004,2005],\n",
    "#              15:[2000,2001,2002,2003,2004,2005,2006,2007,2008,2009],\n",
    "#              16:[2010,2011,2012,2013,2014,2015], #year 2003-2009 is not accessible on site\n",
    "#              17:[2009,2010,2011,2012,2013,2014,2015],\n",
    "#              18:[2010,2011,2012,2013,2014,2015],#2009 not accessible\n",
    "#             } \n",
    "    \n",
    "#     dmsp_feature_list = ['ELE_AVG_ENERGY','ELE_AVG_ENERGY_STD','ELE_TOTAL_ENERGY_FLUX','ELE_TOTAL_ENERGY_FLUX_STD',\n",
    "#                          'SC_AACGM_LAT',  'SC_AACGM_LON','SC_AACGM_LTIME',] \n",
    "#     #                     'ION_AVG_ENERGY','ION_AVG_ENERGY_STD','ION_TOTAL_ENERGY_FLUX','ION_TOTAL_ENERGY_FLUX_STD','SC_GEOCENTRIC_LAT','SC_GEOCENTRIC_LON','SC_GEOCENTRIC_R']\n",
    "#     dmsp_feature_list_19 =['ELE_COUNTS_BKG','ELE_COUNTS_OBS','ELE_DIFF_ENERGY_FLUX','ELE_DIFF_ENERGY_FLUX_STD',\n",
    "#                       'CHANNEL_ENERGIES','ELE_COUNTS_BKG','ELE_COUNTS_OBS',]\n",
    "#     #                  'ELE_GEOMETRIC','ION_COUNTS_BKG','ION_COUNTS_OBS','ION_DIFF_ENERGY_FLUX','ION_DIFF_ENERGY_FLUX_STD','ION_GEOMETRIC',\n",
    "#     #                  'SC_ECI','SC_ECI_LABEL']\n",
    "\n",
    "#     import glob\n",
    "#     all_sc_df = pd.DataFrame()\n",
    "#     count = 0\n",
    "#     for ii in [sc_id]:\n",
    "#         sc_df = pd.DataFrame()\n",
    "#         print(ii)\n",
    "#         directory = 'pub/data/dmsp/dmspf'\n",
    "#         if ii <10:\n",
    "#             directory = directory + '0'\n",
    "#         directory = directory + str(ii) + '/ssj/precipitating-electrons-ions'\n",
    "#         for year in [datetime_start.year]:\n",
    "#             print(year)\n",
    "            \n",
    "#             from ftplib import FTP_TLS\n",
    "#             ftp=FTP_TLS('cdaweb.gsfc.nasa.gov')\n",
    "#             ftp.login()\n",
    "#             ftp.dir()\n",
    "#             download_ftp_tree(ftp, directory, '.',datetime_start,datetime_end)\n",
    "            \n",
    "#             file_list=glob.glob(directory + '/'+str(year)+'/*')\n",
    "            \n",
    "            \n",
    "#             for file in file_list:\n",
    "#                 print(file)\n",
    "#                 try:\n",
    "\n",
    "#                     count = count+1\n",
    "#                     month = int(file[-15:-13])\n",
    "#                     day = int(file[-13:-11])\n",
    "#                     base = datetime.datetime(year, month, day)\n",
    "                    \n",
    "#                     if (datetime_start <= base <= datetime_end):\n",
    "#                         indices = np.array([base + datetime.timedelta(seconds=iii) for iii in range(0,60*24*60)])\n",
    "#                         df = pd.DataFrame(data=indices,columns=['index'])\n",
    "#                         cdf = pycdf.CDF(file)\n",
    "#                         count = count+1\n",
    "#                         for feature in dmsp_feature_list:\n",
    "#                             df[feature] = cdf[feature]\n",
    "\n",
    "#                         ch_energies = cdf['CHANNEL_ENERGIES']\n",
    "# #                         print(np.array(ch_energies))\n",
    "# #                         for i in range(0,19):\n",
    "# #                             print((ch_energies[i]))\n",
    "#                         ch_energies = np.flip((ch_energies[:]))\n",
    "#                         temp_jN = (pd.DataFrame(cdf['ELE_COUNTS_OBS'])- pd.DataFrame(cdf['ELE_COUNTS_BKG'])).values\n",
    "#                         df['ELE_TOTAL_COUNTS'] = temp_jN[:,0]*(ch_energies[1]-ch_energies[0])\n",
    "#                         + temp_jN[:,18]*(ch_energies[18]-ch_energies[17])\n",
    "#                         for i in range(1,18):\n",
    "#                             df['ELE_TOTAL_COUNTS'] = df['ELE_TOTAL_COUNTS']  +     temp_jN[:,i]*(ch_energies[i+1]-ch_energies[i-1])/2.\n",
    "\n",
    "#                         for i in range(0,19):\n",
    "#                             name = 'ELE_COUNT_'+str(i+1)\n",
    "#                             df[name]=temp_jN[:,i]\n",
    "#                         for i in range(0,19):\n",
    "#                             name = 'ELE_diff_'+str(i+1)\n",
    "#                             df[name]=cdf['ELE_DIFF_ENERGY_FLUX'][:,i]\n",
    "\n",
    "#                         df = df.set_index('index')\n",
    "#                         df.index = pd.to_datetime(df.index)\n",
    "\n",
    "#                         print('df',df.shape)#, df)\n",
    "#                         #Create a time window\n",
    "#                         df = df[df['ELE_TOTAL_ENERGY_FLUX'] > 0].dropna(subset=['ELE_TOTAL_ENERGY_FLUX'])\n",
    "\n",
    "#                          #Create a time window\n",
    "#                         sTimeIMF = datetime.datetime(year,month,day)\n",
    "#                         eTimeIMF = sTimeIMF + datetime.timedelta(hours = 24)\n",
    "\n",
    "#                         df_omni_5min = download_omni_data(sTimeIMF- datetime.timedelta(hours = 6),\n",
    "#                                                      eTimeIMF+ datetime.timedelta(hours = 6))\n",
    "#                         print('df_omni_5min',df_omni_5min.shape)#, df_omni_5min)\n",
    "#                         indices = np.array([sTimeIMF- datetime.timedelta(hours = 6)+ datetime.timedelta(minutes=5*iii) for iii in range(0,df_omni_5min.shape[0])])\n",
    "#                         print('indices',indices.shape)#, indices)\n",
    "\n",
    "#                         df_omni_5min = pd.DataFrame(data=df_omni_5min.values,columns=df_omni_5min.columns, index=indices)\n",
    "#                         print('df_omni_5min',df_omni_5min.shape)#, df_omni_5min)\n",
    "\n",
    "\n",
    "#                         # call time_history to clean up omnireader data        \n",
    "#                         df_omni_5min_cleaned = time_history(df_omni_5min)\n",
    "#                         print('df_omni_5min_cleaned',df_omni_5min_cleaned.shape)#,, df_omni_5min_cleaned)\n",
    "#                         df_omni_1min_cleaned = pd.DataFrame(np.repeat(df_omni_5min_cleaned.values,5*60,axis=0))\n",
    "#                         print('df_omni_5min_cleaned',df_omni_5min_cleaned.shape)#,, df_omni_5min_cleaned)\n",
    "\n",
    "#             #             #create the indices\n",
    "#                         indices = []\n",
    "#                         for index in df_omni_5min_cleaned.index:\n",
    "#                             for jj in range(0,60*5):\n",
    "#                                 indices.append(index+ datetime.timedelta(seconds=jj))\n",
    "#                         print('indices',len(indices))#, indices)\n",
    "\n",
    "#                         df_omni_1min_cleaned = pd.DataFrame(data=df_omni_1min_cleaned.values,columns=df_omni_5min_cleaned.columns,\n",
    "#                                                             index=indices)    \n",
    "#                         print('df_omni_5min_cleaned',df_omni_5min_cleaned.shape)#, df_omni_5min_cleaned)\n",
    "\n",
    "#                         intersection_indices = df_omni_1min_cleaned.index.intersection(df.index)\n",
    "#                         print('intersection_indices',intersection_indices.shape)#,, intersection_indices)\n",
    "\n",
    "#                         df = df.loc[intersection_indices]\n",
    "#                         print('df',df.shape)#, df)\n",
    "#                         df_omni_1min_cleaned = df_omni_1min_cleaned.loc[intersection_indices]\n",
    "#                         print('df_omni_5min_cleaned',df_omni_5min_cleaned.shape)#, df_omni_5min_cleaned)\n",
    "\n",
    "#                         #combine Omni and DMSP data\n",
    "#                         for feature in df_omni_1min_cleaned.columns:\n",
    "#                             df[feature] = df_omni_1min_cleaned[feature]  \n",
    "\n",
    "#                         sc_df = pd.concat([sc_df,df])\n",
    "#                         print('sc_df',sc_df.shape)#, sc_df)\n",
    "#                 except Exception as e: \n",
    "#                     print('Error')\n",
    "#                     print(e)\n",
    "#                     print(file)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "#             sc_df['SC_ID']= ii\n",
    "\n",
    "#             #filescname = \"./all_sc_df_1-sec_2010-\" + str(ii) + \".pkl\"\n",
    "#             #pickling_on = open(filescname,\"wb\")\n",
    "#             #pickle.dump(sc_df, pickling_on,protocol=4)\n",
    "\n",
    "#             all_sc_df = pd.concat([all_sc_df,sc_df])   \n",
    "#             #print(i, 'all_sc_df',all_sc_df.shape, all_sc_df)\n",
    "\n",
    "#     test=all_sc_df\n",
    "#     test['datetime']=test.index\n",
    "#     test=test.sort_values(by=['datetime'])\n",
    "#     test = test.set_index('datetime')\n",
    "\n",
    "#     # get rid of ones near equatoer\n",
    "#     test=test[np.abs(test['SC_AACGM_LAT'])>45]\n",
    "#     # swap by for southern hemisphere\n",
    "#     test.loc[test['SC_AACGM_LAT']<0 , 'By'] = -test.loc[test['SC_AACGM_LAT']<0 , 'By']\n",
    "#     #combine southern with northern hemisphere data\n",
    "#     test['SC_AACGM_LAT']=np.abs(test['SC_AACGM_LAT'])\n",
    "\n",
    "#     test['cos_SC_AACGM_LTIME']=np.cos(test['SC_AACGM_LTIME']*2*3.14159/24)\n",
    "#     test['sin_SC_AACGM_LTIME']=np.sin(test['SC_AACGM_LTIME']*2*3.14159/24)\n",
    "\n",
    "#     doy_loop = test.index.day\n",
    "#     ut_loop = test.index.hour*3600 + test.index.minute*60 + test.index.second\n",
    "#     test['sin_doy']= np.sin(2*np.pi*doy_loop/365.)\n",
    "#     test['cos_doy'] = np.cos(2*np.pi*doy_loop/365.)\n",
    "#     test['sin_ut'] = np.sin(2*np.pi*ut_loop/86400.)\n",
    "#     test['cos_ut'] = np.cos(2*np.pi*ut_loop/86400.)\n",
    "\n",
    "#     del doy_loop,ut_loop\n",
    "\n",
    "#     return test    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gets only the input data and no validation data \n",
    "\n",
    "def  get_data_inputs(datetime_start,datetime_end,sc_id):\n",
    "\n",
    "    base = datetime_start\n",
    "\n",
    "    if (1):\n",
    "\n",
    "         #Create a time window\n",
    "        sTimeIMF = datetime_start\n",
    "        eTimeIMF = sTimeIMF + datetime.timedelta(hours = 24)\n",
    "\n",
    "        df_omni_5min = download_omni_data(sTimeIMF- datetime.timedelta(hours = 6),\n",
    "                                     eTimeIMF+ datetime.timedelta(hours = 6))\n",
    "        print('df_omni_5min',df_omni_5min.shape)#, df_omni_5min)\n",
    "        indices = np.array([sTimeIMF- datetime.timedelta(hours = 6)+ datetime.timedelta(minutes=5*iii) for iii in range(0,df_omni_5min.shape[0])])\n",
    "        print('indices',indices.shape)#, indices)\n",
    "\n",
    "        df_omni_5min = pd.DataFrame(data=df_omni_5min.values,columns=df_omni_5min.columns, index=indices)\n",
    "        print('df_omni_5min',df_omni_5min.shape)#, df_omni_5min)\n",
    "\n",
    "\n",
    "        # call time_history to clean up omnireader data        \n",
    "        df_omni_5min_cleaned = time_history(df_omni_5min)\n",
    "        df = df_omni_5min_cleaned[6*12:288+6*12:2]\n",
    "\n",
    "    test=df\n",
    "    test['datetime']=test.index\n",
    "    test=test.sort_values(by=['datetime'])\n",
    "    test = test.set_index('datetime')\n",
    "\n",
    "    test['SC_AACGM_LAT']=0 # set to zero bc not used\n",
    "    test['SC_ID']=sc_id\n",
    "    test['cos_SC_AACGM_LTIME']=0 # set to zero bc not used\n",
    "    test['sin_SC_AACGM_LTIME']=0 # set to zero bc not used\n",
    "\n",
    "    doy_loop = test.index.day\n",
    "    ut_loop = test.index.hour*3600 + test.index.minute*60 + test.index.second\n",
    "    test['sin_doy']= np.sin(2*np.pi*doy_loop/365.)\n",
    "    test['cos_doy'] = np.cos(2*np.pi*doy_loop/365.)\n",
    "    test['sin_ut'] = np.sin(2*np.pi*ut_loop/86400.)\n",
    "    test['cos_ut'] = np.cos(2*np.pi*ut_loop/86400.)\n",
    "\n",
    "    del doy_loop,ut_loop\n",
    "\n",
    "    return test    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105120,)\n",
      "Created interval between 2013-03-16 and 2013-03-18, cadence 5min, start index 21528, end index 21960\n",
      "(8760,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in less\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created interval between 2013-03-16 and 2013-03-18, cadence hourly, start index 1794, end index 1830\n",
      "Applying transform Hourly Kp*10 -> Kp to omni hourly variable KP\n",
      "df_omni_5min (432, 16)\n",
      "indices (432,)\n",
      "df_omni_5min (432, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# get 1 second data \n",
    "\n",
    "# 16 is the sc_id, when predicting could just any of the numbers 6-18\n",
    "\n",
    "test = get_data_inputs( datetime.datetime(2013, 3, 17),datetime.datetime(2013, 3, 17),16)\n",
    "\n",
    "# test =  pd.concat([test, get_data_inputs( datetime.datetime(2013, 3, 17),datetime.datetime(2013, 3, 17),17)])\n",
    "# test = pd.concat([test, get_data_inputs( datetime.datetime(2013, 3, 17),datetime.datetime(2013, 3, 17),18)])\n",
    "\n",
    "# test has model input data at every second in the time range\n",
    "\n",
    "test=test.dropna() # probably not needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_hemisphere_CSV(scaler_X, model, features, test):\n",
    "\n",
    "    mark= ['s', 'o', 'D', 'v']\n",
    "\n",
    "    num = test.shape[0]\n",
    "     \n",
    "    ml_hemi = []\n",
    "    \n",
    "    for i in range(0,num):\n",
    "\n",
    "        fig= plt.figure(figsize=(20,12))\n",
    "        ax5 = plt.subplot2grid((1,2), (0, 1), rowspan=1,colspan=1,polar=True)\n",
    "\n",
    "       #################################\n",
    "        #\n",
    "        #ML model\n",
    "        ##########################\n",
    "        mlatgridN = np.linspace(90,45,num=90)\n",
    "        mltgridN =  np.linspace(0,24,num=96)  \n",
    "        model_input = np.zeros((90,96,len(features)))\n",
    "        flux = np.zeros((mlatgridN.shape[0], mltgridN.shape[0]))\n",
    "\n",
    "\n",
    "        for j in range(0,mlatgridN.shape[0]):\n",
    "            for k in range(0,mltgridN.shape[0]):        \n",
    "                #calc cos and sin\n",
    "                rads = mltgridN[k]*15*3.14159/180.\n",
    "                model_input[j,k,:]=test[features].values[i,:]\n",
    "                model_input[j,k,7]=np.cos(rads)\n",
    "                model_input[j,k,6]=np.sin(rads)\n",
    "                model_input[j,k,0]=mlatgridN[j]\n",
    "        shaped = np.reshape(model_input,(90*96,len(features)))\n",
    "        X_val_scaled = scaler_X.transform(shaped)\n",
    "        #get auroral region and flux\n",
    "        flux = 10**(np.reshape(model.predict(X_val_scaled),(90,96)))*1.60218e-12*3.14159\n",
    "\n",
    "\n",
    "        pcolor_kwargs = {'cmap':'gnuplot','vmin':(10**0)*1.60218e-12,'vmax':(10**12.5)*1.60218e-12*3.14159}\n",
    "        mappableN = pcolor_flux(ax5,mlatgridN,mltgridN,flux,'N',**pcolor_kwargs)\n",
    "\n",
    "        ax5.set_title('Predicted Electron Precipitation Energy Flux (Neural Net)',fontweight=\"bold\", fontsize='medium',pad=10)\n",
    "        ax5.set_theta_zero_location('S')\n",
    "        theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "        theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "        ax5.set_thetagrids(theta_label_values,labels=theta_labels)\n",
    "\n",
    "        r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "        r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "        ax5.set_rgrids(r_label_values,labels=r_labels)\n",
    "        ax5.set_rlim([0.,45.])\n",
    "\n",
    "        plt.colorbar(mappableN,ax=ax5,label='Total Flux [erg/cm^2/s]',fraction=0.05,pad=0.09)\n",
    "          \n",
    "        ax5.tick_params(axis='y', colors='white')\n",
    "\n",
    "            \n",
    "        fig.tight_layout() \n",
    "#         name = 'figures/movie_'+str(i) + '.png'\n",
    "#         fig.savefig(name,dpi=200)\n",
    "        \n",
    "        ml_hemi.append(flux)\n",
    "    \n",
    "        \n",
    "    plt.show()\n",
    "#     os.system('ffmpeg -r:v 5 -i \"figures/movie_%01d.png\" -codec:v libx264 -preset veryslow  \"movie_2013_SC17.mp4\";ffmpeg -i movie_2013_SC17.mp4 -c:v libx264 -c:a libmp3lame -b:a 384K movie_2013_SC17.avi')\n",
    "\n",
    "    return ml_hemi\n",
    "\n",
    "                    \n",
    "def plot_hemisphere_CSV_counts(scaler_X, model, features, test):\n",
    "\n",
    "    mark= ['s', 'o', 'D', 'v']\n",
    "\n",
    "    num = test.shape[0]\n",
    "    \n",
    "    \n",
    "    ml_hemi = []\n",
    "    \n",
    "    for i in range(0,num):\n",
    "\n",
    "        fig= plt.figure(figsize=(20,12))\n",
    "#         ax4 = plt.subplot2grid((1,2), (0,0), rowspan=1,colspan=1,polar=True)\n",
    "        ax5 = plt.subplot2grid((1,2), (0, 1), rowspan=1,colspan=1,polar=True)\n",
    "\n",
    "       #################################\n",
    "        #\n",
    "        #ML model\n",
    "        ##########################\n",
    "        mlatgridN = np.linspace(90,45,num=90)\n",
    "        mltgridN =  np.linspace(0,24,num=96)  \n",
    "        model_input = np.zeros((90,96,len(features)))\n",
    "        flux = np.zeros((mlatgridN.shape[0], mltgridN.shape[0]))*3.14159\n",
    "\n",
    "\n",
    "        for j in range(0,mlatgridN.shape[0]):\n",
    "            for k in range(0,mltgridN.shape[0]):        \n",
    "                #calc cos and sin\n",
    "                rads = mltgridN[k]*15*3.14159/180.\n",
    "                model_input[j,k,:]=test[features].values[i,:]\n",
    "                model_input[j,k,7]=np.cos(rads)\n",
    "                model_input[j,k,6]=np.sin(rads)\n",
    "                model_input[j,k,0]=mlatgridN[j]\n",
    "        shaped = np.reshape(model_input,(90*96,len(features)))\n",
    "        X_val_scaled = scaler_X.transform(shaped)\n",
    "        #get auroral region and flux\n",
    "        flux = 10**(np.reshape(model.predict(X_val_scaled),(90,96)))*3.14159\n",
    "\n",
    "\n",
    "        pcolor_kwargs = {'cmap':'gnuplot','vmin':(10**0),'vmax':(10**8)*3.14159/2}\n",
    "        mappableN = pcolor_flux(ax5,mlatgridN,mltgridN,flux,'N',**pcolor_kwargs)\n",
    "\n",
    "        ax5.set_title('Predicted Electron Precipitation Number Flux (Neural Net)',fontweight=\"bold\", fontsize='medium',pad=10)\n",
    "        ax5.set_theta_zero_location('S')\n",
    "        theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "        theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "        ax5.set_thetagrids(theta_label_values,labels=theta_labels)\n",
    "\n",
    "        r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "        r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "        ax5.set_rgrids(r_label_values,labels=r_labels)\n",
    "        ax5.set_rlim([0.,45.])\n",
    "\n",
    "\n",
    "        plt.colorbar(mappableN,ax=ax5,label='Total Flux [#/cm^2/s]',fraction=0.05,pad=0.09)\n",
    "\n",
    "           \n",
    "\n",
    "        ax5.tick_params(axis='y', colors='white') \n",
    "       \n",
    "        fig.tight_layout() \n",
    "#         name = 'figures/movie_counts_'+str(i) + '.png'\n",
    "#         fig.savefig(name,dpi=200)\n",
    "        \n",
    "        ml_hemi.append(flux)\n",
    "\n",
    "    \n",
    "        \n",
    "    plt.show()\n",
    "#     os.system('ffmpeg -r:v 5 -i \"figures/movie_counts_%01d.png\" -codec:v libx264 -preset veryslow  \"movie_counts_2013_SC17.mp4\";ffmpeg -i movie_counts_2013_SC17.mp4 -c:v libx264 -c:a libmp3lame -b:a 384K movie_counts_2013_SC17.avi;')\n",
    "    return ml_hemi\n",
    "\n",
    "\n",
    "def latlt2polar(lat,lt,hemisphere):\n",
    "    \"\"\"\n",
    "    Converts an array of latitude and lt points to polar for a top-down dialplot (latitude in degrees, LT in hours)\n",
    "    i.e. makes latitude the radial quantity and MLT the azimuthal \n",
    "    get the radial displacement (referenced to down from northern pole if we want to do a top down on the north, \n",
    "    or up from south pole if visa-versa)\n",
    "    \"\"\"\n",
    "    from numpy import pi\n",
    "    if hemisphere=='N':\n",
    "        r = 90.-lat\n",
    "    elif hemisphere=='S':\n",
    "        r = 90.-(-1*lat)\n",
    "    else:\n",
    "        raise ValueError('%s is not a valid hemisphere, N or S, please!' % (hemisphere))\n",
    "    #convert lt to theta (azimuthal angle) in radians\n",
    "    theta = lt/24. * 2*pi\n",
    "\n",
    "    #the pi/2 rotates the coordinate system from\n",
    "    #theta=0 at negative y-axis (local time) to\n",
    "    #theta=0 at positive x axis (traditional polar coordinates)\n",
    "    return r,theta\n",
    "\n",
    "def polar2dial(ax):\n",
    "    \"\"\"\n",
    "    Turns a matplotlib axes polar plot into a dial plot\n",
    "    \"\"\"\n",
    "    #Rotate the plot so that noon is at the top and midnight\n",
    "    #is at the bottom, and fix the labels so radial direction\n",
    "    #is latitude and azimuthal direction is local time in hours\n",
    "    ax.set_theta_zero_location('S')\n",
    "    theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "    theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "    ax.set_thetagrids(theta_label_values,labels=theta_labels)\n",
    "\n",
    "    r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "    r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "    ax.set_rgrids(r_label_values,labels=r_labels)\n",
    "    ax.set_rlim([0.,40.])\n",
    "\n",
    "def pcolor_flux(ax,mlatgrid,mltgrid,fluxgrid,hemisphere,**pcolor_kwargs):\n",
    "    mlats,mlts = mlatgrid.flatten(),mltgrid.flatten()\n",
    "    flux = fluxgrid.flatten()\n",
    "    if 'vmin' not in pcolor_kwargs:\n",
    "        pcolor_kwargs['vmin'] = np.nanpercentile(flux,5)\n",
    "    if 'vmax' not in pcolor_kwargs:\n",
    "        pcolor_kwargs['vmax'] = np.nanpercentile(flux,95)\n",
    "    r,theta = latlt2polar(mlats,mlts,hemisphere)\n",
    "    rgrid = r.reshape(mlatgrid.shape)\n",
    "    thetagrid = theta.reshape(mltgrid.shape)\n",
    "    mappable = ax.pcolormesh(thetagrid,rgrid,fluxgrid,**pcolor_kwargs)\n",
    "    return mappable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For both the energy flux and number flux models, they use the same inputs and scaling, outputs are different\n",
    "#      and the loss functions are different\n",
    "\n",
    "features=['SC_AACGM_LAT', 'SC_ID', 'sin_ut',\n",
    "                             'cos_ut', 'sin_doy', 'cos_doy', 'sin_SC_AACGM_LTIME', 'cos_SC_AACGM_LTIME',\n",
    "                             'F107', 'AE', 'AL', 'AU', 'SymH', \n",
    "                             'F107_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr',\n",
    "                              'F107_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', \n",
    "                             'F107_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', \n",
    "                             'F107_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr']\n",
    "\n",
    "with open ('scalar_X_33_new_DB.pkl', 'rb') as f:\n",
    "    scaler_X =pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: tail_loss_new_pipeline_33/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-489098cb16ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tail_loss_new_pipeline_33'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel_energy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'custom_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcustom_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    111\u001b[0m                   (export_dir,\n\u001b[1;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: tail_loss_new_pipeline_33/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "# this is for the energy flux model\n",
    "\n",
    "def custom_loss(y_true, y_pred):  \n",
    "\n",
    "    loss = K.mean( \n",
    "        2.5*(y_true-y_pred)*(y_true-y_pred)\n",
    "        *K.cast(K.greater(y_true,11.5),'float32')*K.cast(K.less_equal(y_pred,11.5),'float32') +\n",
    "        5*(y_true-y_pred)*(y_true-y_pred) \n",
    "        *K.cast(K.greater(y_true,12.),'float32')*K.cast(K.less_equal(y_pred,12.0),'float32') +\n",
    "        10*(y_true-y_pred)*(y_true-y_pred)\n",
    "        *K.cast(K.greater(y_true,12.5),'float32')*K.cast(K.less_equal(y_pred,12.5),'float32')  \n",
    "        +\n",
    "        10*(y_true-y_pred)*(y_true-y_pred)\n",
    "        *K.cast(K.greater(y_true,12.75),'float32')*K.cast(K.less_equal(y_pred,12.75),'float32')  \n",
    "        +\n",
    "        10*(y_true-y_pred)*(y_true-y_pred)\n",
    "        *K.cast(K.greater(y_true,13.),'float32')*K.cast(K.less_equal(y_pred,13.),'float32')  \n",
    "    )             \n",
    "                 \n",
    "            \n",
    "    loss =  loss+K.mean((y_true-y_pred)*(y_true-y_pred))\n",
    "\n",
    "    return loss\n",
    "                             \n",
    "filename='tail_loss_new_pipeline_33'\n",
    "model_energy = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for the number flux model\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "\n",
    "    loss = K.mean( \n",
    "        2.5*(y_true-y_pred)*(y_true-y_pred)\n",
    "        *K.cast(K.greater(y_true,6.25),'float32')*K.cast(K.less_equal(y_pred,6.25),'float32') +\n",
    "        5*(y_true-y_pred)*(y_true-y_pred) \n",
    "        *K.cast(K.greater(y_true,6.75),'float32')*K.cast(K.less_equal(y_pred,6.75),'float32') +\n",
    "        10*(y_true-y_pred)*(y_true-y_pred)\n",
    "        *K.cast(K.greater(y_true,7),'float32')*K.cast(K.less_equal(y_pred,7),'float32')  \n",
    "        +\n",
    "        10*(y_true-y_pred)*(y_true-y_pred)\n",
    "        *K.cast(K.greater(y_true,7.25),'float32')*K.cast(K.less_equal(y_pred,7.25),'float32')  \n",
    "        +\n",
    "        10*(y_true-y_pred)*(y_true-y_pred)\n",
    "        *K.cast(K.greater(y_true,7.5),'float32')*K.cast(K.less_equal(y_pred,7.5),'float32')  \n",
    "    )             \n",
    "                 \n",
    "            \n",
    "    loss =  loss+K.mean((y_true-y_pred)*(y_true-y_pred))\n",
    "\n",
    "    return loss\n",
    "\n",
    "filename='counts_tail_loss_new_pipeline_33'\n",
    "model_number = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW only evalvuating the model every 600 seconds (10 min)\n",
    "test=test[0:test.shape[0]:60*10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model at all times in the dataframe \"test\".  the output is for a data frame spanning many m_latitudes and m_local_times\n",
    "\n",
    "ml_energy = plot_hemisphere_CSV(scaler_X, model_energy, features,test)\n",
    "\n",
    "ml_counts = plot_hemisphere_CSV_counts(scaler_X, model_number, features,test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "mlatgridN = np.linspace(90,45,num=90)\n",
    "mltgridN =  np.linspace(0,24,num=96)  \n",
    "df = pd.DataFrame(columns=['SC_AACGM_LAT_deg', 'SC_AACGM_LTIME_hr', 'ELE_TOTAL_ENERGY_FLUX_erg_per_cm2_s','ELE_TOTAL_COUNTS_#_per_cm2_s','time'])\n",
    "#, index=test10min.index)\n",
    "for i in range(0, test.shape[0] ):\n",
    "    dt = test.index[i]\n",
    "    for j in range(0,90):\n",
    "        mlat = mlatgridN[j]\n",
    "        for k in range(0,96):\n",
    "            mlt = mltgridN[k]\n",
    "            df.loc[n]=[mlat, mlt, ml_energy[i][j,k],ml_counts[i][j,k],dt]\n",
    "            n=n+1\n",
    "df = df.set_index('time')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "df.to_csv('data_march_17_2013_hemisphere_10min_cadence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
