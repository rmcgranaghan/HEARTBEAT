{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-81e0a3f4a125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_pruned_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mInteractiveSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mInteractiveSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_session_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "\n",
    "os.system('source /home/jackalak/Downloads/cdf37_1-dist/bin/definitions.B')\n",
    "os.environ[\"CDF_LIB\"] = '/home/jackalak/Downloads/cdf37_1-dist/lib'\n",
    "from spacepy import pycdf\n",
    "\n",
    "from ovationpyme.ovation_prime import FluxEstimator,AverageEnergyEstimator,BinCorrector\n",
    "from ovationpyme.ovation_utilities import calc_avg_solarwind\n",
    "from ovationpyme.ovation_plotting import latlt2polar,polar2dial,pcolor_flux\n",
    "from nasaomnireader import omnireader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 14}\n",
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12) \n",
    "matplotlib.rc('font', **font)\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 9\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "import ftplib\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.keras.backend.set_floatx(\n",
    "    'float32'\n",
    ")\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "import tensorflow\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import datetime\n",
    "from os.path import isfile, join\n",
    "from sys import getsizeof\n",
    "import glob\n",
    "\n",
    "from random import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "\n",
    "# os.system('mkdir figures2')\n",
    "# os.system('mkdir figures2/energyflux')\n",
    "# os.system('mkdir figures2/numberflux')\n",
    "# os.system('mkdir figures2/channelnumberflux')\n",
    "# os.system('mkdir figures2/numberflux_from_channels')\n",
    "# os.system('mkdir figures')\n",
    "\n",
    "os.system('source /home/jackalak/heartbeat/cdf38_0-dist/bin/definitions.B')\n",
    "os.environ[\"CDF_LIB\"] = '/home/jackalak/heartbeat/cdf38_0-dist/lib'\n",
    "from spacepy import pycdf\n",
    "import matplotlib.pyplot as plt\n",
    "from ovationpyme.ovation_prime import FluxEstimator,AverageEnergyEstimator,BinCorrector\n",
    "from ovationpyme.ovation_utilities import calc_avg_solarwind\n",
    "from ovationpyme.ovation_plotting import latlt2polar,polar2dial,pcolor_flux\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "from nasaomnireader import omnireader\n",
    "\n",
    "import ftplib\n",
    "import os\n",
    "\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pickle\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import datetime\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "import tensorflow\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "from sklearn import preprocessing\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 14}\n",
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12) \n",
    "matplotlib.rc('font', **font)\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 13, 10\n",
    "\n",
    "import datetime\n",
    "from os.path import isfile, join\n",
    "from sys import getsizeof\n",
    "from random import *\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/tensorflow/tensorflow/issues/956\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import InputSpec \n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "\n",
    "class PeriodicPadding2D(layers.Layer):\n",
    "\n",
    "  def __init__(self, padding=1, **kwargs):\n",
    "    super(PeriodicPadding2D, self).__init__(**kwargs)\n",
    "    self.padding = conv_utils.normalize_tuple(padding, 1, 'padding')\n",
    "    self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "  def wrap_pad(self, input, size):\n",
    "    M1 = tf.concat([input[:,:, -size:], input, input[:,:, 0:size]], 2)\n",
    "    M1 = tf.concat([M1[:,0:size, :], M1, M1[:,0:size, :]], 1) #not periodic\n",
    "    return M1\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  \n",
    "    if shape[1] is not None:\n",
    "      length = shape[1] + 2*self.padding[0]\n",
    "    else:\n",
    "      length = None\n",
    "    return tuple([shape[0], length, length])\n",
    "\n",
    "  def call(self, inputs): \n",
    "    return self.wrap_pad(inputs, self.padding[0])\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {'padding': self.padding}\n",
    "    base_config = super(PeriodicPadding2D, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "  \n",
    "    loss = K.sum( K.cast(K.greater(y_true, 0),'float64')*\n",
    "               K.square(y_true-y_pred) )  / ( \n",
    "                K.sum( K.cast(K.greater(y_true, 0),'float64') ))# finds the number of y_true  > 0\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    "    \n",
    "\n",
    "    \n",
    "    mse = K.sum( K.cast(K.greater(y_true, 0),'float64')*(\n",
    "        K.square(y_true-y_pred)) )/params['batch_size']  \n",
    "    return mse\n",
    "\n",
    "def for_CSV_val(scaler_X, model, features, test, XX_test):\n",
    "\n",
    "\n",
    "    num = test.shape[0]\n",
    "    \n",
    "\n",
    "    result_val = []\n",
    "\n",
    "    \n",
    "    for i in range(0,num):\n",
    "\n",
    "\n",
    "       #################################\n",
    "        #\n",
    "        #ML model\n",
    "        ##########################\n",
    "        results = model.predict( XX_test[i:i+1,:])\n",
    "        flux = results[0,:,:,0]             \n",
    "        pt = i\n",
    "\n",
    "        result_val.append( \n",
    "            flux[int((90-test['SC_AACGM_LAT'][pt])/45*128),int((test['SC_AACGM_LTIME'][pt])/24*128)] )     \n",
    "    \n",
    "        \n",
    "\n",
    "    return result_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(df_results, test):\n",
    "    \n",
    "    subdir = 'figures2/energyflux/'\n",
    "\n",
    "    y_val_log = np.log10(test['ELE_TOTAL_ENERGY_FLUX'])\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Electron Total Energy Flux, log10 scale')    \n",
    "    plt.plot(y_val_log[:],alpha=0.5)\n",
    "    plt.plot(df_results[:],alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('log10 eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'1',dpi=dpi)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Electron Total Energy Flux')   \n",
    "    plt.plot(10**y_val_log[:]*1.6e-6,alpha=0.5)\n",
    "    plt.plot(10**df_results[:]*1.6e-6,alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('erg/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'2',dpi=dpi)\n",
    "    start=int(y_val_log.shape[0]/2)\n",
    "    plt.figure()\n",
    "    plt.title('Electron Total Energy Flux, log10 scale')   \n",
    "    plt.plot(y_val_log[start:2000+start],alpha=0.5)\n",
    "    plt.plot(df_results[start:2000+start],alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('log10 eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'3',dpi=dpi)\n",
    "    plt.figure()\n",
    "    plt.title('Electron Total Energy Flux')   \n",
    "    plt.plot(10**y_val_log[start:2000+start]*1.6e-6,alpha=0.5)\n",
    "    plt.plot(10**df_results[start:2000+start]*1.6e-6,alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('erg/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'4',dpi=dpi)\n",
    "    \n",
    "\n",
    "    minr = np.min(y_val_log.values)\n",
    "    maxr = np.max(y_val_log.values)\n",
    "    plt.figure()\n",
    "    plt.title('Histogram of Electron Total Energy Flux, log10 scale')   \n",
    "    plt.hist(y_val_log.values,bins=200,alpha=0.5,range=(minr,maxr))\n",
    "    plt.hist(df_results.values,bins=200,alpha=0.5,range=(minr,maxr))\n",
    "    plt.legend(['val','result'], loc='upper left')\n",
    "    plt.ylabel('#/bin')\n",
    "    plt.xlabel('log10 eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'5',dpi=dpi)\n",
    "    plt.figure()\n",
    "    plt.title('Histogram of Electron Total Energy Flux')   \n",
    "    plt.hist(10**y_val_log.values*1.6e-6,bins=100, log=True,range=(10**minr*1.6e-6,10**maxr*1.6e-6),alpha=0.5)\n",
    "    plt.hist(10**df_results.values*1.6e-6,bins=100, log=True,range=(10**minr*1.6e-6,10**maxr*1.6e-6),alpha=0.5)\n",
    "    plt.legend(['val', 'result'], loc='upper left')\n",
    "    plt.ylabel('#/bin')\n",
    "    plt.xlabel('erg/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'6',dpi=dpi)\n",
    "    import matplotlib.colors as mcolors\n",
    "    gamma = 0.2#[0.8, 0.5, 0.3]\n",
    "\n",
    "    errors= y_val_log.values-df_results.values[:,0]\n",
    "    plt.figure();\n",
    "    plt.hist2d(test['SC_AACGM_LAT'].values, errors,\n",
    "                  bins=50, norm=mcolors.PowerNorm(gamma))\n",
    "    plt.title('Error density over SC_AACGM_LAT Bins')\n",
    "    plt.xlabel('SC_AACGM_LAT degrees')\n",
    "    plt.ylabel('log10(y_true)-log10(y_pred) eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'7',dpi=dpi)\n",
    "    plt.figure()\n",
    "    bin_total = np.zeros((200))\n",
    "    bin_error_total = np.zeros((200))\n",
    "    for j in range(0,y_val_log.values.shape[0]):\n",
    "        i = int((test['SC_AACGM_LAT'].values[j]-45)/((90-45)/200))\n",
    "        if i < 200:\n",
    "            bin_total[i] = bin_total[i]+1\n",
    "            bin_error_total[i] = bin_error_total[i] + np.abs(errors[j])\n",
    "\n",
    "    avg_error_over_hist = bin_error_total/bin_total\n",
    "    plt.scatter(np.linspace(45,90,num=200),avg_error_over_hist)\n",
    "    plt.title('Average Validation Error over SC_AACGM_LAT Bins')\n",
    "    plt.xlabel('SC_AACGM_LAT degrees')\n",
    "    plt.ylabel('log10(y_true)-log10(y_pred) eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'8',dpi=dpi)\n",
    "    bin_total = np.zeros((200))\n",
    "    bin_error_total = np.zeros((200))\n",
    "    for j in range(0,y_val_log.values.shape[0]):\n",
    "        i = int((y_val_log[j]-minr)/((maxr-minr)/200))\n",
    "        if i < 200:\n",
    "            bin_total[i] = bin_total[i]+1\n",
    "            bin_error_total[i] = bin_error_total[i] + np.abs(errors[j])\n",
    "\n",
    "    avg_error_over_hist = bin_error_total/(bin_total+.00001)\n",
    "    plt.figure()\n",
    "    plt.scatter(np.linspace(minr,maxr,num=200),avg_error_over_hist)\n",
    "    plt.title('Average Validation Error over target Bins')    \n",
    "    plt.xlabel('log10(y_true) eV/cm^2/ster/s')\n",
    "    plt.ylabel('log10(y_true)-log10(y_pred) eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'9',dpi=dpi)\n",
    "    import matplotlib.colors as mcolors\n",
    "    gamma = 0.2#[0.8, 0.5, 0.3]\n",
    "    errors= y_val_log.values-df_results.values[:,0]\n",
    "\n",
    "    plt.figure();\n",
    "    plt.hist2d(y_val_log.values, errors,\n",
    "                  bins=50, norm=mcolors.PowerNorm(gamma))\n",
    "    plt.colorbar()\n",
    "    plt.title('Error Density')\n",
    "    plt.xlabel('log10(y_true) eV/cm^2/ster/s')\n",
    "    plt.ylabel('log10(y_true)-log10(y_pred) eV/cm^2/ster/s')\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'10',dpi=dpi)\n",
    "    plt.figure();\n",
    "    plt.hist2d(y_val_log.values, df_results.values[:,0],\n",
    "                  bins=50, norm=mcolors.PowerNorm(gamma))\n",
    "    plt.colorbar()\n",
    "    plt.title('Error Density')\n",
    "    plt.xlabel('log10(y_true) eV/cm^2/ster/s')\n",
    "    plt.ylabel('log10(y_pred) eV/cm^2/ster/s')\n",
    "    temp = np.array([7,8,9,10,11,12,13])\n",
    "    plt.plot(temp,temp,color='k')\n",
    "    plt.xlim([7,13])\n",
    "    plt.ylim([7,13])\n",
    "    plt.show()\n",
    "    plt.savefig(subdir+'11',dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_omni_data(t_start, t_end):\n",
    "\n",
    "    #--------------------------------------------------------#\n",
    "    #\tOMNI Data - includes solar wind, and geomag params   #\n",
    "    #--------------------------------------------------------#\n",
    "\n",
    "    #get OMNI data\n",
    "    omniInt = omnireader.omni_interval(t_start,t_end,'5min', cdf_or_txt = 'txt')\n",
    "\n",
    "    #print(omniInt.cdfs[0].vars) #prints all the variables available on omni\n",
    "\n",
    "    epochs = omniInt['Epoch'] #time array for omni 5min data\n",
    "    By,Bz,AE,SymH = omniInt['BY_GSM'],omniInt['BZ_GSM'],omniInt['AE_INDEX'], omniInt['SYM_H']\n",
    "    AL, AU = omniInt['AL_INDEX'],omniInt['AU_INDEX']\n",
    "    vsw,psw = omniInt['flow_speed'], omniInt['Pressure']\n",
    "    borovsky_reader = omnireader.borovsky(omniInt)\n",
    "    borovsky = borovsky_reader()\n",
    "    #newell_reader = omnireader.newell(omniInt)\n",
    "    #newell = newell_reader()\n",
    "\n",
    "    def NewellCF_calc(v,bz,by):\n",
    "        # v expected in km/s\n",
    "        # b's expected in nT    \n",
    "        NCF = np.zeros_like(v)\n",
    "        NCF.fill(np.nan)\n",
    "        bt = np.sqrt(by**2 + bz**2)\n",
    "        bztemp = bz\n",
    "        bztemp[bz == 0] = .001\n",
    "        #Caculate clock angle (theta_c = t_c)\n",
    "        tc = np.arctan2(by,bztemp)\n",
    "        neg_tc = bt*np.cos(tc)*bz < 0 \n",
    "        tc[neg_tc] = tc[neg_tc] + np.pi\n",
    "        sintc = np.abs(np.sin(tc/2.))\n",
    "        NCF = (v**1.33333)*(sintc**2.66667)*(bt**0.66667)\n",
    "        return NCF\n",
    "\n",
    "\n",
    "    newell = NewellCF_calc(vsw, Bz, By)\n",
    "\n",
    "\n",
    "    # \tproton_flux_10MeV, proton_flux_30MeV, proton_flux_60MeV = omniInt['PR-FLX_10'], omniInt['PR-FLX_30'], omniInt['PR-FLX_60']\n",
    "\n",
    "\n",
    "    #calculate clock angle\n",
    "    clock_angle = np.degrees(np.arctan2( By,Bz))\n",
    "    clock_angle[clock_angle < 0] = clock_angle[clock_angle<0] + 360.\n",
    "\n",
    "    #print('Got 5 minutes data')\n",
    "\n",
    "    omniInt_1hr = omnireader.omni_interval(t_start,t_end,'hourly', cdf_or_txt = 'txt')\n",
    "    F107,KP = omniInt_1hr['F10_INDEX'], omniInt_1hr['KP']\n",
    "    KP = pd.DataFrame(np.repeat(KP,12,axis=0))\n",
    "    F107 = pd.DataFrame(np.repeat(F107,12,axis=0))\n",
    "\n",
    "\n",
    "\n",
    "    #put all in a dataframe and save\n",
    "\n",
    "    dataframe = pd.DataFrame()\n",
    "    dataframe['Bz'] = Bz\n",
    "    dataframe['By'] = By\n",
    "    dataframe['Vsw'] = vsw\n",
    "    dataframe['Vx'] = omniInt['Vx']\n",
    "    dataframe['Psw'] = psw\n",
    "    dataframe['AE'] = AE\n",
    "    dataframe['AL'] = AL\n",
    "    dataframe['AU'] = AU\n",
    "    dataframe['SymH'] = SymH\n",
    "    dataframe['Clock Angle'] = clock_angle\n",
    "    dataframe['newell'] = newell\n",
    "    dataframe['borovsky'] = borovsky\n",
    "    dataframe['Kp'] = KP\n",
    "    dataframe['F107'] = F107\n",
    "    dataframe['PC'] = omniInt['PC_N_INDEX']\n",
    "    dataframe['Bx'] = omniInt['BX_GSE']\n",
    "    # \tdataframe = dataframe.replace(9999.99, np.nan) #replace 9999.99 with nans ??????????????\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "class t_hist():\n",
    "    '''\n",
    "    Class which takes solar wind data and creates some time history\n",
    "    for some specific time.\n",
    "    \n",
    "    Input:\n",
    "        Data ---------- A DataFrame of solar wind data at 5 minute\n",
    "                        cadence and datetime index.\n",
    "        Historic_time - The number of minutes into the past you\n",
    "                        would like the hisotry for. (E.g. for 1hr ago\n",
    "                        you would input 60 minutes).\n",
    "        window_mins --- If averaging for the time history, then this\n",
    "                        input specifies the window length, in minutes,\n",
    "                        centred on the historic_time specified.\n",
    "    '''\n",
    "    def __init__(self,data,historic_time,window_mins):\n",
    "        self.data = data\n",
    "        self.time = historic_time\n",
    "        self.window = window_mins\n",
    "        \n",
    "    def avg_hist(self):\n",
    "        '''\n",
    "        Function which returns a historic_time value, averaged over\n",
    "        the window_mins.\n",
    "        \n",
    "        Output:\n",
    "            - A dataframe of values of the time history.\n",
    "        '''\n",
    "        # Check that indices are datetime\n",
    "        self.is_datetime()\n",
    "        \n",
    "        if self.time % 60:\n",
    "            raise ValueError('Please choose a historic time value '+\n",
    "                             'which correspond to an integer '+\n",
    "                             'number of hours!')\n",
    "        \n",
    "        window_s = timedelta(minutes = self.time + self.window/2.0)\n",
    "        # '+5' ensures that the window is closed on the right\n",
    "        window_e = timedelta(minutes = self.time + 5 -\n",
    "                             self.window/2.0)\n",
    "        indices = self.data.index\n",
    "        \n",
    "        hist = [self.data[i-window_s : i-window_e].mean().values\n",
    "                for i in indices]\n",
    "        \n",
    "        col_label = '_'+str(self.time/60.0)[0]+'hr'\n",
    "        columns = [i+col_label for i in self.data.columns]\n",
    "        \n",
    "        th_df = pd.DataFrame(hist, index=indices,columns=columns)\n",
    "        \n",
    "        return th_df[th_df.index[0]+window_s:]\n",
    "    \n",
    "    def instant_hist(self):\n",
    "        '''\n",
    "        Function which returns an instantaneous historic_time value.\n",
    "        \n",
    "        Output:\n",
    "            - A dataframe of instantaneous values corresponding to\n",
    "              historic_time minutes in the past.\n",
    "        '''\n",
    "        # Check that indices are datetime\n",
    "        self.is_datetime()\n",
    "        \n",
    "        if self.time % 5:\n",
    "            raise ValueError('Please choose a historic time value '+\n",
    "                             'which correspond to a multiple of 5 '+\n",
    "                             'minutes!')\n",
    "            \n",
    "        t_offset = timedelta(minutes=self.time)\n",
    "        indices = self.data.index\n",
    "        \n",
    "\n",
    "        hist = [self.data.loc[i-t_offset].values\n",
    "                 for i in self.data.index\n",
    "                 if i >= indices[0]+t_offset]\n",
    "        \n",
    "        if self.time < 60:\n",
    "            if self.time >= 10:\n",
    "                col_label = '_'+str(self.time)[0:2]+'min'\n",
    "            else:\n",
    "                col_label = '_'+str(self.time)[0]+'min'\n",
    "        else:\n",
    "            col_label = '_'+str(self.time/60.0)[0]+'hr_I'\n",
    "        columns = [i+col_label for i in self.data.columns]\n",
    "        return pd.DataFrame(hist, index=indices[int(self.time/5):],\n",
    "                            columns=columns)\n",
    "    \n",
    "    def is_datetime(self):\n",
    "        dt_type = pd.core.indexes.datetimes.DatetimeIndex\n",
    "        if type(self.data.index) != dt_type:\n",
    "            raise ValueError('Dataframe index is not '+\n",
    "                             'in the correct datetime '+\n",
    "                             'format')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "def cleaning_data(data,safe_cols=[],sigma_val=4):\n",
    "    '''\n",
    "    Function which removes data which is 'sigma_val' stdevs from\n",
    "    the mean.\n",
    "\n",
    "    Note: 4 sigma encompasses ~99.994% of the data.\n",
    "          ~1 real piece of 5 min data is removed for\n",
    "          every 55 days of such data (assuming Gaussian).\n",
    "\n",
    "    Inputs:\n",
    "    sigma_val - (float) number of standard deviations from the\n",
    "                mean to consider as the limit of 'good' data.\n",
    "    safe_cols - Columns in the data which one might like to\n",
    "                keep without any changes (i.e., if there are\n",
    "                no null values in the initial dataset etc.).\n",
    "\n",
    "    Returns:\n",
    "     - Cleaned solar wind data dataframe.\n",
    "     - Dataframe of 'bad' solar wind data.\n",
    "    '''\n",
    "\n",
    "#   Initialising data and empty lists\n",
    "    sw_df = data\n",
    "    cleaned_cols = []\n",
    "    trash_data = []\n",
    "#   Looping through dataframe columns and removing 'bad' values\n",
    "    for i in sw_df.columns:\n",
    "        if i not in safe_cols:\n",
    "            std = sw_df[i].std()\n",
    "            mean = sw_df[i].mean()\n",
    "\n",
    "            cleaned = sw_df[i][sw_df[i]<mean+std*sigma_val]\n",
    "            trash = sw_df[i][sw_df[i]>=mean+std*sigma_val]\n",
    "\n",
    "            cleaned_cols.append(cleaned)\n",
    "            trash_data.append(trash)\n",
    "        else:\n",
    "            cleaned_cols.append(sw_df[i])\n",
    "            trash_data.append([np.nan])\n",
    "#   Initialising empty dataframes and appending data\n",
    "    sw_c_df = pd.DataFrame()\n",
    "    trash_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(sw_df.columns)):\n",
    "#         sw_c_df[sw_df.columns[i]] = cleaned_cols[i]\n",
    "#         sw_c_df = sw_c_df\n",
    "        sw_c_df_temp = pd.DataFrame(cleaned_cols[i],\n",
    "                                 columns=[sw_df.columns[i]])\n",
    "        sw_c_df = pd.concat([sw_c_df,sw_c_df_temp], axis=1)\n",
    "\n",
    "        trash_df_temp = pd.DataFrame(trash_data[i],\n",
    "                                     columns=[sw_df.columns[i]])\n",
    "        trash_df = pd.concat([trash_df,trash_df_temp], axis=1)\n",
    "\n",
    "#   Checking if the trash data contains non-'bad' data.\n",
    "    check_trash(trash_df)\n",
    "    return (sw_c_df, trash_df)\n",
    "\n",
    "#################################\n",
    "\n",
    "def sw_interp(data,method='linear'):\n",
    "    '''\n",
    "    Function which interpolates NaN values in the cleaned\n",
    "    data dataframe.\n",
    "\n",
    "    See Pandas documentation for other methods.\n",
    "\n",
    "    Input:\n",
    "    method - method of interpolation.\n",
    "\n",
    "    Return:\n",
    "     - Cleaned, interpolated data.\n",
    "    '''\n",
    "    return data.interpolate(method=method)\n",
    "\n",
    "#################################\n",
    "\n",
    "def check_trash(trash_data):\n",
    "    '''\n",
    "    Function which checks to see if all the removed data\n",
    "    is the 'bad' data fill value.\n",
    "\n",
    "    Returns:\n",
    "     - String detailing which parameters have had real\n",
    "       removed from them.\n",
    "    '''\n",
    "    for i in trash_data.columns:\n",
    "        if (trash_data[i].mean() <\n",
    "            trash_data[i].max()):\n",
    "            print('Some real data has been removed from: ',i)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "def time_history(data,auto=True):\n",
    "    '''\n",
    "    Function which calculates time history information\n",
    "    given an input dataframe.\n",
    "    \n",
    "    Averages are centred on the respective time-history\n",
    "    specified.\n",
    "    \n",
    "    Input:\n",
    "    data - a Pandas DataFrame containing 5 minute cadence\n",
    "           data.\n",
    "           MUST HAVE DATETIME INDEX.\n",
    "    auto - Whether or not to automatically clean the data.\n",
    "           If not True, then the cleaning_data() and\n",
    "           sw_interp() functions must be called individually\n",
    "           and the results of sw_interp() should be the data\n",
    "           fed to this function, time_history(). Set to false\n",
    "           to retrieve the non-interpolated data and the trash\n",
    "           data.\n",
    "    \n",
    "    Output:\n",
    "    A concatenated DataFrame containing\n",
    "        - the original data\n",
    "        - t-6hrs (1hr avg)\n",
    "        - t-5hrs (1hr avg)\n",
    "        - t-3hrs (30min avg)\n",
    "        - t-1hrs (30min avg)\n",
    "        - t-45min (instant)\n",
    "        - t-30min (instant)\n",
    "        - t-15min (instant)\n",
    "        - t-10min (instant)\n",
    "        - t-5min (instant)\n",
    "    '''\n",
    "    if auto is True:\n",
    "        c_data,t_data = cleaning_data(data,sigma_val=4,\n",
    "                                  safe_cols=[None])\n",
    "        c_i_data = sw_interp(c_data,method='linear')\n",
    "        \n",
    "        data = c_i_data\n",
    "    else:\n",
    "        pass\n",
    "    return pd.concat((data,\n",
    "                      t_hist(data,360,60).avg_hist(),\n",
    "                      t_hist(data,300,60).avg_hist(),\n",
    "                      t_hist(data,180,30).avg_hist(),\n",
    "                      t_hist(data,60,30).avg_hist(),\n",
    "                      t_hist(data,45,0).instant_hist(),\n",
    "                      t_hist(data,30,0).instant_hist(),\n",
    "                      t_hist(data,15,0).instant_hist(),\n",
    "                      t_hist(data,10,0).instant_hist(),\n",
    "                      t_hist(data,5,0).instant_hist()),axis=1)\n",
    "\n",
    "import ftplib\n",
    "import os\n",
    "\n",
    "def _is_ftp_dir(ftp_handle, name, guess_by_extension=True):\n",
    "    \"\"\" simply determines if an item listed on the ftp server is a valid directory or not \"\"\"\n",
    "\n",
    "    # if the name has a \".\" in the fourth to last position, its probably a file extension\n",
    "    # this is MUCH faster than trying to set every file to a working directory, and will work 99% of time.\n",
    "    if guess_by_extension is True:\n",
    "        if name[-4] == '.':\n",
    "            return False\n",
    "\n",
    "    original_cwd = ftp_handle.pwd()     # remember the current working directory\n",
    "    try:\n",
    "        ftp_handle.cwd(name)            # try to set directory to new name\n",
    "        ftp_handle.cwd(original_cwd)    # set it back to what it was\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _make_parent_dir(fpath):\n",
    "    \"\"\" ensures the parent directory of a filepath exists \"\"\"\n",
    "    dirname = os.path.dirname(fpath)\n",
    "    while not os.path.exists(dirname):\n",
    "        try:\n",
    "            os.mkdir(dirname)\n",
    "            print(\"created {0}\".format(dirname))\n",
    "        except:\n",
    "            _make_parent_dir(dirname)\n",
    "\n",
    "\n",
    "def _download_ftp_file(ftp_handle, name, dest, overwrite,datetime_start,datetime_end):\n",
    "    \"\"\" downloads a single file from an ftp server \"\"\"\n",
    "    _make_parent_dir(dest)\n",
    "    \n",
    "    if (name[-7:-1]!='SHA1SU'): #ignore SHA1SUM files\n",
    "        month = int(name[-15:-13])\n",
    "        day = int(name[-13:-11])\n",
    "        year = int(name[-19:-15])\n",
    "        base = datetime.datetime(year, month, day)\n",
    "        if (datetime_start <= base <= datetime_end):\n",
    "\n",
    "            if not os.path.exists(dest) or overwrite is True:\n",
    "                with open(dest, 'wb') as f:\n",
    "                    ftp_handle.retrbinary(\"RETR {0}\".format(name), f.write)\n",
    "                print(\"downloaded: {0}\".format(dest))\n",
    "            else:\n",
    "                print(\"already exists: {0}\".format(dest))\n",
    "\n",
    "\n",
    "def _mirror_ftp_dir(ftp_handle, name, overwrite, guess_by_extension,datetime_start,datetime_end):\n",
    "    \"\"\" replicates a directory on an ftp server recursively \"\"\"\n",
    "    for item in ftp_handle.nlst(name):\n",
    "        if _is_ftp_dir(ftp_handle, item):\n",
    "            _mirror_ftp_dir(ftp_handle, item, overwrite, guess_by_extension,datetime_start,datetime_end)\n",
    "        else:\n",
    "            _download_ftp_file(ftp_handle, item, item, overwrite,datetime_start,datetime_end)\n",
    "\n",
    "\n",
    "def download_ftp_tree(ftp_handle, path, destination, datetime_start,datetime_end, overwrite=False, guess_by_extension=True):\n",
    "    \"\"\"\n",
    "    Downloads an entire directory tree from an ftp server to the local destination\n",
    "\n",
    "    :param ftp_handle: an authenticated ftplib.FTP instance\n",
    "    :param path: the folder on the ftp server to download\n",
    "    :param destination: the local directory to store the copied folder\n",
    "    :param overwrite: set to True to force re-download of all files, even if they appear to exist already\n",
    "    :param guess_by_extension: It takes a while to explicitly check if every item is a directory or a file.\n",
    "        if this flag is set to True, it will assume any file ending with a three character extension \".???\" is\n",
    "        a file and not a directory. Set to False if some folders may have a \".\" in their names -4th position.\n",
    "    \"\"\"\n",
    "    os.chdir(destination)\n",
    "    _mirror_ftp_dir(ftp_handle, path, overwrite, guess_by_extension,datetime_start,datetime_end)\n",
    "    \n",
    "def  get_data(datetime_start,datetime_end,sc_id):\n",
    "\n",
    "    years = {6:[1987], 7:[1987],8:[1987],9:[1988]\n",
    "             ,12:[2000,2001,2002], 13:[2000,2001,2002,2003,2004,2005,2006,2007],14:[2000,2001,2002,2003,2004,2005],\n",
    "             15:[2000,2001,2002,2003,2004,2005,2006,2007,2008,2009],\n",
    "             16:[2010,2011,2012,2013,2014,2015], #year 2003-2009 is not accessible on site\n",
    "             17:[2009,2010,2011,2012,2013,2014,2015],\n",
    "             18:[2010,2011,2012,2013,2014,2015],#2009 not accessible\n",
    "            } \n",
    "    \n",
    "#     if !(years[sc_id].any() != datetime_start.year):\n",
    "#         print('bad sc_id year combination')\n",
    "#         exit()\n",
    "    \n",
    "    dmsp_feature_list = ['ELE_AVG_ENERGY','ELE_AVG_ENERGY_STD','ELE_TOTAL_ENERGY_FLUX','ELE_TOTAL_ENERGY_FLUX_STD',\n",
    "                         'SC_AACGM_LAT',  'SC_AACGM_LON','SC_AACGM_LTIME',] \n",
    "    #                     'ION_AVG_ENERGY','ION_AVG_ENERGY_STD','ION_TOTAL_ENERGY_FLUX','ION_TOTAL_ENERGY_FLUX_STD','SC_GEOCENTRIC_LAT','SC_GEOCENTRIC_LON','SC_GEOCENTRIC_R']\n",
    "    dmsp_feature_list_19 =['ELE_COUNTS_BKG','ELE_COUNTS_OBS','ELE_DIFF_ENERGY_FLUX','ELE_DIFF_ENERGY_FLUX_STD',\n",
    "                      'CHANNEL_ENERGIES','ELE_COUNTS_BKG','ELE_COUNTS_OBS',]\n",
    "    #                  'ELE_GEOMETRIC','ION_COUNTS_BKG','ION_COUNTS_OBS','ION_DIFF_ENERGY_FLUX','ION_DIFF_ENERGY_FLUX_STD','ION_GEOMETRIC',\n",
    "    #                  'SC_ECI','SC_ECI_LABEL']\n",
    "\n",
    "    import glob\n",
    "    all_sc_df = pd.DataFrame()\n",
    "    count = 0\n",
    "    for ii in [sc_id]:\n",
    "        sc_df = pd.DataFrame()\n",
    "        print(ii)\n",
    "        directory = 'pub/data/dmsp/dmspf'\n",
    "        if ii <10:\n",
    "            directory = directory + '0'\n",
    "        directory = directory + str(ii) + '/ssj/precipitating-electrons-ions'\n",
    "        for year in [datetime_start.year]:\n",
    "            print(year)\n",
    "            \n",
    "            from ftplib import FTP_TLS\n",
    "            ftp=FTP_TLS('cdaweb.gsfc.nasa.gov')\n",
    "            ftp.login()\n",
    "            ftp.dir()\n",
    "            download_ftp_tree(ftp, directory, '.',datetime_start,datetime_end)\n",
    "            \n",
    "            file_list=glob.glob(directory + '/'+str(year)+'/*')\n",
    "            \n",
    "            \n",
    "            for file in file_list:\n",
    "                print(file)\n",
    "                #try:\n",
    "\n",
    "                count = count+1\n",
    "                month = int(file[-15:-13])\n",
    "                day = int(file[-13:-11])\n",
    "                base = datetime.datetime(year, month, day)\n",
    "\n",
    "                if (datetime_start <= base <= datetime_end):\n",
    "                    indices = np.array([base + datetime.timedelta(seconds=iii) for iii in range(0,60*24*60)])\n",
    "                    df = pd.DataFrame(data=indices,columns=['index'])\n",
    "                    cdf = pycdf.CDF(file)\n",
    "                    count = count+1\n",
    "                    for feature in dmsp_feature_list:\n",
    "                        df[feature] = cdf[feature]\n",
    "\n",
    "                    ch_energies = cdf['CHANNEL_ENERGIES']\n",
    "                    #print(ch_energies[:])\n",
    "                    ch_energies = np.flip((ch_energies[:]))\n",
    "                    ele_geom = np.flip( cdf['ELE_GEOMETRIC'][:])\n",
    "                                       \n",
    "                    obs_m_bkg = np.flip(cdf['ELE_COUNTS_OBS'][:,:]- cdf['ELE_COUNTS_BKG'][:,:],axis=1)\n",
    "                    temp_jN = np.zeros((60*24*60,19))\n",
    "                    for kk in range(0,19):\n",
    "                        temp_jN[:,kk] =( obs_m_bkg[:,kk]\n",
    "                             )/          ele_geom[kk]\n",
    "                        \n",
    "                    df['ELE_TOTAL_COUNTS'] = temp_jN[:,0]*(ch_energies[1]-ch_energies[0])\n",
    "                    + temp_jN[:,18]*(ch_energies[18]-ch_energies[17])\n",
    "                    for i in range(1,18):\n",
    "                        df['ELE_TOTAL_COUNTS'] = df['ELE_TOTAL_COUNTS']  +     temp_jN[:,i]*(ch_energies[i+1]-ch_energies[i-1])/2.\n",
    "\n",
    "                    for i in range(0,19):\n",
    "                        name = 'ELE_COUNT_'+str(i+1)\n",
    "                        df[name]=temp_jN[:,i]\n",
    "                    for i in range(0,19):\n",
    "                        name = 'ELE_diff_'+str(i+1)\n",
    "                        df[name]=cdf['ELE_DIFF_ENERGY_FLUX'][:,i]\n",
    "\n",
    "                    df = df.set_index('index')\n",
    "                    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "                    print('df',df.shape)#, df)\n",
    "                    #Create a time window\n",
    "                    df = df[df['ELE_TOTAL_ENERGY_FLUX'] > 0].dropna(subset=['ELE_TOTAL_ENERGY_FLUX'])\n",
    "\n",
    "                     #Create a time window\n",
    "                    sTimeIMF = datetime.datetime(year,month,day)\n",
    "                    eTimeIMF = sTimeIMF + datetime.timedelta(hours = 24)\n",
    "\n",
    "                    df_omni_5min = download_omni_data(sTimeIMF- datetime.timedelta(hours = 6),\n",
    "                                                 eTimeIMF+ datetime.timedelta(hours = 6))\n",
    "                    print('df_omni_5min',df_omni_5min.shape)#, df_omni_5min)\n",
    "                    indices = np.array([sTimeIMF- datetime.timedelta(hours = 6)+ datetime.timedelta(minutes=5*iii) for iii in range(0,df_omni_5min.shape[0])])\n",
    "                    print('indices',indices.shape)#, indices)\n",
    "\n",
    "                    df_omni_5min = pd.DataFrame(data=df_omni_5min.values,columns=df_omni_5min.columns, index=indices)\n",
    "                    print('df_omni_5min',df_omni_5min.shape)#, df_omni_5min)\n",
    "\n",
    "\n",
    "                    # call time_history to clean up omnireader data        \n",
    "                    df_omni_5min_cleaned = time_history(df_omni_5min)\n",
    "                    print('df_omni_5min_cleaned',df_omni_5min_cleaned.shape)#,, df_omni_5min_cleaned)\n",
    "                    df_omni_1min_cleaned = pd.DataFrame(np.repeat(df_omni_5min_cleaned.values,5*60,axis=0))\n",
    "                    print('df_omni_5min_cleaned',df_omni_5min_cleaned.shape)#,, df_omni_5min_cleaned)\n",
    "\n",
    "        #             #create the indices\n",
    "                    indices = []\n",
    "                    for index in df_omni_5min_cleaned.index:\n",
    "                        for jj in range(0,60*5):\n",
    "                            indices.append(index+ datetime.timedelta(seconds=jj))\n",
    "                    print('indices',len(indices))#, indices)\n",
    "\n",
    "                    df_omni_1min_cleaned = pd.DataFrame(data=df_omni_1min_cleaned.values,columns=df_omni_5min_cleaned.columns,\n",
    "                                                        index=indices)    \n",
    "                    print('df_omni_5min_cleaned',df_omni_5min_cleaned.shape)#, df_omni_5min_cleaned)\n",
    "\n",
    "                    intersection_indices = df_omni_1min_cleaned.index.intersection(df.index)\n",
    "                    print('intersection_indices',intersection_indices.shape)#,, intersection_indices)\n",
    "\n",
    "                    df = df.loc[intersection_indices]\n",
    "                    print('df',df.shape)#, df)\n",
    "                    df_omni_1min_cleaned = df_omni_1min_cleaned.loc[intersection_indices]\n",
    "                    print('df_omni_5min_cleaned',df_omni_5min_cleaned.shape)#, df_omni_5min_cleaned)\n",
    "\n",
    "                    #combine Omni and DMSP data\n",
    "                    for feature in df_omni_1min_cleaned.columns:\n",
    "                        df[feature] = df_omni_1min_cleaned[feature]  \n",
    "\n",
    "                    sc_df = pd.concat([sc_df,df])\n",
    "                    print('sc_df',sc_df.shape)#, sc_df)\n",
    "#                 except Exception as e: \n",
    "#                     print('Error')\n",
    "#                     print(e)\n",
    "#                     print(file)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            sc_df['SC_ID']= ii\n",
    "\n",
    "            #filescname = \"./all_sc_df_1-sec_2010-\" + str(ii) + \".pkl\"\n",
    "            #pickling_on = open(filescname,\"wb\")\n",
    "            #pickle.dump(sc_df, pickling_on,protocol=4)\n",
    "\n",
    "            all_sc_df = pd.concat([all_sc_df,sc_df])   \n",
    "            #print(i, 'all_sc_df',all_sc_df.shape, all_sc_df)\n",
    "\n",
    "    test=all_sc_df\n",
    "    test['datetime']=test.index\n",
    "    test=test.sort_values(by=['datetime'])\n",
    "    test = test.set_index('datetime')\n",
    "\n",
    "    # get rid of ones near equatoer\n",
    "    test=test[np.abs(test['SC_AACGM_LAT'])>45]\n",
    "    # swap by for southern hemisphere\n",
    "    test.loc[test['SC_AACGM_LAT']<0 , 'By'] = -test.loc[test['SC_AACGM_LAT']<0 , 'By']\n",
    "    #combine southern with northern hemisphere data\n",
    "    test['SC_AACGM_LAT']=np.abs(test['SC_AACGM_LAT'])\n",
    "\n",
    "    test['cos_SC_AACGM_LTIME']=np.cos(test['SC_AACGM_LTIME']*2*3.14159/24)\n",
    "    test['sin_SC_AACGM_LTIME']=np.sin(test['SC_AACGM_LTIME']*2*3.14159/24)\n",
    "\n",
    "    doy_loop = test.index.day\n",
    "    ut_loop = test.index.hour*3600 + test.index.minute*60 + test.index.second\n",
    "    test['sin_doy']= np.sin(2*np.pi*doy_loop/365.)\n",
    "    test['cos_doy'] = np.cos(2*np.pi*doy_loop/365.)\n",
    "    test['sin_ut'] = np.sin(2*np.pi*ut_loop/86400.)\n",
    "    test['cos_ut'] = np.cos(2*np.pi*ut_loop/86400.)\n",
    "\n",
    "    del doy_loop,ut_loop\n",
    "    cdf\n",
    "    return test    \n",
    "\n",
    "\n",
    "dpi=200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_multiple(df_results,model_names, y_val_log):\n",
    "    \n",
    "    num = len(df_results)\n",
    "    names = ['val']\n",
    "    for i in range(0,num):\n",
    "        names.append(model_names[i])\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(y_val_log[:],alpha = 0.5)\n",
    "    for i in range(0,num):\n",
    "        plt.plot(df_results[i],alpha = 0.5)\n",
    "    plt.legend(names, loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(10**y_val_log[:]*1.6e-6,alpha = 0.5)\n",
    "    for i in range(0,num):\n",
    "        plt.plot(10**df_results[i]*1.6e-6,alpha = 0.5)\n",
    "    plt.legend(names, loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(10**y_val_log.values*1.6e-6,bins=200, range=(10**6*1.6e-6,10**14*1.6e-6), log=True,alpha = 0.5)\n",
    "    for i in range(0,num):\n",
    "        plt.hist(10**df_results[i].values*1.6e-6,bins=200, log=True,alpha = 0.5)\n",
    "    plt.legend(names, loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(y_val_log.values,bins=200,range=(6,14),alpha = 0.5)\n",
    "    for i in range(0,num):  \n",
    "        plt.hist(df_results[i].values,bins=200,range=(6,14),alpha = 0.5)\n",
    "    plt.legend(names, loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(y_val_log.values,bins=200,range=(6,14))\n",
    "    for k in range(0,num):\n",
    "        plt.hist(df_results[k].values,bins=200,range=(6,14),alpha = 0.5)\n",
    "    plt.legend(names, loc='upper left')\n",
    "    plt.ylim([0,2000])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure();\n",
    "    for k in range(0,num):     \n",
    "        errors= y_val_log.values-df_results[k].values[:,0]\n",
    "        plt.scatter(y_val_log.values,errors,s=1,alpha = 0.5)\n",
    "    plt.title('Errors over target Bins')\n",
    "    plt.legend(model_names, loc='upper left')                      \n",
    "    plt.show()\n",
    "\n",
    "    plt.figure();\n",
    "    for k in range(0,num):\n",
    "        errors= y_val_log.values-df_results[k].values[:,0]     \n",
    "        bin_total = np.zeros((200))\n",
    "        bin_error_total = np.zeros((200))\n",
    "        for j in range(0,y_val_log.values.shape[0]):\n",
    "            i = int((y_val_log[j]-6)/((14-6)/200))\n",
    "            if i < 200:\n",
    "                bin_total[i] = bin_total[i]+1\n",
    "                bin_error_total[i] = bin_error_total[i] + np.abs(errors[j])\n",
    "        avg_error_over_hist = bin_error_total/(bin_total+.00001)\n",
    "        plt.scatter(np.linspace(6,14,num=200),avg_error_over_hist,alpha = 0.5)\n",
    "    plt.title('Average Validation Error over target Bins')    \n",
    "    plt.legend(model_names, loc='upper left')\n",
    "    plt.ylim([0,2.6])\n",
    "    plt.show()\n",
    "\n",
    "    for k in range(0,num):  \n",
    "        plt.figure();\n",
    "\n",
    "        errors= y_val_log.values-df_results[k].values[:,0]\n",
    "        plt.hist2d(y_val_log.values,errors,bins=200)\n",
    "        plt.title('Errors over target Bins'+model_names[k])\n",
    "        plt.show()\n",
    "        \n",
    "    plt.figure();\n",
    "    for k in range(0,num):      \n",
    "        errors= y_val_log.values-df_results[k].values[:,0]     \n",
    "        plt.scatter(df_val['SC_AACGM_LAT'].values,errors,s=1,alpha = 0.5)\n",
    "    plt.legend(model_names, loc='upper left')                      \n",
    "    plt.title('Errors over SC_AACGM_LAT Bins')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure();\n",
    "    for k in range(0,num):\n",
    "        errors= y_val_log.values-df_results[k].values[:,0]     \n",
    "        bin_total = np.zeros((200))\n",
    "        bin_error_total = np.zeros((200))\n",
    "        for j in range(0,y_val_log.values.shape[0]):\n",
    "            i = int((df_val['SC_AACGM_LAT'].values[j]-45)/((90-45)/200))\n",
    "            if i < 200:\n",
    "                bin_total[i] = bin_total[i]+1\n",
    "                bin_error_total[i] = bin_error_total[i] + np.abs(errors[j])\n",
    "        avg_error_over_hist = bin_error_total/bin_total\n",
    "        plt.scatter(np.linspace(45,90,num=200),avg_error_over_hist,alpha = 0.5)\n",
    "    plt.title('Average Validation Error over SC_AACGM_LAT Bins')\n",
    "    plt.legend(model_names, loc='upper left')\n",
    "    plt.ylim([0,1.2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.system('source /home/jackalak/Downloads/cdf37_1-dist/bin/definitions.B')\n",
    "os.environ[\"CDF_LIB\"] = '/home/jackalak/Downloads/cdf37_1-dist/lib'\n",
    "from spacepy import pycdf\n",
    "\n",
    "from ovationpyme.ovation_prime import FluxEstimator,AverageEnergyEstimator,BinCorrector\n",
    "from ovationpyme.ovation_utilities import calc_avg_solarwind\n",
    "from ovationpyme.ovation_plotting import latlt2polar,polar2dial,pcolor_flux\n",
    "    \n",
    "def plot_model_multiple_polar(models,model_names, scalars, model_input_keys, dims, model_pipeline,df_val_inputs):\n",
    "    for pt in range(0,df_val_inputs.shape[0]):\n",
    "        dt =   df_val_inputs.index[pt]#  datetime.datetime( df_val_inputs.index[pt].date().timetuple().tm_year , df_val_inputs.index[pt].date().timetuple().tm_mon,\n",
    "                                 # df_val_inputs.index[pt].date().timetuple().tm_mday, \n",
    "                                 # df_val_inputs.index[pt].date().timetuple().tm_hour, )\n",
    "#                                   df_val_inputs.index[pt].date().timetuple().tm_min) #year,month,day,hour,minute)\n",
    "        print(pt,df_val_inputs.index[pt])\n",
    "        print(dt)      \n",
    "        \n",
    "        num = len(models)\n",
    "\n",
    "        subplotlist=[]\n",
    "        fig= plt.figure(figsize=(12,16))\n",
    "        subplotlist.append(plt.subplot2grid((6,2), (0,0), rowspan=1,colspan=1,polar=True) )\n",
    "        subplotlist.append( plt.subplot2grid((6,2), (0, 1), rowspan=1,colspan=1,polar=True) ) \n",
    "        subplotlist.append( plt.subplot2grid((6,2), (1,0), rowspan=1,colspan=1,polar=True))\n",
    "        subplotlist.append( plt.subplot2grid((6,2), (1, 1), rowspan=1,colspan=1,polar=True))\n",
    "        subplotlist.append( plt.subplot2grid((6,2), (2,0), rowspan=1,colspan=1,polar=True))\n",
    "        subplotlist.append( plt.subplot2grid((6,2), (2, 1), rowspan=1,colspan=1,polar=True))\n",
    "        subplotlist.append( plt.subplot2grid((6,2), (3,0), rowspan=1,colspan=1,polar=True))\n",
    "        subplotlist.append( plt.subplot2grid((6,2), (3, 1), rowspan=1,colspan=1,polar=True))    \n",
    "        subplotlist.append( plt.subplot2grid((6,2), (4,0), rowspan=1,colspan=1,polar=True))\n",
    "        subplotlist.append( plt.subplot2grid((6,2), (4, 1), rowspan=1,colspan=1,polar=True))   \n",
    "        subplotlist.append( plt.subplot2grid((6,2), (5,0), rowspan=1,colspan=1,polar=True))\n",
    "        subplotlist.append( plt.subplot2grid((6,2), (5, 1), rowspan=1,colspan=1,polar=True))\n",
    "        mark= ['s', 'o', 'D', 'v']\n",
    "\n",
    "#         %matplotlib inline\n",
    "#         plt.ioff()\n",
    "\n",
    "        pcolor_kwargs = {'cmap':'gnuplot','vmin':7.5,'vmax':13}\n",
    "\n",
    "        for i in range(0,num):\n",
    "            print(model_names[i])\n",
    "            model = models[i]\n",
    "            X_test=df_val_inputs[model_input_keys[i]][pt:pt+1]\n",
    "\n",
    "            \n",
    "            if (dims[i] == 2):\n",
    "                \n",
    "                X_test= scalars[i].transform(X_test.values)\n",
    "                results = model.predict(X_test)\n",
    "                mlatgridN = np.linspace(90,45,num=128)\n",
    "                mltgridN =  np.linspace(0,24,num=128)   \n",
    "                flux = 10**(results[0,:,:,0])*1.60218e-12*3.14159\n",
    "\n",
    "                ax=subplotlist[i+1]\n",
    "                mappableN = pcolor_flux(ax,mlatgridN,mltgridN,np.log10(flux/1.60218e-12),'N',**pcolor_kwargs)\n",
    "\n",
    "                ax.set_title(model_names[i],fontweight=\"bold\", fontsize='medium',pad=10)\n",
    "\n",
    "\n",
    "                ax.set_theta_zero_location('S')\n",
    "                theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "                theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "                ax.set_thetagrids(theta_label_values,labels=theta_labels)\n",
    "\n",
    "                r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "                r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "                ax.set_rgrids(r_label_values,labels=r_labels)\n",
    "                ax.set_rlim([0.,45.])\n",
    "\n",
    "#                 ax.scatter(np.linspace(90,45,128),np.log10(flux[:,int(23/24*128)]/1.60218e-12), marker=mark[3])\n",
    "            else:\n",
    "                \n",
    "                \n",
    "                mlatgridN = np.linspace(90,45,num=128)\n",
    "                mltgridN =  np.linspace(0,24,num=128)          \n",
    "\n",
    "                #################################\n",
    "                #\n",
    "                #ML model\n",
    "                ##########################\n",
    "                model_input = np.zeros((128,128,len(model_input_keys[i])))\n",
    "                flux = np.zeros((mlatgridN.shape[0], mltgridN.shape[0]))\n",
    "\n",
    "\n",
    "                for j in range(0,mlatgridN.shape[0]):\n",
    "                    for k in range(0,mltgridN.shape[0]):        \n",
    "                        #calc cos and sin\n",
    "                        rads = mltgridN[k]*15*3.14159/180.\n",
    "                        model_input[j,k,:]=X_test.values\n",
    "                        if ( (model_pipeline[i] ==1) \n",
    "                            or (len(model_input_keys[i])==33 or len(model_input_keys[i])==59)  \n",
    "                           ) :\n",
    "                            model_input[j,k,7]=np.cos(rads)\n",
    "                            model_input[j,k,6]=np.sin(rads)\n",
    "                            model_input[j,k,0]=mlatgridN[j]\n",
    "                        elif  ( len(model_input_keys[i])== 72)  :\n",
    "                            model_input[j,k,72-6]=np.cos(rads)\n",
    "                            model_input[j,k,72-5]=np.sin(rads)\n",
    "                            model_input[j,k,0]=mlatgridN[j] \n",
    "                        else: \n",
    "                            model_input[j,k,142]=np.cos(rads)\n",
    "                            model_input[j,k,143]=np.sin(rads)\n",
    "                            model_input[j,k,0]=mlatgridN[j]          \n",
    "                shaped = np.reshape(model_input,(128*128,len(model_input_keys[i])))\n",
    "                X_val_scaled = scalars[i].transform(shaped)\n",
    "                #get auroral region and flux\n",
    "                flux = np.log10(10**(np.reshape(model.predict(X_val_scaled),(128,128)))*3.14159)\n",
    "        \n",
    "                ax=subplotlist[i+1]\n",
    "                mappableN = pcolor_flux(ax,mlatgridN,mltgridN,flux,'N',**pcolor_kwargs)\n",
    "\n",
    "                ax.set_title(model_names[i],fontweight=\"bold\", fontsize='medium',pad=10)\n",
    "\n",
    "\n",
    "                ax.set_theta_zero_location('S')\n",
    "                theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "                theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "                ax.set_thetagrids(theta_label_values,labels=theta_labels)\n",
    "\n",
    "                r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "                r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "                ax.set_rgrids(r_label_values,labels=r_labels)\n",
    "                ax.set_rlim([0.,45.])\n",
    "\n",
    "#                 ax.scatter(np.linspace(90,45,128),np.log10(flux[:,int(23/24*128)]/1.60218e-12), marker=mark[3])\n",
    "\n",
    "\n",
    "\n",
    "#         range1=np.min(np.min([-df_val['AL'].values,df_val['AE'].values,df_val['AU'].values]))\n",
    "#         range2=np.max(np.max([-df_val['AL'].values,df_val['AE'].values,df_val['AU'].values]))\n",
    "\n",
    "\n",
    "        ###########################3\n",
    "        # Ovation128\n",
    "        ########################\n",
    "\n",
    "        auroral_types = ['diff','mono','wave','ions']\n",
    "        # axS = f.add_subplot(122,projection='polar')\n",
    "        for jj in range(0,3):\n",
    "            atype = auroral_types[jj]\n",
    "            jtype =\"energy\"\n",
    "            bincorrect = True\n",
    "            combine_hemispheres = True\n",
    "            dtstr = dt.strftime('%Y%m%d %H:%M')\n",
    "            if jtype=='average energy':\n",
    "                estimator = AverageEnergyEstimator(atype)\n",
    "                get_precip_for_time = estimator.get_eavg_for_time\n",
    "            else:\n",
    "                estimator = FluxEstimator(atype,jtype)\n",
    "                get_precip_for_time = estimator.get_flux_for_time\n",
    "\n",
    "\n",
    "            tflux_kwargs = {'combine_hemispheres':combine_hemispheres,\n",
    "                            'return_dF':True}\n",
    "            fluxtupleN = get_precip_for_time(dt,hemi='N',**tflux_kwargs)\n",
    "            mlatgridN,mltgridN,fluxgridN,newell_coupling = fluxtupleN\n",
    "            fluxtupleS = get_precip_for_time(dt,hemi='S',**tflux_kwargs)\n",
    "            mlatgridS,mltgridS,fluxgridS,newell_coupling = fluxtupleS\n",
    "\n",
    "            if bincorrect:\n",
    "                bcN = BinCorrector(mlatgridN,mltgridN)\n",
    "                fluxgridN = bcN.fix(fluxgridN)\n",
    "                bcS = BinCorrector(mlatgridS,mltgridS)\n",
    "                fluxgridS = bcS.fix(fluxgridS)\n",
    "                print(\"Correction Applied\")\n",
    "\n",
    "            if jj== 0:\n",
    "                fluxgridN_sum = fluxgridN\n",
    "                fluxgridS_sum = fluxgridS\n",
    "            else:\n",
    "                fluxgridN_sum = fluxgridN_sum+fluxgridN\n",
    "                fluxgridS_sum = fluxgridN_sum+fluxgridS\n",
    "\n",
    "        mappableN = pcolor_flux( subplotlist[0],mlatgridN,mltgridN,np.log10(fluxgridN_sum/1.60218e-12),'N',**pcolor_kwargs)\n",
    "\n",
    "        subplotlist[0].set_title('OVATION Pyme',pad =10,fontweight=\"bold\", fontsize='medium')\n",
    "\n",
    "\n",
    "        subplotlist[0].set_theta_zero_location('S')\n",
    "        theta_label_values = np.array([0.,3.,6.,9.,12.,15.,18.,21.])*180./12\n",
    "        theta_labels = ['%d:00' % (int(th/180.*12)) for th in theta_label_values.flatten().tolist()]\n",
    "        subplotlist[0].set_thetagrids(theta_label_values,labels=theta_labels,fontsize='medium', )\n",
    "\n",
    "        r_label_values = 90.-np.array([80.,70.,60.,50.])\n",
    "        r_labels = [r'$%d^{o}$' % (int(90.-rv)) for rv in r_label_values.flatten().tolist()]\n",
    "        subplotlist[0].set_rgrids(r_label_values,labels=r_labels)\n",
    "        subplotlist[0].set_rlim([0.,45.])\n",
    "\n",
    "#         subplotlist[0].scatter(np.ones((20))*23/24*2*3.14159 ,                   np.linspace(0,40,20)      )     \n",
    "\n",
    "        plt.colorbar(mappableN,ax=subplotlist[0],label='Total Energy Flux [erg/cm^s/s]')        \n",
    "\n",
    "        fig.tight_layout() \n",
    "#         fig.set_title('Log10 Predicted Electron Precipitation Energy Flux')\n",
    "        plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "model_names = []\n",
    "scalars = []\n",
    "model_input_keys = []\n",
    "dims = [2,2,1,1,2,1,1]\n",
    "model_pipeline = [1,1,1,1,1,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-93278f8a5493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2010\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2010\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "df_val = get_data( datetime.datetime(2010, 1, 1),datetime.datetime(2010, 12, 31),16)\n",
    "df_val=df_val[0:df_val.shape[0]:60*60]\n",
    "df_val=df_val.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "  \n",
    "    loss = K.sum( K.cast(K.greater(y_true, 0),'float64')*\n",
    "               K.square(y_true-y_pred) )  / ( \n",
    "                K.sum( K.cast(K.greater(y_true, 0),'float64') ))# finds the number of y_true  > 0\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    "    \n",
    "\n",
    "    \n",
    "    mse = K.sum( K.cast(K.greater(y_true, 0),'float64')*(\n",
    "        K.square(y_true-y_pred)) )/params['batch_size']  \n",
    "    return mse\n",
    "\n",
    "def for_CSV_val(scaler_X, model, features, test, XX_test):\n",
    "\n",
    "\n",
    "    num = test.shape[0]\n",
    "    \n",
    "\n",
    "    result_val = []\n",
    "\n",
    "    \n",
    "    for i in range(0,num):\n",
    "\n",
    "\n",
    "       #################################\n",
    "        #\n",
    "        #ML model\n",
    "        ##########################\n",
    "        results = model.predict( XX_test[i:i+1,:])\n",
    "        flux = results[0,:,:,0]             \n",
    "        pt = i\n",
    "\n",
    "        result_val.append( \n",
    "            flux[int((90-test['SC_AACGM_LAT'][pt])/45*128),int((test['SC_AACGM_LTIME'][pt])/24*128)] )     \n",
    "    \n",
    "        \n",
    "\n",
    "    return result_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_145.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "filename='mse_pipeline1_33_2D_128x128_periodic_dropout_float64'\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "keys = ['SC_ID', 'sin_ut','cos_ut', 'sin_doy', 'cos_doy', \n",
    "                     'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', \n",
    "    'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr',\n",
    "    'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr',\n",
    "    'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr',\n",
    "    'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr',\n",
    "    'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr',\n",
    "    'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr',\n",
    "    'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', \n",
    "    'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', \n",
    "    'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', \n",
    "    'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', \n",
    "    'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', \n",
    "    'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', \n",
    "    'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', \n",
    "    'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', \n",
    "    'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', \n",
    "    'borovsky_5min', 'newell_5min']\n",
    "\n",
    "X_val = (df_val[keys])\n",
    "\n",
    "X_val_scaled = scaler_X.transform(X_val.values)\n",
    "\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val_log = df_val['ELE_TOTAL_ENERGY_FLUX']\n",
    "# # y_val_log['ELE_TOTAL_ENERGY_FLUX'] = np.array(y_val_log,dtype='float64')\n",
    "# # print(df_val)\n",
    "\n",
    "# y_val_log_90= pd.DataFrame(data=y_val_log.values, index = y_val_log.index, columns = ['energy'])\n",
    "# y_val_log_90 = y_val_log_90.nlargest(int(0.1*y_val_log_90.shape[0]),columns=['energy'])\n",
    "# X_val_90 = pd.DataFrame(data=X_val_scaled, index = y_val_log.index)\n",
    "# X_val_90 = X_val_90.loc[y_val_log_90.index,:]\n",
    "# df_val_90 = df_val.loc[y_val_log_90.index,:]\n",
    "# print('90% percentile', np.min(y_val_log_90.values),np.max(y_val_log_90.values))\n",
    "\n",
    "# y_val_log_95= pd.DataFrame(data=y_val_log.values, index = y_val_log.index, columns = ['energy'])\n",
    "# y_val_log_95 = y_val_log_95.nlargest(int(0.05*y_val_log_95.shape[0]),columns=['energy'])\n",
    "# X_val_95 = pd.DataFrame(data=X_val_scaled, index = y_val_log.index)\n",
    "# X_val_95 = X_val_95.loc[y_val_log_95.index,:]\n",
    "# df_val_95 = df_val.loc[y_val_log_95.index,:]\n",
    "\n",
    "# print('95% percentile', np.min(y_val_log_95.values),np.max(y_val_log_95.values))\n",
    "\n",
    "# y_val_log_99= pd.DataFrame(data=y_val_log.values, index = y_val_log.index, columns = ['energy'])\n",
    "# y_val_log_99 = y_val_log_99.nlargest(int(0.01*y_val_log_99.shape[0]),columns=['energy'])\n",
    "# X_val_99 = pd.DataFrame(data=X_val_scaled, index = y_val_log.index)\n",
    "# X_val_99 = X_val_99.loc[y_val_log_99.index,:]\n",
    "# df_val_99 = df_val.loc[y_val_log_99.index,:]\n",
    "# print('99% percentile', np.min(y_val_log_99.values),np.max(y_val_log_99.values))\n",
    "\n",
    "# # model.evaluate(X_val_90.values,y_val_log_90.values.astype(float))\n",
    "# results_2d_pipeline1_90 = pd.DataFrame(data=for_CSV_val(0, model, 0, df_val_90, X_val_90.values), index = y_val_log_90.index)\n",
    "# np.mean((results_2d_pipeline1_90-y_val_log_90)**2)\n",
    "# # model.evaluate(X_val_95.values,y_val_log_95.values.astype(float))\n",
    "# results_2d_pipeline1_95 = pd.DataFrame(data=for_CSV_val(0, model, 0, df_val_90, X_val_95.values), index = y_val_log_95.index)\n",
    "# np.mean((results_2d_pipeline1_95-y_val_log_95)**2)\n",
    "# # model.evaluate(X_val_99.values,y_val_log_99.values.astype(float))\n",
    "# results_2d_pipeline1_90 = pd.DataFrame(data=for_CSV_val(0, model, 0, df_val_90, X_val_90.values), index = y_val_log_90.index)\n",
    "# np.mean((results_2d_pipeline1_95-y_val_log_95)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = for_CSV_val(0, model, 0, df_val, X_test)\n",
    "\n",
    "results_2d_pipeline1 = pd.DataFrame(data=results, index = df_val.index)\n",
    "\n",
    "\n",
    "# filename ='results_2d_pipeline2_eval.dat'\n",
    "# results_2d_pipeline2_eval.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_145.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "filename='mse_new_pipeline_145_2D_128x128_periodic_dropout_tailloss_pipline1'\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "keys = ['SC_ID', 'sin_ut','cos_ut', 'sin_doy', 'cos_doy', \n",
    "                     'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', \n",
    "    'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr',\n",
    "    'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr',\n",
    "    'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr',\n",
    "    'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr',\n",
    "    'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr',\n",
    "    'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr',\n",
    "    'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', \n",
    "    'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', \n",
    "    'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', \n",
    "    'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', \n",
    "    'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', \n",
    "    'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', \n",
    "    'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', \n",
    "    'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', \n",
    "    'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', \n",
    "    'borovsky_5min', 'newell_5min']\n",
    "\n",
    "X_val = (df_val[keys])\n",
    "\n",
    "X_val_scaled = scaler_X.transform(X_val.values)\n",
    "\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)\n",
    "\n",
    "results = for_CSV_val(0, model, 0, df_val, X_test)\n",
    "\n",
    "results_2d_tailloss_pipeline1 = pd.DataFrame(data=results, index = df_val.index)\n",
    "\n",
    "\n",
    "# filename ='results_2d_pipeline2_eval.dat'\n",
    "# results_2d_pipeline2_eval.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "keys = ['SC_AACGM_LAT', 'SC_ID', 'sin_ut', 'cos_ut', 'sin_doy', 'cos_doy', \n",
    "        'sin_SC_AACGM_LTIME', 'cos_SC_AACGM_LTIME', 'F107', 'Bz', 'By', 'Bx', 'AE', 'AL',\n",
    "        'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', 'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr', 'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr', 'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', 'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', 'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', 'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', 'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'borovsky_5min', 'newell_5min']\n",
    "\n",
    "X_val_scaled = scaler_X.transform(df_val[keys].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "filename='old_pipeline1_148'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "model_3_pipeline1 = pd.DataFrame(data=results, index =df_val.index)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)\n",
    "\n",
    "# filename ='model_3_pipeline1_pipeline2_eval.dat'\n",
    "# model_3_pipeline1_pipeline2_eval.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "keys = ['SC_AACGM_LAT', 'SC_ID', 'sin_ut', 'cos_ut', 'sin_doy', 'cos_doy', \n",
    "        'sin_SC_AACGM_LTIME', 'cos_SC_AACGM_LTIME', 'F107', 'Bz', 'By', 'Bx', 'AE', 'AL',\n",
    "        'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', 'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr', 'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr', 'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', 'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', 'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', 'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', 'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'borovsky_5min', 'newell_5min']\n",
    "\n",
    "X_val_scaled = scaler_X.transform(df_val[keys].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "filename='model_3_dist_tail3_2'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "model_3_dist_tail_pipeline1 = pd.DataFrame(data=results, index =df_val.index)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)\n",
    "\n",
    "# filename ='model_3_pipeline1_pipeline2_eval.dat'\n",
    "# model_3_pipeline1_pipeline2_eval.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_145.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "keys=['SC_ID', 'sin_ut','cos_ut', 'sin_doy', 'cos_doy', \n",
    "                     'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', \n",
    "    'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr',\n",
    "    'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr',\n",
    "    'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr',\n",
    "    'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr',\n",
    "    'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr',\n",
    "    'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr',\n",
    "    'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', \n",
    "    'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', \n",
    "    'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', \n",
    "    'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', \n",
    "    'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', \n",
    "    'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', \n",
    "    'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', \n",
    "    'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', \n",
    "    'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', \n",
    "    'borovsky_5min', 'newell_5min'\n",
    "]\n",
    "X_val = (df_val[keys])\n",
    "\n",
    "X_val_scaled = scaler_X.transform(X_val.values)\n",
    "\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "\n",
    "filename='results_2d_no_window_pipeline1'\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)\n",
    "\n",
    "# filename ='results_2d_no_window_pipeline2_eval.dat'\n",
    "# results_2d_no_window_pipeline2_eval.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = for_CSV_val(scaler_X, model, X_val.columns, df_val, X_test)\n",
    "\n",
    "results_2d_no_window_pipeline1 = pd.DataFrame(data=results, index = X_val.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline2_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "keys = ['SC_AACGM_LAT', 'Bz', 'By', 'Vsw', 'Vx', 'Psw', 'AE', 'AL', 'AU', 'SymH', 'newell', 'borovsky', 'F107', 'PC', 'Bx', 'Bz_6hr', 'By_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'newell_6hr', 'borovsky_6hr', 'F107_6hr', 'PC_6hr', 'Bx_6hr', 'Bz_5hr', 'By_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'newell_5hr', 'borovsky_5hr', 'F107_5hr', 'PC_5hr', 'Bx_5hr', 'Bz_3hr', 'By_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'newell_3hr', 'borovsky_3hr', 'F107_3hr', 'PC_3hr', 'Bx_3hr', 'Bz_1hr', 'By_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'newell_1hr', 'borovsky_1hr', 'F107_1hr', 'PC_1hr', 'Bx_1hr', 'Bz_45min', 'By_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'newell_45min', 'borovsky_45min', 'F107_45min', 'PC_45min', 'Bx_45min', 'Bz_30min', 'By_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'newell_30min', 'borovsky_30min', 'F107_30min', 'PC_30min', 'Bx_30min', 'Bz_15min', 'By_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'newell_15min', 'borovsky_15min', 'F107_15min', 'PC_15min', 'Bx_15min', 'Bz_10min', 'By_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'newell_10min', 'borovsky_10min', 'F107_10min', 'PC_10min', 'Bx_10min', 'Bz_5min', 'By_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'newell_5min', 'borovsky_5min', 'F107_5min', 'PC_5min', 'Bx_5min', 'SC_ID', 'cos_SC_AACGM_LTIME', 'sin_SC_AACGM_LTIME', 'sin_doy', 'cos_doy', 'sin_ut', 'cos_ut']\n",
    "X_val_scaled = scaler_X.transform(df_val[keys].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "filename='new_pipeline2_148'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "df_results = pd.DataFrame(data=results, index = df_val.index)\n",
    "\n",
    "model_3_pipeline2 = df_results\n",
    "\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline2_15sec_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "keys=['SC_AACGM_LAT', 'Bz', 'By', 'Vsw', 'Vx', 'Psw', 'AE', 'AL', 'AU', 'SymH', 'newell', 'borovsky', 'F107', 'PC', 'Bx', 'Bz_6hr', 'By_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'newell_6hr', 'borovsky_6hr', 'F107_6hr', 'PC_6hr', 'Bx_6hr', 'Bz_5hr', 'By_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'newell_5hr', 'borovsky_5hr', 'F107_5hr', 'PC_5hr', 'Bx_5hr', 'Bz_3hr', 'By_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'newell_3hr', 'borovsky_3hr', 'F107_3hr', 'PC_3hr', 'Bx_3hr', 'Bz_1hr', 'By_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'newell_1hr', 'borovsky_1hr', 'F107_1hr', 'PC_1hr', 'Bx_1hr', 'Bz_45min', 'By_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'newell_45min', 'borovsky_45min', 'F107_45min', 'PC_45min', 'Bx_45min', 'Bz_30min', 'By_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'newell_30min', \n",
    "      'borovsky_30min', 'F107_30min', 'PC_30min', 'Bx_30min', 'Bz_15min', 'By_15min', 'Vsw_15min', 'Vx_15min', \n",
    "      'Psw_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'newell_15min', 'borovsky_15min', 'F107_15min',\n",
    "      'PC_15min', 'Bx_15min', 'Bz_10min', 'By_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'AE_10min', 'AL_10min',\n",
    "      'AU_10min', 'SymH_10min', 'newell_10min', 'borovsky_10min', 'F107_10min', 'PC_10min', 'Bx_10min', 'Bz_5min', \n",
    "      'By_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'newell_5min', \n",
    "      'borovsky_5min', 'F107_5min', 'PC_5min', 'Bx_5min', 'SC_ID', 'cos_SC_AACGM_LTIME', 'sin_SC_AACGM_LTIME',\n",
    "      'sin_doy', 'cos_doy', 'sin_ut', 'cos_ut']\n",
    "X_val_scaled = scaler_X.transform(df_val[keys].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "filename='new_pipeline2_148_15sec'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "df_results = pd.DataFrame(data=results, index = df_val.index)\n",
    "\n",
    "\n",
    "model_3_pipeline2_15sec = df_results\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model_multiple_polar(models,model_names, scalars, model_input_keys, dims, model_pipeline,df_val[0:(df_val.shape[0]):3600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = df_val['ELE_TOTAL_ENERGY_FLUX'].copy(deep=True)\n",
    "\n",
    "\n",
    "y_val[y_val == 0] = 0.0001\n",
    "y_val_log = np.log10(y_val.copy(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot((model_3_pipeline2_15sec ))\n",
    "plt.plot((model_3_pipeline1))\n",
    "plt.plot((model_3_pipeline2))\n",
    "plt.plot((results_2d_pipeline1))\n",
    "plt.plot((results_2d_no_window_pipeline1))\n",
    "plt.plot(y_val_log, alpha=.5)\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1','val','y_val_log'])\n",
    "plt.show()\n",
    "\n",
    "filename='march_17_17_1sec_compare.pkl'\n",
    "pickle.dump( (model_3_pipeline2_15sec,\n",
    "              model_3_pipeline1,\n",
    "              model_3_pipeline2,\n",
    "              results_2d_pipeline1,\n",
    "              results_2d_no_window_pipeline1,\n",
    "              y_val_log)\n",
    "            ,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot((model_3_pipeline2_15sec ))\n",
    "plt.plot((model_3_pipeline1))\n",
    "plt.plot((model_3_pipeline2))\n",
    "plt.plot((results_2d_pipeline1))\n",
    "plt.plot((results_2d_no_window_pipeline1))\n",
    "#plt.plot(y_val_log)\n",
    "plt.plot(y_val_log[0:y_val_log.shape[0]:15], alpha=.5)\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1','val','y_val_log'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot((model_3_pipeline2_15sec[0:y_val_log.shape[0]:15] ))\n",
    "plt.plot((model_3_pipeline1[0:y_val_log.shape[0]:15]))\n",
    "plt.plot((model_3_pipeline2[0:y_val_log.shape[0]:15]))\n",
    "plt.plot((results_2d_pipeline1[0:y_val_log.shape[0]:15]))\n",
    "plt.plot((results_2d_no_window_pipeline1[0:y_val_log.shape[0]:15]))\n",
    "#plt.plot(y_val_log)\n",
    "plt.plot(y_val_log[0:y_val_log.shape[0]:15], alpha=.5)\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1','val','y_val_log'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename='march_17_17_1sec_compare.pkl'\n",
    "(model_3_pipeline2_15sec,\n",
    "              model_3_pipeline1,\n",
    "              model_3_pipeline2,\n",
    "              results_2d_pipeline1,\n",
    "              results_2d_no_window_pipeline1,\n",
    "              y_val_log) =   pickle.load(open(filename,'rb'))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot((model_3_pipeline2_15sec ))\n",
    "plt.plot((model_3_pipeline1))\n",
    "plt.plot((model_3_pipeline2))\n",
    "plt.plot((results_2d_pipeline1))\n",
    "plt.plot((results_2d_no_window_pipeline1))\n",
    "plt.plot(y_val_log[0:y_val_log.shape[0]:15], alpha=.5)\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1','val','y_val_log'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_no_window_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1','val','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend([ 'model_3_pipeline1','results_2d_pipeline1','y_val'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "# plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5)\n",
    "# plt.hist((model_3_pipeline1),bins=100,alpha = 0.5)\n",
    "# plt.hist((model_3_pipeline2),bins=100, alpha = 0.5)\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_no_window_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['results_2d_pipeline1','results_2d_no_window_pipeline1','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec','model_3_pipeline2','y_val_log'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_model_multiple([model_3_pipeline2_15sec,model_3_pipeline1,model_3_pipeline2,results_2d_pipeline1,results_2d_no_window_pipeline1],\n",
    "                    ['model_3_pipeline2_15sec','model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1'],y_val_log)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "model_names = []\n",
    "scalars = []\n",
    "model_input_keys = []\n",
    "dims = [2,1,2,1,1]\n",
    "model_pipeline = [1,1,1,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now for 2010 sc 16 veriying\n",
    "\n",
    "# LOAD THE COMBINED SOURTHERN HEMISPHERE AND NORTHERN HEMISPHERE AS CREATED ABOVE\n",
    "df_train = pd.DataFrame()\n",
    "ii=16\n",
    "filescname = \"/home/jackalak/heartbeat/Downloads/sc_df_cleaned_GFi_\" + str(ii) + \".pkl\"\n",
    "pickle_file = open(filescname, \"rb\") \n",
    "df_train=( (pickle.load(pickle_file)) )     \n",
    "df_train = df_train.sort_index()\n",
    "\n",
    "\n",
    "# Separate training and testing data\n",
    "mask_val = [(df_train.index.year == 2010) & (df_train['SC_ID'].values==16)]\n",
    "df_val = df_train[mask_val[0]].copy(deep=True)\n",
    "df_train = df_train.drop( df_train.index[mask_val[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_145.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "filename='mse_pipeline1_33_2D_128x128_periodic_dropout_float64'\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "\n",
    "X_val = (df_val[['SC_ID', 'sin_ut','cos_ut', 'sin_doy', 'cos_doy', \n",
    "                     'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', \n",
    "    'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr',\n",
    "    'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr',\n",
    "    'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr',\n",
    "    'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr',\n",
    "    'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr',\n",
    "    'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr',\n",
    "    'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', \n",
    "    'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', \n",
    "    'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', \n",
    "    'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', \n",
    "    'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', \n",
    "    'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', \n",
    "    'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', \n",
    "    'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', \n",
    "    'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', \n",
    "    'borovsky_5min', 'newell_5min'\n",
    "]])\n",
    "\n",
    "X_val_scaled = scaler_X.transform(X_val.values)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "results = for_CSV_val(0, model, 0, df_val, X_test)\n",
    "\n",
    "results_2d_pipeline1 = pd.DataFrame(data=results, index = df_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "\n",
    "X_val_scaled = scaler_X.transform(df_val[['SC_AACGM_LAT', 'SC_ID', 'sin_ut', 'cos_ut', 'sin_doy', 'cos_doy', 'sin_SC_AACGM_LTIME', 'cos_SC_AACGM_LTIME', 'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', 'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr', 'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr', 'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', 'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', 'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', 'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', 'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'borovsky_5min', 'newell_5min']].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "filename='old_pipeline1_148'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "model_3_pipeline1 = pd.DataFrame(data=results, index =df_val.index)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline1_145.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "X_val = (df_val[['SC_ID', 'sin_ut','cos_ut', 'sin_doy', 'cos_doy', \n",
    "                     'F107', 'Bz', 'By', 'Bx', 'AE', 'AL', 'AU', 'SymH', 'PC', 'Vsw', 'Vx', 'Psw', \n",
    "    'borovsky', 'newell', 'F107_6hr', 'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr',\n",
    "    'SymH_6hr', 'PC_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr',\n",
    "    'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'Vsw_5hr', 'Vx_5hr',\n",
    "    'Psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AE_3hr', 'AL_3hr',\n",
    "    'AU_3hr', 'SymH_3hr', 'PC_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr',\n",
    "    'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'Vsw_1hr', 'Vx_1hr',\n",
    "    'Psw_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'Bz_45min', 'By_45min', 'Bx_45min', 'AE_45min', \n",
    "    'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'borovsky_45min', \n",
    "    'newell_45min', 'F107_30min', 'Bz_30min', 'By_30min', 'Bx_30min', 'AE_30min', 'AL_30min', 'AU_30min', \n",
    "    'SymH_30min', 'PC_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'borovsky_30min', 'newell_30min', \n",
    "    'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', \n",
    "    'PC_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', \n",
    "    'Bz_10min', 'By_10min', 'Bx_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min', \n",
    "    'Vsw_10min', 'Vx_10min', 'Psw_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', \n",
    "    'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', \n",
    "    'borovsky_5min', 'newell_5min'\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_scaled = scaler_X.transform(X_val.values)\n",
    "\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "\n",
    "filename='results_2d_no_window_pipeline1'\n",
    "model = tensorflow.keras.models.load_model(filename, custom_objects={'custom_loss': custom_loss, 'custom_mse': custom_mse})\n",
    "\n",
    "results = for_CSV_val(scaler_X, model, X_val.columns, df_val, X_test)\n",
    "\n",
    "results_2d_no_window_pipeline1 = pd.DataFrame(data=results, index = X_val.index)\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Scalar_X_pipeline2_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "\n",
    "X_val_scaled = scaler_X.transform(df_val[['SC_AACGM_LAT', 'Bz', 'By', 'Vsw', 'Vx', 'Psw', 'AE', 'AL', 'AU', 'SymH', 'newell', 'borovsky', 'F107', 'PC', 'Bx', 'Bz_6hr', 'By_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'newell_6hr', 'borovsky_6hr', 'F107_6hr', 'PC_6hr', 'Bx_6hr', 'Bz_5hr', 'By_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'newell_5hr', 'borovsky_5hr', 'F107_5hr', 'PC_5hr', 'Bx_5hr', 'Bz_3hr', 'By_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'newell_3hr', 'borovsky_3hr', 'F107_3hr', 'PC_3hr', 'Bx_3hr', 'Bz_1hr', 'By_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'newell_1hr', 'borovsky_1hr', 'F107_1hr', 'PC_1hr', 'Bx_1hr', 'Bz_45min', 'By_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'newell_45min', 'borovsky_45min', 'F107_45min', 'PC_45min', 'Bx_45min', 'Bz_30min', 'By_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'newell_30min', 'borovsky_30min', 'F107_30min', 'PC_30min', 'Bx_30min', 'Bz_15min', 'By_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'newell_15min', 'borovsky_15min', 'F107_15min', 'PC_15min', 'Bx_15min', 'Bz_10min', 'By_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'newell_10min', 'borovsky_10min', 'F107_10min', 'PC_10min', 'Bx_10min', 'Bz_5min', 'By_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'newell_5min', 'borovsky_5min', 'F107_5min', 'PC_5min', 'Bx_5min', 'SC_ID', 'cos_SC_AACGM_LTIME', 'sin_SC_AACGM_LTIME', 'sin_doy', 'cos_doy', 'sin_ut', 'cos_ut']].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "\n",
    "filename='new_pipeline2_148'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "df_results = pd.DataFrame(data=results, index = df_val.index)\n",
    "\n",
    "model_3_pipeline2 = df_results\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)\n",
    "\n",
    "filename = 'Scalar_X_pipeline2_15sec_148.pkl'\n",
    "scaler_X = pickle.load( open(filename,\"rb\"))\n",
    "\n",
    "\n",
    "X_val_scaled = scaler_X.transform(df_val[['SC_AACGM_LAT', 'Bz', 'By', 'Vsw', 'Vx', 'Psw', 'AE', 'AL', 'AU', 'SymH', 'newell', 'borovsky', 'F107', 'PC', 'Bx', 'Bz_6hr', 'By_6hr', 'Vsw_6hr', 'Vx_6hr', 'Psw_6hr', 'AE_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'newell_6hr', 'borovsky_6hr', 'F107_6hr', 'PC_6hr', 'Bx_6hr', 'Bz_5hr', 'By_5hr', 'Vsw_5hr', 'Vx_5hr', 'Psw_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'newell_5hr', 'borovsky_5hr', 'F107_5hr', 'PC_5hr', 'Bx_5hr', 'Bz_3hr', 'By_3hr', 'Vsw_3hr', 'Vx_3hr', 'Psw_3hr', 'AE_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'newell_3hr', 'borovsky_3hr', 'F107_3hr', 'PC_3hr', 'Bx_3hr', 'Bz_1hr', 'By_1hr', 'Vsw_1hr', 'Vx_1hr', 'Psw_1hr', 'AE_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'newell_1hr', 'borovsky_1hr', 'F107_1hr', 'PC_1hr', 'Bx_1hr', 'Bz_45min', 'By_45min', 'Vsw_45min', 'Vx_45min', 'Psw_45min', 'AE_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'newell_45min', 'borovsky_45min', 'F107_45min', 'PC_45min', 'Bx_45min', 'Bz_30min', 'By_30min', 'Vsw_30min', 'Vx_30min', 'Psw_30min', 'AE_30min', 'AL_30min', 'AU_30min', 'SymH_30min', 'newell_30min', 'borovsky_30min', 'F107_30min', 'PC_30min', 'Bx_30min', 'Bz_15min', 'By_15min', 'Vsw_15min', 'Vx_15min', 'Psw_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'newell_15min', 'borovsky_15min', 'F107_15min', 'PC_15min', 'Bx_15min', 'Bz_10min', 'By_10min', 'Vsw_10min', 'Vx_10min', 'Psw_10min', 'AE_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'newell_10min', 'borovsky_10min', 'F107_10min', 'PC_10min', 'Bx_10min', 'Bz_5min', 'By_5min', 'Vsw_5min', 'Vx_5min', 'Psw_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'newell_5min', 'borovsky_5min', 'F107_5min', 'PC_5min', 'Bx_5min', 'SC_ID', 'cos_SC_AACGM_LTIME', 'sin_SC_AACGM_LTIME', 'sin_doy', 'cos_doy', 'sin_ut', 'cos_ut']].values)\n",
    "X_test = np.array(X_val_scaled, dtype=np.float32)\n",
    "filename='new_pipeline2_148_15sec'\n",
    "model = tensorflow.keras.models.load_model(filename)\n",
    "\n",
    "results = model.predict(X_test)\n",
    "\n",
    "df_results = pd.DataFrame(data=results, index = df_val.index)\n",
    "\n",
    "\n",
    "model_3_pipeline2_15sec = df_results\n",
    "\n",
    "models.append(model)\n",
    "model_names.append(filename)\n",
    "scalars.append(scaler_X)\n",
    "model_input_keys.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_multiple_polar(models,model_names, scalars, model_input_keys, dims, model_pipeline,df_val[0:(df_val.shape[0]):3600])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_val = df_val['ELE_TOTAL_ENERGY_FLUX'].copy(deep=True)\n",
    "\n",
    "\n",
    "y_val[y_val == 0] = 0.0001\n",
    "y_val_log = np.log10(y_val.copy(deep=True))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot((model_3_pipeline2_15sec ))\n",
    "plt.plot((model_3_pipeline1))\n",
    "plt.plot((model_3_pipeline2))\n",
    "plt.plot((results_2d_pipeline1))\n",
    "# plt.plot((results_2d_no_window_pipeline1))\n",
    "plt.plot(y_val_log, alpha=.5)\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','val','y_val_log'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# filename='16_2010_15sec_compare.pkl'\n",
    "# pickle.dump( (model_3_pipeline2_15sec,\n",
    "#               model_3_pipeline1,\n",
    "#               model_3_pipeline2,\n",
    "#               results_2d_pipeline1,\n",
    "#               results_2d_no_window_pipeline1,\n",
    "#               y_val_log)\n",
    "#             ,open(filename,'wb'))\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "#plt.hist((results_2d_no_window_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','val','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend([ 'model_3_pipeline1','results_2d_pipeline1','y_val'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "# # plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5)\n",
    "# # plt.hist((model_3_pipeline1),bins=100,alpha = 0.5)\n",
    "# # plt.hist((model_3_pipeline2),bins=100, alpha = 0.5)\n",
    "# plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "# plt.hist((results_2d_no_window_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "# plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "# plt.legend(['results_2d_pipeline1','results_2d_no_window_pipeline1','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec','model_3_pipeline2','y_val_log'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "# plt.hist((results_2d_no_window_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1','val','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend([ 'model_3_pipeline1','results_2d_pipeline1','y_val'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "# plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5)\n",
    "# plt.hist((model_3_pipeline1),bins=100,alpha = 0.5)\n",
    "# plt.hist((model_3_pipeline2),bins=100, alpha = 0.5)\n",
    "plt.hist((results_2d_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((results_2d_no_window_pipeline1),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['results_2d_pipeline1','results_2d_no_window_pipeline1','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline1),bins=100,alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec', 'model_3_pipeline1','model_3_pipeline2','y_val_log'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Histogram of Electron Flux [eV/cm^2/ster/s]')\n",
    "plt.hist((model_3_pipeline2_15sec ),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "plt.hist((model_3_pipeline2),bins=100, alpha = 0.5, range=(6.5,13.5))\n",
    "\n",
    "plt.hist(y_val_log, alpha=.5,bins=100, range=(6.5,13.5))\n",
    "\n",
    "plt.legend(['model_3_pipeline2_15sec','model_3_pipeline2','y_val_log'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_model_multiple([model_3_pipeline2_15sec,model_3_pipeline1,model_3_pipeline2,results_2d_pipeline1,results_2d_no_window_pipeline1],\n",
    "                    ['model_3_pipeline2_15sec','model_3_pipeline1','model_3_pipeline2','results_2d_pipeline1','results_2d_no_window_pipeline1'],y_val_log)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_multiple([model_3_pipeline1,model_3_dist_tail_pipeline1,results_2d_pipeline1,results_2d_tailloss_pipeline1],\n",
    "                    ['model_3_pipeline1','model_3_dist_tail_pipeline1','results_2d_pipeline1','results_2d_tailloss_pipeline1'],y_val_log)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# todo:\n",
    "# compare to old pipeline no moving window 2d\n",
    "# all histogram and avg error over bins\n",
    "# all on 2016 pipeline1\n",
    "# 1 sec compare\n",
    "# all on 2016 first day 1 sec\n",
    "# all on march test\n",
    "# all on jan test\n",
    "\n",
    "# original data with distribution weighted loss function\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
